{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e55bf4d12b1242f2bb1451401b9f3959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f82250d70fce4929bd6c620227c7e2f1",
              "IPY_MODEL_f34969978aa34479954f0f33f07c2f32",
              "IPY_MODEL_ca49dafaa811461190b5b31b2ad989af"
            ],
            "layout": "IPY_MODEL_8959789c1b0e44e59a9e5ca8c8710b60"
          }
        },
        "f82250d70fce4929bd6c620227c7e2f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_345421dfa3bf41bd99bbc292cc62ecd9",
            "placeholder": "​",
            "style": "IPY_MODEL_89fe2d5c8a184aed872d92739d6515db",
            "value": "artifact.metadata: 100%"
          }
        },
        "f34969978aa34479954f0f33f07c2f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e0c60839c24bdd86fe6860b1cc353c",
            "max": 1633,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd7db5590e8f49248c6f983b1d85f47a",
            "value": 1633
          }
        },
        "ca49dafaa811461190b5b31b2ad989af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8fe7fae09394c40b6567f38a278b22b",
            "placeholder": "​",
            "style": "IPY_MODEL_19d4822e79e648f58ba258e05fb04f09",
            "value": " 1.63k/1.63k [00:00&lt;00:00, 57.7kB/s]"
          }
        },
        "8959789c1b0e44e59a9e5ca8c8710b60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "345421dfa3bf41bd99bbc292cc62ecd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89fe2d5c8a184aed872d92739d6515db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54e0c60839c24bdd86fe6860b1cc353c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd7db5590e8f49248c6f983b1d85f47a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8fe7fae09394c40b6567f38a278b22b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19d4822e79e648f58ba258e05fb04f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e04e9283e8f4bc88c043a7cadd442c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36e8eeb064114bf48bf86648a2698665",
              "IPY_MODEL_eb5b16e5a6e2422f946b3ac38215decc",
              "IPY_MODEL_f87db95b81e34b1d9b5e955607754581"
            ],
            "layout": "IPY_MODEL_06f0f1a383324bd4a03e2a542a70e936"
          }
        },
        "36e8eeb064114bf48bf86648a2698665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea570c169ec34575bd7305990ce8bdee",
            "placeholder": "​",
            "style": "IPY_MODEL_b709d7de0fd943bcbf6672d086b5f575",
            "value": "config.json: 100%"
          }
        },
        "eb5b16e5a6e2422f946b3ac38215decc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ac7989cbba94714a92b89cb78a64f98",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef98f7aeeeb74814a7ca03346ae0d4cd",
            "value": 743
          }
        },
        "f87db95b81e34b1d9b5e955607754581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e04b4e8063488594b744ed1722616c",
            "placeholder": "​",
            "style": "IPY_MODEL_eea8da3101f642e0b0fb6c2693d03e7e",
            "value": " 743/743 [00:00&lt;00:00, 31.9kB/s]"
          }
        },
        "06f0f1a383324bd4a03e2a542a70e936": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea570c169ec34575bd7305990ce8bdee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b709d7de0fd943bcbf6672d086b5f575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ac7989cbba94714a92b89cb78a64f98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef98f7aeeeb74814a7ca03346ae0d4cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6e04b4e8063488594b744ed1722616c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eea8da3101f642e0b0fb6c2693d03e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "876568fcde2a48febfbd58fc75ad5bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8eabdb31df974ed19ec7f1e74963a61e",
              "IPY_MODEL_9b7bacccbf554265abf24514ac23ec9f",
              "IPY_MODEL_9bb524e8fa3243f6aad8b60ed4d3821c"
            ],
            "layout": "IPY_MODEL_3b0fac19503e4f72a9b42d18a7581f09"
          }
        },
        "8eabdb31df974ed19ec7f1e74963a61e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5fe94d18b84486398a1b0e5c578f16a",
            "placeholder": "​",
            "style": "IPY_MODEL_a78a6e18e872462495aa3fb7c1ff1188",
            "value": "model.safetensors: 100%"
          }
        },
        "9b7bacccbf554265abf24514ac23ec9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a4a2d50a19a40d7999f4143b1f0cc54",
            "max": 438349816,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ddae8ee784a14be2b680b0e0e03f1623",
            "value": 438349816
          }
        },
        "9bb524e8fa3243f6aad8b60ed4d3821c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1dc1fc551b4305852c7eb058f5650d",
            "placeholder": "​",
            "style": "IPY_MODEL_10119b85c88a452c811dcc6781c92bb3",
            "value": " 438M/438M [00:02&lt;00:00, 172MB/s]"
          }
        },
        "3b0fac19503e4f72a9b42d18a7581f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5fe94d18b84486398a1b0e5c578f16a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78a6e18e872462495aa3fb7c1ff1188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a4a2d50a19a40d7999f4143b1f0cc54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddae8ee784a14be2b680b0e0e03f1623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd1dc1fc551b4305852c7eb058f5650d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10119b85c88a452c811dcc6781c92bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be79dc149eda4c9aa7e7ac0c4fd31127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d55b86734cd4d92a3af7b118d89ba3e",
              "IPY_MODEL_51309041f5c34a8ab6efde8a005a5880",
              "IPY_MODEL_bf09ead2c8754ed6bd2df916a66afc74"
            ],
            "layout": "IPY_MODEL_cbe3ecc181764d95a9625d9aa8dc1c5f"
          }
        },
        "9d55b86734cd4d92a3af7b118d89ba3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_048b44bcebda4f86a7a82f2e34634af1",
            "placeholder": "​",
            "style": "IPY_MODEL_a5933cfbdd44434fb1a8716dd7b86369",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "51309041f5c34a8ab6efde8a005a5880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a21b50251e04d84a3b93c042755c576",
            "max": 405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_201364d00be7486fb74dab510a3b4e6a",
            "value": 405
          }
        },
        "bf09ead2c8754ed6bd2df916a66afc74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5153119805345c0b093d90fbd5f506d",
            "placeholder": "​",
            "style": "IPY_MODEL_818d1493d4144f99aae78a2b78b4503b",
            "value": " 405/405 [00:00&lt;00:00, 23.1kB/s]"
          }
        },
        "cbe3ecc181764d95a9625d9aa8dc1c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "048b44bcebda4f86a7a82f2e34634af1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5933cfbdd44434fb1a8716dd7b86369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a21b50251e04d84a3b93c042755c576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "201364d00be7486fb74dab510a3b4e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5153119805345c0b093d90fbd5f506d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "818d1493d4144f99aae78a2b78b4503b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eddd1f4024a4f8aafed7fec3be863bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbc012475191426a9e7402ff41e71172",
              "IPY_MODEL_70fb0f9791964d81b4eec42166698f64",
              "IPY_MODEL_eed53f24c8d2462c91ad42ca94e032fb"
            ],
            "layout": "IPY_MODEL_9459c4bb547c408dbe9aeed974cb2adc"
          }
        },
        "bbc012475191426a9e7402ff41e71172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ea3594707b844f893aac22b4acb03ef",
            "placeholder": "​",
            "style": "IPY_MODEL_f7e1e1ea86fa4964bb7917088130ebd6",
            "value": "vocab.txt: 100%"
          }
        },
        "70fb0f9791964d81b4eec42166698f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4de5bb02f51d4036a2189d85f3324f34",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8e70882994541089ec5905a9828baa5",
            "value": 231508
          }
        },
        "eed53f24c8d2462c91ad42ca94e032fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b8dbfa378fb4e5c9d01274dfffc34b1",
            "placeholder": "​",
            "style": "IPY_MODEL_3c2eeb2238504decaff99ef2b4b3402d",
            "value": " 232k/232k [00:00&lt;00:00, 4.27MB/s]"
          }
        },
        "9459c4bb547c408dbe9aeed974cb2adc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ea3594707b844f893aac22b4acb03ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7e1e1ea86fa4964bb7917088130ebd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4de5bb02f51d4036a2189d85f3324f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8e70882994541089ec5905a9828baa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b8dbfa378fb4e5c9d01274dfffc34b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c2eeb2238504decaff99ef2b4b3402d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4043eec731d64ce2b1c1c891184b9abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc758e8aa48b441bbcc9a88187d9d7f0",
              "IPY_MODEL_99e641e0f2a94f12ae226fc0c434f5ef",
              "IPY_MODEL_0fba15ad88184b109c0a2c822424f889"
            ],
            "layout": "IPY_MODEL_db13c96f247e4703bc138e81a6d3a5c1"
          }
        },
        "cc758e8aa48b441bbcc9a88187d9d7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c285c6b280c442b69b37984a5d4b3bce",
            "placeholder": "​",
            "style": "IPY_MODEL_ee653e8808df4c1ab178a6d9c37e300b",
            "value": "tokenizer.json: 100%"
          }
        },
        "99e641e0f2a94f12ae226fc0c434f5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d7259bbce06440683140af866ed313e",
            "max": 466081,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43182b2a488a4fbb84aee0272eb09fc5",
            "value": 466081
          }
        },
        "0fba15ad88184b109c0a2c822424f889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d5e0573fad44087974f04a2a103f9c0",
            "placeholder": "​",
            "style": "IPY_MODEL_56015f34b907410889e5027e9df55765",
            "value": " 466k/466k [00:00&lt;00:00, 19.2MB/s]"
          }
        },
        "db13c96f247e4703bc138e81a6d3a5c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c285c6b280c442b69b37984a5d4b3bce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee653e8808df4c1ab178a6d9c37e300b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d7259bbce06440683140af866ed313e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43182b2a488a4fbb84aee0272eb09fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d5e0573fad44087974f04a2a103f9c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56015f34b907410889e5027e9df55765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "517d1ec16a094351bb75dce851ca54f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79ea59cdebde4d54bafcc133548571c4",
              "IPY_MODEL_086a0583ffaf4af4a6532338130fc9d2",
              "IPY_MODEL_0d3ca5e1c32142169a4b2544a4d61e95"
            ],
            "layout": "IPY_MODEL_05ddb68176b145478e3f36cd80a889e4"
          }
        },
        "79ea59cdebde4d54bafcc133548571c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc529aaa53b3421fa3c5146e92437c99",
            "placeholder": "​",
            "style": "IPY_MODEL_fb5f0251a5ca44b1aa6aec35abf9cee9",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "086a0583ffaf4af4a6532338130fc9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f2860311dcb48c9baf157a313342d27",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_970eaf2c49434dfa80fe2a997a97d325",
            "value": 112
          }
        },
        "0d3ca5e1c32142169a4b2544a4d61e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aaacff2e72c44d9a581cedd534f9d46",
            "placeholder": "​",
            "style": "IPY_MODEL_091a32447a3b46abb1eed6069104e49c",
            "value": " 112/112 [00:00&lt;00:00, 7.87kB/s]"
          }
        },
        "05ddb68176b145478e3f36cd80a889e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc529aaa53b3421fa3c5146e92437c99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb5f0251a5ca44b1aa6aec35abf9cee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f2860311dcb48c9baf157a313342d27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "970eaf2c49434dfa80fe2a997a97d325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5aaacff2e72c44d9a581cedd534f9d46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091a32447a3b46abb1eed6069104e49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taaha3244/RAG/blob/main/RAGaTouille_ColBERTv2_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain"
      ],
      "metadata": {
        "id": "9phP3bkIJXSK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EoZCzWEXJNd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63aaa2cf-3958-453e-e0dc-1a45af2a59bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for colbert-ai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U ragatouille\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain-openai\n",
        "!pip install -q langchain-core\n",
        "!pip install -q langchain-community\n",
        "!pip install -q pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "e55bf4d12b1242f2bb1451401b9f3959",
            "f82250d70fce4929bd6c620227c7e2f1",
            "f34969978aa34479954f0f33f07c2f32",
            "ca49dafaa811461190b5b31b2ad989af",
            "8959789c1b0e44e59a9e5ca8c8710b60",
            "345421dfa3bf41bd99bbc292cc62ecd9",
            "89fe2d5c8a184aed872d92739d6515db",
            "54e0c60839c24bdd86fe6860b1cc353c",
            "bd7db5590e8f49248c6f983b1d85f47a",
            "b8fe7fae09394c40b6567f38a278b22b",
            "19d4822e79e648f58ba258e05fb04f09",
            "7e04e9283e8f4bc88c043a7cadd442c8",
            "36e8eeb064114bf48bf86648a2698665",
            "eb5b16e5a6e2422f946b3ac38215decc",
            "f87db95b81e34b1d9b5e955607754581",
            "06f0f1a383324bd4a03e2a542a70e936",
            "ea570c169ec34575bd7305990ce8bdee",
            "b709d7de0fd943bcbf6672d086b5f575",
            "4ac7989cbba94714a92b89cb78a64f98",
            "ef98f7aeeeb74814a7ca03346ae0d4cd",
            "b6e04b4e8063488594b744ed1722616c",
            "eea8da3101f642e0b0fb6c2693d03e7e",
            "876568fcde2a48febfbd58fc75ad5bf8",
            "8eabdb31df974ed19ec7f1e74963a61e",
            "9b7bacccbf554265abf24514ac23ec9f",
            "9bb524e8fa3243f6aad8b60ed4d3821c",
            "3b0fac19503e4f72a9b42d18a7581f09",
            "b5fe94d18b84486398a1b0e5c578f16a",
            "a78a6e18e872462495aa3fb7c1ff1188",
            "1a4a2d50a19a40d7999f4143b1f0cc54",
            "ddae8ee784a14be2b680b0e0e03f1623",
            "fd1dc1fc551b4305852c7eb058f5650d",
            "10119b85c88a452c811dcc6781c92bb3",
            "be79dc149eda4c9aa7e7ac0c4fd31127",
            "9d55b86734cd4d92a3af7b118d89ba3e",
            "51309041f5c34a8ab6efde8a005a5880",
            "bf09ead2c8754ed6bd2df916a66afc74",
            "cbe3ecc181764d95a9625d9aa8dc1c5f",
            "048b44bcebda4f86a7a82f2e34634af1",
            "a5933cfbdd44434fb1a8716dd7b86369",
            "2a21b50251e04d84a3b93c042755c576",
            "201364d00be7486fb74dab510a3b4e6a",
            "a5153119805345c0b093d90fbd5f506d",
            "818d1493d4144f99aae78a2b78b4503b",
            "9eddd1f4024a4f8aafed7fec3be863bc",
            "bbc012475191426a9e7402ff41e71172",
            "70fb0f9791964d81b4eec42166698f64",
            "eed53f24c8d2462c91ad42ca94e032fb",
            "9459c4bb547c408dbe9aeed974cb2adc",
            "2ea3594707b844f893aac22b4acb03ef",
            "f7e1e1ea86fa4964bb7917088130ebd6",
            "4de5bb02f51d4036a2189d85f3324f34",
            "e8e70882994541089ec5905a9828baa5",
            "1b8dbfa378fb4e5c9d01274dfffc34b1",
            "3c2eeb2238504decaff99ef2b4b3402d",
            "4043eec731d64ce2b1c1c891184b9abc",
            "cc758e8aa48b441bbcc9a88187d9d7f0",
            "99e641e0f2a94f12ae226fc0c434f5ef",
            "0fba15ad88184b109c0a2c822424f889",
            "db13c96f247e4703bc138e81a6d3a5c1",
            "c285c6b280c442b69b37984a5d4b3bce",
            "ee653e8808df4c1ab178a6d9c37e300b",
            "9d7259bbce06440683140af866ed313e",
            "43182b2a488a4fbb84aee0272eb09fc5",
            "4d5e0573fad44087974f04a2a103f9c0",
            "56015f34b907410889e5027e9df55765",
            "517d1ec16a094351bb75dce851ca54f4",
            "79ea59cdebde4d54bafcc133548571c4",
            "086a0583ffaf4af4a6532338130fc9d2",
            "0d3ca5e1c32142169a4b2544a4d61e95",
            "05ddb68176b145478e3f36cd80a889e4",
            "fc529aaa53b3421fa3c5146e92437c99",
            "fb5f0251a5ca44b1aa6aec35abf9cee9",
            "3f2860311dcb48c9baf157a313342d27",
            "970eaf2c49434dfa80fe2a997a97d325",
            "5aaacff2e72c44d9a581cedd534f9d46",
            "091a32447a3b46abb1eed6069104e49c"
          ]
        },
        "id": "MeWqFfyKJeEh",
        "outputId": "ccf929b5-0d07-4590-bff6-2c093caa6572"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e55bf4d12b1242f2bb1451401b9f3959"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e04e9283e8f4bc88c043a7cadd442c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "876568fcde2a48febfbd58fc75ad5bf8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be79dc149eda4c9aa7e7ac0c4fd31127"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eddd1f4024a4f8aafed7fec3be863bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4043eec731d64ce2b1c1c891184b9abc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "517d1ec16a094351bb75dce851ca54f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/Data-Science-from-Scratch-First-Principles-with-Python-by-Joel-Grus-z-lib.org_.epub_ (1).pdf\")\n",
        "pages = loader.load_and_split()\n",
        "\n"
      ],
      "metadata": {
        "id": "WwsExlvaKKP2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSH_4VNWK7N4",
        "outputId": "345847aa-4a25-40ff-ed0b-820eb5a823c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_document = \"\"\n",
        "\n",
        "for page in pages:\n",
        "  full_document += page.page_content"
      ],
      "metadata": {
        "id": "uNRKc_vLLP_L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_VN3CY1LbI_",
        "outputId": "4bf6aa51-1428-4899-deba-d13436d1f7fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Science from Scratch\n",
            "SECOND EDITION\n",
            "F i r s t  P r i n c i p l e s  w i t h  P y t h o n\n",
            "Joel GrusData Science from Scratch\n",
            "by Joel Grus\n",
            "Copyright © 2019 Joel Grus. All rights reserved.\n",
            "Printed in the United States of America.\n",
            "Published by O’Reilly Media, Inc. , 1005 Gravenstein Highway North,\n",
            "Sebastopol, CA 95472.\n",
            "O’Reilly books may be purchased for educational, business, or sales\n",
            "promotional use. Online editions are also available for most titles\n",
            "( http://or eilly .com ). For more information, contact our\n",
            "corporate/institutional sales department: 800-998-9938 or\n",
            "corporate@or eilly .com .\n",
            "Editor:  Michele Cronin\n",
            "Production Editor:  Deborah Baker\n",
            "Copy Editor:  Rachel Monaghan\n",
            "Proofreader:  Rachel Head\n",
            "Indexer:  Judy McConville\n",
            "Interior Designer:  David Futato\n",
            "Cover Designer:  Karen Montgomery\n",
            "Illustrator:  Rebecca Demarest\n",
            "April 2015:  First Edition\n",
            "May 2019:  Second Edition\n",
            "Revision History for the Second Edition2019-04-10:  First Release\n",
            "See http://oreilly .com/catalog/errata.csp?isbn=9781492041 139  for release\n",
            "details.\n",
            "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Data\n",
            "Science fr om Scratch , Second Edition, the cover image of a rock ptarmigan,\n",
            "and related trade dress are trademarks of O’Reilly Media, Inc.\n",
            "While the publisher and the author have used good faith ef forts to ensure\n",
            "that the information and instructions contained in this work are accurate, the\n",
            "publisher and the author disclaim all responsibility for errors or omissions,\n",
            "including without limitation responsibility for damages resulting from the\n",
            "use of or reliance on this work. Use of the information and instructions\n",
            "contained in this work is at your own risk. If any code samples or other\n",
            "technology this work contains or describes is subject to open source\n",
            "licenses or the intellectual property rights of others, it is your responsibility\n",
            "to ensure that your use thereof complies with such licenses and/or rights.\n",
            "978-1-492-041 13-9\n",
            "[LSI]Preface to the Second Edition\n",
            "I am exceptionally proud of the first edition of Data Science fr om Scratch .\n",
            "It turned out very much the book I wanted it to be. But several years of\n",
            "developments in data science, of progress in the Python ecosystem, and of\n",
            "personal growth as a developer and educator have changed  what I think a\n",
            "first book in data science should look like.\n",
            "In life, there are no do-overs. In writing, however , there are second editions.\n",
            "Accordingly , I’ve rewritten all the code and examples using Python 3.6 (and\n",
            "many of its newly introduced features, like type annotations). I’ve woven\n",
            "into the book an emphasis on writing clean code. I’ve replaced some of the\n",
            "first edition’ s toy examples with more realistic ones using “real” datasets.\n",
            "I’ve added new material on topics such as deep learning, statistics, and\n",
            "natural language processing, corresponding to things that today’ s data\n",
            "scientists are likely to be working with. (I’ve also removed some material\n",
            "that seems less relevant.) And I’ve gone over the book with a fine-toothed\n",
            "comb, fixing bugs, rewriting explanations that are less clear than they could\n",
            "be, and freshening up some of the jokes.\n",
            "The first edition was a great book, and this edition is even better . Enjoy!\n",
            "Joel Grus\n",
            "Seattle, W A\n",
            "2019\n",
            "C o n v e n t i o n s  U s e d  i n  T h i s  B o o k\n",
            "The following typographical conventions are used in this book:\n",
            "ItalicIndicates new terms, URLs, email addresses, filenames, and file\n",
            "extensions.\n",
            "Constant width\n",
            "Used for program listings, as well as within paragraphs to refer to\n",
            "program elements such as variable or function names, databases, data\n",
            "types, environment variables, statements, and keywords.\n",
            "Constant width bold\n",
            "Shows commands or other text that should be typed literally by the user .\n",
            "Constant width italic\n",
            "Shows text that should be replaced with user -supplied values or by\n",
            "values determined by context.\n",
            "T I P\n",
            "This element signifies a tip or suggestion.\n",
            "N O T E\n",
            "This element signifies a general note.\n",
            "W A R N I N G\n",
            "This element indicates a warning or caution.\n",
            "U s i n g  C o d e  E x a m p l e s\n",
            "Supplemental  material (code examples, exercises, etc.) is available for\n",
            "download at https://github.com/joelgrus/data-science-fr om-scratch .This book is here to help you get your job done. In general, if example code\n",
            "is of fered with this book, you may use it in your programs and\n",
            "documentation. Y ou do not need to contact us for permission unless you’re\n",
            "reproducing a significant portion of the code. For example, writing a\n",
            "program that uses several chunks of code from this book does not require\n",
            "permission. Selling or distributing a CD-ROM of examples from O’Reilly\n",
            "books does require permission. Answering a question by citing this book\n",
            "and quoting example code does not require permission. Incorporating a\n",
            "significant amount of example code from this book into your product’ s\n",
            "documentation does require permission.\n",
            "W e appreciate, but do not require, attribution. An attribution usually\n",
            "includes the title, author , publisher , and ISBN. For example: “ Data Science\n",
            "fr om Scratch , Second Edition, by Joel Grus (O’Reilly). Copyright 2019 Joel\n",
            "Grus, 978-1-492-041 13-9.”\n",
            "If you feel your use of code examples falls outside fair use or the\n",
            "permission given above, feel free to contact us at permissions@or eilly .com .\n",
            "O ’ R e i l l y  O n l i n e  L e a r n i n g\n",
            "N O T E\n",
            "For almost 40 years, O’Reilly Media  has provided technology and business training,\n",
            "knowledge, and insight to help companies succeed.\n",
            "Our unique network of experts and innovators share their knowledge and\n",
            "expertise through books, articles, conferences, and our online learning\n",
            "platform. O’Reilly’ s online learning platform gives you on-demand access\n",
            "to live training courses, in-depth learning paths, interactive coding\n",
            "environments, and a vast collection of text and video from O’Reilly and\n",
            "200+ other publishers. For more information, please visit http://or eilly .com .H o w  t o  C o n t a c t  U s\n",
            "Please address comments and questions concerning this book to the\n",
            "publisher:\n",
            "O’Reilly Media, Inc.\n",
            "1005 Gravenstein Highway North\n",
            "Sebastopol, CA 95472\n",
            "800-998-9938 (in the United States or Canada)\n",
            "707-829-0515 (international or local)\n",
            "707-829-0104 (fax)\n",
            "W e have a web page for this book, where we list errata, examples, and any\n",
            "additional information. Y ou can access this page at http://bit.ly/data-\n",
            "science-fr om-scratch-2e .\n",
            "T o comment or ask technical questions about this book, send email to\n",
            "bookquestions@or eilly .com .\n",
            "For more information about our books, courses, conferences, and news, see\n",
            "our website at http://www .or eilly .com .\n",
            "Find us on Facebook: http://facebook.com/or eilly\n",
            "Follow us on T witter: http://twitter .com/or eillymedia\n",
            "W atch us on Y ouT ube: http://www .youtube.com/or eillymedia\n",
            "A c k n o w l e d g m e n t s\n",
            "First, I would like to thank Mike Loukides for accepting my proposal for\n",
            "this book (and for insisting that I pare it down to a reasonable size). It\n",
            "would have been very easy for him to say , “Who’ s this person who keepsemailing me sample chapters, and how do I get him to go away?” I’m\n",
            "grateful he didn’ t. I’d also like to thank my editors, Michele Cronin and\n",
            "Marie Beaugureau, for guiding me through the publishing process and\n",
            "getting the book in a much better state than I ever would have gotten it on\n",
            "my own.\n",
            "I couldn’ t have written this book if I’d never learned data science, and I\n",
            "probably wouldn’ t have learned data science if not for the influence of Dave\n",
            "Hsu, Igor T atarinov , John Rauser , and the rest of the Farecast gang. (So long\n",
            "ago that it wasn’ t even called data science at the time!) The good folks at\n",
            "Coursera and DataT au deserve a lot of credit, too.\n",
            "I am also grateful to my beta readers and reviewers. Jay Fundling found a\n",
            "ton of mistakes and pointed out many unclear explanations, and the book is\n",
            "much better (and much more correct) thanks to him. Debashis Ghosh is a\n",
            "hero for sanity-checking all of my statistics. Andrew Musselman suggested\n",
            "toning down the “people who prefer R to Python are moral reprobates”\n",
            "aspect of the book, which I think ended up being pretty good advice. T rey\n",
            "Causey , R yan Matthew Balfanz, Loris Mularoni, Núria Pujol, Rob\n",
            "Jef ferson, Mary Pat Campbell, Zach Geary , Denise Mauldin, Jimmy\n",
            "O’Donnell, and W endy Grus also provided invaluable feedback. Thanks to\n",
            "everyone who read the first edition and helped make this a better book. Any\n",
            "errors remaining are of course my responsibility .\n",
            "I owe a lot to the T witter #datascience commmunity , for exposing me to a\n",
            "ton of new concepts, introducing me to a lot of great people, and making\n",
            "me feel like enough of an underachiever that I went out and wrote a book to\n",
            "compensate. Special thanks to T rey Causey (again), for (inadvertently)\n",
            "reminding me to include a chapter on linear algebra, and to Sean J. T aylor ,\n",
            "for (inadvertently) pointing out a couple of huge gaps in the “W orking with\n",
            "Data” chapter .\n",
            "Above all, I owe immense thanks to Ganga and Madeline. The only thing\n",
            "harder than writing a book is living with someone who’ s writing a book,\n",
            "and I couldn’ t have pulled it of f without their support.Preface to the First Edition\n",
            "D a t a  S c i e n c e\n",
            "Data scientist has been called “the sexiest job of the 21st century ,”\n",
            "presumably by someone who has never visited a fire station. Nonetheless,\n",
            "data science is a hot and growing field, and it doesn’ t take a great deal of\n",
            "sleuthing to find analysts breathlessly prognosticating that over the next 10\n",
            "years, we’ll need billions and billions more data scientists than we currently\n",
            "have.\n",
            "But what is data science? After all, we can’ t produce data scientists if we\n",
            "don’ t know what data science is. According to a V enn diagram  that is\n",
            "somewhat famous in the industry , data science lies at the intersection of:\n",
            "Hacking skills\n",
            "Math and statistics knowledge\n",
            "Substantive expertise\n",
            "Although I originally intended to write a book covering all three, I quickly\n",
            "realized that a thorough treatment of “substantive expertise” would require\n",
            "tens of thousands of pages. At that point, I decided to focus on the first two.\n",
            "My goal is to help you develop the hacking skills that you’ll need to get\n",
            "started doing data science. And my goal is to help you get comfortable with\n",
            "the mathematics and statistics that are at the core of data science.\n",
            "This is a somewhat heavy aspiration for a book. The best way to learn\n",
            "hacking skills is by hacking on things. By reading this book, you will get a\n",
            "good understanding of the way I hack on things, which may not necessarily\n",
            "be the best way for you to hack on things. Y ou will get a good\n",
            "understanding of some of the tools I use, which will not necessarily be the\n",
            "best tools for you to use. Y ou will get a good understanding of the way I\n",
            "approach data problems, which may not necessarily be the best way for youto approach data problems. The intent (and the hope) is that my examples\n",
            "will inspire you to try things your own way . All  the code and data from the\n",
            "book is available on GitHub  to get you started.\n",
            "Similarly , the best way to learn mathematics is by doing mathematics. This\n",
            "is emphatically not a math book, and for the most part, we won’ t be “doing\n",
            "mathematics.” However , you can’ t really do data science without some\n",
            "understanding of probability and statistics and linear algebra. This means\n",
            "that, where appropriate, we will dive into mathematical equations,\n",
            "mathematical intuition, mathematical axioms, and cartoon versions of big\n",
            "mathematical ideas. I hope that you won’ t be afraid to dive in with me.\n",
            "Throughout it all, I also hope to give you a sense that playing with data is\n",
            "fun, because, well, playing with data is fun! (Especially compared to some\n",
            "of the alternatives, like tax preparation or coal mining.)\n",
            "F r o m  S c r a t c h\n",
            "There are lots and lots of data science libraries, frameworks, modules, and\n",
            "toolkits that ef ficiently implement the most common (as well as the least\n",
            "common) data science algorithms and techniques. If you become a data\n",
            "scientist, you will become intimately familiar with NumPy , with scikit-\n",
            "learn, with pandas, and with a panoply of other libraries. They are great for\n",
            "doing data science. But they are also a good way to start doing data science\n",
            "without actually understanding data science.\n",
            "In this book, we will be approaching data science  from scratch. That means\n",
            "we’ll be building tools and implementing algorithms by hand in order to\n",
            "better understand them. I put a lot of thought into creating implementations\n",
            "and examples that are clear , well commented, and readable. In most cases,\n",
            "the tools we build will be illuminating but impractical. They will work well\n",
            "on small toy datasets but fall over on “web-scale” ones.\n",
            "Throughout the book, I will point you to libraries you might use to apply\n",
            "these techniques to lar ger datasets. But we won’ t be using them here.There is a healthy debate raging over the best language for learning data\n",
            "science. Many people believe it’ s the statistical programming language R.\n",
            "(W e call those people wr ong .) A few people suggest Java or Scala.\n",
            "However , in my opinion, Python is the obvious choice.\n",
            "Python  has several features that make it well suited for learning (and doing)\n",
            "data science:\n",
            "It’ s free.\n",
            "It’ s relatively simple to code in (and, in particular , to understand).\n",
            "It has lots of useful data science–related libraries.\n",
            "I am hesitant to call Python my favorite programming language. There are\n",
            "other languages I find more pleasant, better designed, or just more fun to\n",
            "code in. And yet pretty much every time I start a new data science project, I\n",
            "end up using Python. Every time I need to quickly prototype something that\n",
            "just works, I end up using Python. And every time I want to demonstrate\n",
            "data science concepts in a clear , easy-to-understand way , I end up using\n",
            "Python. Accordingly , this book uses Python.\n",
            "The goal of this book is not to teach you Python. (Although it is nearly\n",
            "certain that by reading this book you will learn some Python.) I’ll take you\n",
            "through a chapter -long crash course that highlights the features that are\n",
            "most important for our purposes, but if you know nothing about\n",
            "programming in Python (or about programming at all), then you might want\n",
            "to supplement this book with some sort of “Python for Beginners” tutorial.\n",
            "The remainder of our introduction to data science will take this same\n",
            "approach—going into detail where going into detail seems crucial or\n",
            "illuminating, at other times leaving details for you to figure out yourself (or\n",
            "look up on W ikipedia).\n",
            "Over the years, I’ve trained a number of data scientists. While not all of\n",
            "them have gone on to become world-changing data ninja rockstars, I’ve left\n",
            "them all better data scientists than I found them. And I’ve grown to believe\n",
            "that anyone who has some amount of mathematical aptitude and someamount of programming skill has the necessary raw materials to do data\n",
            "science. All she needs is an inquisitive mind, a willingness to work hard,\n",
            "and this book. Hence this book.Chapter 1. Introduction\n",
            "“Data! Data! Data!” he cried impatiently . “I can’ t make bricks without\n",
            "clay .”\n",
            "— Arthur Conan Doyle\n",
            "T h e  A s c e n d a n c e  o f  D a t a\n",
            "W e  live in a world that’ s drowning in data. W ebsites track every user ’ s\n",
            "every click. Y our smartphone is building up a record of your location and\n",
            "speed every second of every day . “Quantified selfers” wear pedometers-on-\n",
            "steroids that are always recording their heart rates, movement habits, diet,\n",
            "and sleep patterns. Smart cars collect driving habits, smart homes collect\n",
            "living habits, and smart marketers collect purchasing habits. The internet\n",
            "itself represents a huge graph of knowledge that contains (among other\n",
            "things) an enormous cross-referenced encyclopedia; domain-specific\n",
            "databases about movies, music, sports results, pinball machines, memes,\n",
            "and cocktails; and too many government statistics (some of them nearly\n",
            "true!) from too many governments to wrap your head around.\n",
            "Buried in these data are answers to countless questions that no one’ s ever\n",
            "thought to ask. In this book, we’ll learn how to find them.\n",
            "W h a t  I s  D a t a  S c i e n c e ?\n",
            "There’ s  a joke that says a data scientist is someone who knows more\n",
            "statistics than a computer scientist and more computer science than a\n",
            "statistician. (I didn’ t say it was a good joke.) In fact, some data scientists are\n",
            "—for all practical purposes—statisticians, while others are fairly\n",
            "indistinguishable from software engineers. Some are machine learning\n",
            "experts, while others couldn’ t machine-learn their way out of kinder garten.\n",
            "Some are PhDs with impressive publication records, while others havenever read an academic paper (shame on them, though). In short, pretty\n",
            "much no matter how you define data science, you’ll find practitioners for\n",
            "whom the definition is totally , absolutely wrong.\n",
            "Nonetheless, we won’ t let that stop us from trying. W e’ll say that a data\n",
            "scientist is someone who extracts insights from messy data. T oday’ s world\n",
            "is full of people trying to turn data into insight.\n",
            "For  instance, the dating site OkCupid asks its members to answer thousands\n",
            "of questions in order to find the most appropriate matches for them. But it\n",
            "also analyzes these results to figure out innocuous-sounding questions you\n",
            "can ask someone to find out how likely someone is to sleep with you on the\n",
            "first date .\n",
            "Facebook asks you to list your hometown and your current location,\n",
            "ostensibly to make it easier for your friends to find and connect with you.\n",
            "But it also analyzes these locations to identify global migration patterns  and\n",
            "where the fanbases of dif ferent football teams live .\n",
            "As a lar ge retailer , T ar get tracks your purchases and interactions, both\n",
            "online and in-store. And it uses the data to predictively model  which of its\n",
            "customers are pregnant, to better market baby-related purchases to them.\n",
            "In 2012, the Obama campaign employed dozens of data scientists who data-\n",
            "mined and experimented their way to identifying voters who needed extra\n",
            "attention, choosing optimal donor -specific fundraising appeals and\n",
            "programs, and focusing get-out-the-vote ef forts where they were most likely\n",
            "to be useful. And in 2016 the T rump campaign tested a staggering variety of\n",
            "online ads  and analyzed the data to find what worked and what didn’ t.\n",
            "Now , before you start feeling too jaded: some data scientists also\n",
            "occasionally use their skills for good— using data to make government\n",
            "more ef fective , to help the homeless , and to improve public health . But it\n",
            "certainly won’ t hurt your career if you like figuring out the best way to get\n",
            "people to click on advertisements.\n",
            "M o t i v a t i n g  H y p o t h e t i c a l :  D a t a S c i e n c e s t e rCongratulations! Y ou’ve just been hired to lead the data science ef forts at\n",
            "DataSciencester , the  social network for data scientists.\n",
            "N O T E\n",
            "When I wrote the first edition of this book, I thought that “a social network for data\n",
            "scientists” was a fun, silly hypothetical. Since then people have actually created social\n",
            "networks for data scientists, and have raised much more money from venture capitalists\n",
            "than I made from my book. Most likely there is a valuable lesson here about silly data\n",
            "science hypotheticals and/or book publishing.\n",
            "Despite being for  data scientists, DataSciencester has never actually\n",
            "invested in building its own data science practice. (In fairness,\n",
            "DataSciencester has never really invested in building its product either .)\n",
            "That will be your job! Throughout the book, we’ll be learning about data\n",
            "science concepts by solving problems that you encounter at work.\n",
            "Sometimes we’ll look at data explicitly supplied by users, sometimes we’ll\n",
            "look at data generated through their interactions with the site, and\n",
            "sometimes we’ll even look at data from experiments that we’ll design.\n",
            "And because DataSciencester has a strong “not-invented-here” mentality ,\n",
            "we’ll be building our own tools from scratch. At the end, you’ll have a\n",
            "pretty solid understanding of the fundamentals of data science. And you’ll\n",
            "be ready to apply your skills at a company with a less shaky premise, or to\n",
            "any other problems that happen to interest you.\n",
            "W elcome aboard, and good luck! (Y ou’re allowed to wear jeans on Fridays,\n",
            "and the bathroom is down the hall on the right.)\n",
            "Finding Key Connectors\n",
            "It’ s  your first day on the job at DataSciencester , and the VP of Networking\n",
            "is full of questions about your users. Until now he’ s had no one to ask, so\n",
            "he’ s very excited to have you aboard.In particular , he wants you to identify who the “key connectors” are among\n",
            "data scientists. T o this end, he gives you a dump of the entire\n",
            "DataSciencester network. (In real life, people don’ t typically hand you the\n",
            "data you need. Chapter 9  is devoted to getting data.)\n",
            "What does this data dump look like? It consists of a list of users, each\n",
            "represented by a dict  that contains that user ’ s id  (which is a number) and\n",
            "name  (which, in one of the great cosmic coincidences, rhymes with the\n",
            "user ’ s id ):\n",
            "users = [ \n",
            "    { \"id\": 0, \"name\": \"Hero\" }, \n",
            "    { \"id\": 1, \"name\": \"Dunn\" }, \n",
            "    { \"id\": 2, \"name\": \"Sue\" }, \n",
            "    { \"id\": 3, \"name\": \"Chi\" }, \n",
            "    { \"id\": 4, \"name\": \"Thor\" }, \n",
            "    { \"id\": 5, \"name\": \"Clive\" }, \n",
            "    { \"id\": 6, \"name\": \"Hicks\" }, \n",
            "    { \"id\": 7, \"name\": \"Devin\" }, \n",
            "    { \"id\": 8, \"name\": \"Kate\" }, \n",
            "    { \"id\": 9, \"name\": \"Klein\" } \n",
            "]\n",
            "He also gives you the “friendship” data, represented as a list of pairs of IDs:\n",
            "friendship_pairs = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4), \n",
            "                    (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n",
            "For example, the tuple (0, 1)  indicates that the data scientist with id  0\n",
            "(Hero) and the data scientist with id  1 (Dunn) are friends. The network is\n",
            "illustrated in Figure 1-1 .Figur e 1-1. The DataSciencester network\n",
            "Having friendships represented as a list of pairs is not the easiest way to\n",
            "work with them. T o find all the friendships for user 1, you have to iterate\n",
            "over every pair looking for pairs containing 1. If you had a lot of pairs, this\n",
            "would take a long time.\n",
            "Instead, let’ s create a dict  where the keys are user id s and the values are\n",
            "lists of friend id s. (Looking things up in a dict  is very fast.)\n",
            "N O T E\n",
            "Don’ t get too hung up on the details of the code right now . In Chapter 2 , I’ll take you\n",
            "through a crash course in Python. For now just try to get the general flavor of what\n",
            "we’re doing.\n",
            "W e’ll still have to look at every pair to create the dict , but we only have to\n",
            "do that once, and we’ll get cheap lookups after that:\n",
            "# Initialize the dict with an empty list for each user id: \n",
            "friendships = {user[\"id\"]: [] for user in users} \n",
            " \n",
            "# And loop over the friendship pairs to populate it: \n",
            "for i, j in friendship_pairs: \n",
            "    friendships[i].append(j)  # Add j as a friend of user i \n",
            "    friendships[j].append(i)  # Add i as a friend of user j\n",
            "Now that we have the friendships in a dict , we can easily ask questions of\n",
            "our graph, like “What’ s the average number of connections?”First we find the total  number of connections, by summing up the lengths of\n",
            "all the friends  lists:\n",
            "def number_of_friends(user): \n",
            "    \"\"\"How many friends does _user_ have?\"\"\" \n",
            "    user_id = user[\"id\"] \n",
            "    friend_ids = friendships[user_id] \n",
            "    return len(friend_ids) \n",
            " \n",
            "total_connections = sum(number_of_friends(user) \n",
            "                        for user in users)        # 24\n",
            "And then we just divide by the number of users:\n",
            "num_users = len(users)                            # length of the users list \n",
            "avg_connections = total_connections / num_users   # 24 / 10 == 2.4\n",
            "It’ s also easy to find the most connected people—they’re the people who\n",
            "have the lar gest numbers of friends.\n",
            "Since there aren’ t very many users, we can simply sort them from “most\n",
            "friends” to “least friends”:\n",
            "# Create a list (user_id, number_of_friends). \n",
            "num_friends_by_id = [(user[\"id\"], number_of_friends(user)) \n",
            "                     for user in users] \n",
            " \n",
            "num_friends_by_id.sort(                                # Sort the list \n",
            "       key=lambda id_and_friends: id_and_friends[1],   # by num_friends \n",
            "       reverse=True)                                   # largest to smallest \n",
            " \n",
            "# Each pair is (user_id, num_friends): \n",
            "# [(1, 3), (2, 3), (3, 3), (5, 3), (8, 3), \n",
            "#  (0, 2), (4, 2), (6, 2), (7, 2), (9, 1)]\n",
            "One  way to think of what we’ve done is as a way of identifying people who\n",
            "are somehow central to the network. In fact, what we’ve just computed is\n",
            "the network metric degr ee centrality  ( Figure 1-2 ).Figur e 1-2. The DataSciencester network sized by degr ee\n",
            "This has the virtue of being pretty easy to calculate, but it doesn’ t always\n",
            "give the results you’d want or expect. For example, in the DataSciencester\n",
            "network Thor ( id  4) only has two connections, while Dunn ( id  1) has three.\n",
            "Y et when we look at the network, it intuitively seems like Thor should be\n",
            "more central. In Chapter 22 , we’ll investigate networks in more detail, and\n",
            "we’ll look at more complex notions of centrality that may or may not\n",
            "accord better with our intuition.\n",
            "Data Scientists Y ou May Know\n",
            "While  you’re still filling out new-hire paperwork, the VP of Fraternization\n",
            "comes by your desk. She wants to encourage more connections among your\n",
            "members, and she asks you to design a “Data Scientists Y ou May Know”\n",
            "suggester .\n",
            "Y our first instinct is to suggest that users might know the friends of their\n",
            "friends. So you write some code to iterate over their friends and collect the\n",
            "friends’ friends:\n",
            "def foaf_ids_bad(user): \n",
            "    \"\"\"foaf is short for \"friend of a friend\" \"\"\" \n",
            "    return [foaf_id \n",
            "            for friend_id in friendships[user[\"id\"]] \n",
            "            for foaf_id in friendships[friend_id]]\n",
            "When we call this on users[0]  (Hero), it produces:[0, 2, 3, 0, 1, 3]\n",
            "It includes user 0 twice, since Hero is indeed friends with both of his\n",
            "friends. It includes users 1 and 2, although they are both friends with Hero\n",
            "already . And it includes user 3 twice, as Chi is reachable through two\n",
            "dif ferent friends:\n",
            "print(friendships[0])  # [1, 2] \n",
            "print(friendships[1])  # [0, 2, 3] \n",
            "print(friendships[2])  # [0, 1, 3]\n",
            "Knowing that people are friends of friends in multiple ways seems like\n",
            "interesting information, so maybe instead we should produce a count  of\n",
            "mutual friends. And we should probably exclude people already known to\n",
            "the user:\n",
            "from collections import Counter                   # not loaded by default \n",
            " \n",
            "def friends_of_friends(user): \n",
            "    user_id = user[\"id\"] \n",
            "    return Counter( \n",
            "        foaf_id \n",
            "        for friend_id in friendships[user_id]     # For each of my friends, \n",
            "        for foaf_id in friendships[friend_id]     # find their friends \n",
            "        if foaf_id != user_id                     # who aren't me \n",
            "        and foaf_id not in friendships[user_id]   # and aren't my friends. \n",
            "    ) \n",
            " \n",
            " \n",
            "print(friends_of_friends(users[3]))               # Counter({0: 2, 5: 1})\n",
            "This correctly tells Chi ( id  3) that she has two mutual friends with Hero ( id\n",
            "0) but only one mutual friend with Clive ( id  5).\n",
            "As a data scientist, you know that you also might enjoy meeting users with\n",
            "similar interests. (This is a good example of the “substantive expertise”\n",
            "aspect of data science.) After asking around, you manage to get your hands\n",
            "on this data, as a list of pairs (user_id, interest) :interests = [ \n",
            "    (0, \"Hadoop\"), (0, \"Big Data\"), (0, \"HBase\"), (0, \"Java\"), \n",
            "    (0, \"Spark\"), (0, \"Storm\"), (0, \"Cassandra\"), \n",
            "    (1, \"NoSQL\"), (1, \"MongoDB\"), (1, \"Cassandra\"), (1, \"HBase\"), \n",
            "    (1, \"Postgres\"), (2, \"Python\"), (2, \"scikit-learn\"), (2, \"scipy\"), \n",
            "    (2, \"numpy\"), (2, \"statsmodels\"), (2, \"pandas\"), (3, \"R\"), (3, \"Python\"), \n",
            "    (3, \"statistics\"), (3, \"regression\"), (3, \"probability\"), \n",
            "    (4, \"machine learning\"), (4, \"regression\"), (4, \"decision trees\"), \n",
            "    (4, \"libsvm\"), (5, \"Python\"), (5, \"R\"), (5, \"Java\"), (5, \"C++\"), \n",
            "    (5, \"Haskell\"), (5, \"programming languages\"), (6, \"statistics\"), \n",
            "    (6, \"probability\"), (6, \"mathematics\"), (6, \"theory\"), \n",
            "    (7, \"machine learning\"), (7, \"scikit-learn\"), (7, \"Mahout\"), \n",
            "    (7, \"neural networks\"), (8, \"neural networks\"), (8, \"deep learning\"), \n",
            "    (8, \"Big Data\"), (8, \"artificial intelligence\"), (9, \"Hadoop\"), \n",
            "    (9, \"Java\"), (9, \"MapReduce\"), (9, \"Big Data\") \n",
            "]\n",
            "For example, Hero ( id  0) has no friends in common with Klein ( id  9), but\n",
            "they share interests in Java and big data.\n",
            "It’ s easy to build a function that finds users with a certain interest:\n",
            "def data_scientists_who_like(target_interest): \n",
            "    \"\"\"Find the ids of all users who like the target interest.\"\"\" \n",
            "    return [user_id \n",
            "            for user_id, user_interest in interests \n",
            "            if user_interest == target_interest]\n",
            "This works, but it has to examine the whole list of interests for every\n",
            "search. If we have a lot of users and interests (or if we just want to do a lot\n",
            "of searches), we’re probably better of f building an index from interests to\n",
            "users:\n",
            "from collections import defaultdict \n",
            " \n",
            "# Keys are interests, values are lists of user_ids with that interest \n",
            "user_ids_by_interest = defaultdict(list) \n",
            " \n",
            "for user_id, interest in interests: \n",
            "    user_ids_by_interest[interest].append(user_id)\n",
            "And another from users to interests:# Keys are user_ids, values are lists of interests for that user_id. \n",
            "interests_by_user_id = defaultdict(list) \n",
            " \n",
            "for user_id, interest in interests: \n",
            "    interests_by_user_id[user_id].append(interest)\n",
            "Now it’ s easy to find who has the most interests in common with a given\n",
            "user:\n",
            "Iterate over the user ’ s interests.\n",
            "For each interest, iterate over the other users with that interest.\n",
            "Keep count of how many times we see each other user .\n",
            "In code:\n",
            "def most_common_interests_with(user): \n",
            "    return Counter( \n",
            "        interested_user_id \n",
            "        for interest in interests_by_user_id[user[\"id\"]] \n",
            "        for interested_user_id in user_ids_by_interest[interest] \n",
            "        if interested_user_id != user[\"id\"] \n",
            "    )\n",
            "W e could then use this to build a richer “Data Scientists Y ou May Know”\n",
            "feature based on a combination of mutual friends and mutual interests.\n",
            "W e’ll explore these kinds of applications in Chapter 23 .\n",
            "Salaries and Experience\n",
            "Right  as you’re about to head to lunch, the VP of Public Relations asks if\n",
            "you can provide some fun facts about how much data scientists earn. Salary\n",
            "data is of course sensitive, but he manages to provide you an anonymous\n",
            "dataset containing each user ’ s salary  (in dollars) and tenure  as a data\n",
            "scientist (in years):\n",
            "salaries_and_tenures = [(83000, 8.7), (88000, 8.1), \n",
            "                        (48000, 0.7), (76000, 6), \n",
            "                        (69000, 6.5), (76000, 7.5),(60000, 2.5), (83000, 10), \n",
            "                        (48000, 1.9), (63000, 4.2)]\n",
            "The natural first step is to plot the data (which we’ll see how to do in\n",
            "Chapter 3 ). Y ou can see the results in Figure 1-3 .\n",
            "Figur e 1-3. Salary by years of experience\n",
            "It seems clear that people with more experience tend to earn more. How can\n",
            "you turn this into a fun fact? Y our first idea is to look at the average salary\n",
            "for each tenure:\n",
            "# Keys are years, values are lists of the salaries for each tenure. \n",
            "salary_by_tenure = defaultdict(list) \n",
            " \n",
            "for salary, tenure in salaries_and_tenures: \n",
            "    salary_by_tenure[tenure].append(salary) \n",
            " \n",
            "# Keys are years, each value is average salary for that tenure. \n",
            "average_salary_by_tenure = {tenure: sum(salaries) / len(salaries) \n",
            "    for tenure, salaries in salary_by_tenure.items() \n",
            "}\n",
            "This turns out to be not particularly useful, as none of the users have the\n",
            "same tenure, which means we’re just reporting the individual users’\n",
            "salaries:\n",
            "{0.7: 48000.0, \n",
            " 1.9: 48000.0, \n",
            " 2.5: 60000.0, \n",
            " 4.2: 63000.0, \n",
            " 6: 76000.0, \n",
            " 6.5: 69000.0, \n",
            " 7.5: 76000.0, \n",
            " 8.1: 88000.0, \n",
            " 8.7: 83000.0, \n",
            " 10: 83000.0}\n",
            "It might be more helpful to bucket the tenures:\n",
            "def tenure_bucket(tenure): \n",
            "    if tenure < 2: \n",
            "        return \"less than two\" \n",
            "    elif tenure < 5: \n",
            "        return \"between two and five\" \n",
            "    else: \n",
            "        return \"more than five\"\n",
            "Then we can group together the salaries corresponding to each bucket:\n",
            "# Keys are tenure buckets, values are lists of salaries for that bucket. \n",
            "salary_by_tenure_bucket = defaultdict(list) \n",
            " \n",
            "for salary, tenure in salaries_and_tenures: \n",
            "    bucket = tenure_bucket(tenure) \n",
            "    salary_by_tenure_bucket[bucket].append(salary)\n",
            "And finally compute the average salary for each group:\n",
            "# Keys are tenure buckets, values are average salary for that bucket. \n",
            "average_salary_by_bucket = {tenure_bucket: sum(salaries) / len(salaries) \n",
            "  for tenure_bucket, salaries in salary_by_tenure_bucket.items() \n",
            "}\n",
            "Which is more interesting:\n",
            "{'between two and five': 61500.0, \n",
            " 'less than two': 48000.0, \n",
            " 'more than five': 79166.66666666667}\n",
            "And you have your soundbite: “Data scientists with more than five years’\n",
            "experience earn 65% more than data scientists with little or no experience!”\n",
            "But we chose the buckets in a pretty arbitrary way . What we’d really like is\n",
            "to make some statement about the salary ef fect—on average—of having an\n",
            "additional year of experience. In addition to making for a snappier fun fact,\n",
            "this allows us to make pr edictions  about salaries that we don’ t know . W e’ll\n",
            "explore this idea in Chapter 14 .\n",
            "Paid Accounts\n",
            "When  you get back to your desk, the VP of Revenue is waiting for you. She\n",
            "wants to better understand which users pay for accounts and which don’ t.\n",
            "(She knows their names, but that’ s not particularly actionable information.)\n",
            "Y ou notice that there seems to be a correspondence between years of\n",
            "experience and paid accounts:\n",
            "0.7  paid \n",
            "1.9  unpaid \n",
            "2.5  paid \n",
            "4.2  unpaid \n",
            "6.0  unpaid \n",
            "6.5  unpaid \n",
            "7.5  unpaid \n",
            "8.1  unpaid \n",
            "8.7  paid \n",
            "10.0 paidUsers with very few and very many years of experience tend to pay; users\n",
            "with average amounts of experience don’ t. Accordingly , if you wanted to\n",
            "create a model—though this is definitely not enough data to base a model\n",
            "on—you might try to predict “paid” for users with very few and very many\n",
            "years of experience, and “unpaid” for users with middling amounts of\n",
            "experience:\n",
            "def predict_paid_or_unpaid(years_experience): \n",
            "  if years_experience < 3.0: \n",
            "    return \"paid\" \n",
            "  elif years_experience < 8.5: \n",
            "    return \"unpaid\" \n",
            "  else: \n",
            "    return \"paid\"\n",
            "Of course, we totally eyeballed the cutof fs.\n",
            "W ith more data (and more mathematics), we could build a model predicting\n",
            "the likelihood that a user would pay based on his years of experience. W e’ll\n",
            "investigate this sort of problem in Chapter 16 .\n",
            "T opics of Interest\n",
            "As  you’re wrapping up your first day , the VP of Content Strategy asks you\n",
            "for data about what topics users are most interested in, so that she can plan\n",
            "out her blog calendar accordingly . Y ou already have the raw data from the\n",
            "friend-suggester project:\n",
            "interests = [ \n",
            "    (0, \"Hadoop\"), (0, \"Big Data\"), (0, \"HBase\"), (0, \"Java\"), \n",
            "    (0, \"Spark\"), (0, \"Storm\"), (0, \"Cassandra\"), \n",
            "    (1, \"NoSQL\"), (1, \"MongoDB\"), (1, \"Cassandra\"), (1, \"HBase\"), \n",
            "    (1, \"Postgres\"), (2, \"Python\"), (2, \"scikit-learn\"), (2, \"scipy\"), \n",
            "    (2, \"numpy\"), (2, \"statsmodels\"), (2, \"pandas\"), (3, \"R\"), (3, \"Python\"), \n",
            "    (3, \"statistics\"), (3, \"regression\"), (3, \"probability\"), \n",
            "    (4, \"machine learning\"), (4, \"regression\"), (4, \"decision trees\"), \n",
            "    (4, \"libsvm\"), (5, \"Python\"), (5, \"R\"), (5, \"Java\"), (5, \"C++\"), \n",
            "    (5, \"Haskell\"), (5, \"programming languages\"), (6, \"statistics\"), \n",
            "    (6, \"probability\"), (6, \"mathematics\"), (6, \"theory\"), \n",
            "    (7, \"machine learning\"), (7, \"scikit-learn\"), (7, \"Mahout\"), \n",
            "    (7, \"neural networks\"), (8, \"neural networks\"), (8, \"deep learning\"),(8, \"Big Data\"), (8, \"artificial intelligence\"), (9, \"Hadoop\"), \n",
            "    (9, \"Java\"), (9, \"MapReduce\"), (9, \"Big Data\") \n",
            "]\n",
            "One simple (if not particularly exciting) way to find the most popular\n",
            "interests is to count the words:\n",
            "1 . Lowercase each interest (since dif ferent users may or may not\n",
            "capitalize their interests).\n",
            "2 . Split it into words.\n",
            "3 . Count the results.\n",
            "In code:\n",
            "words_and_counts = Counter(word \n",
            "                           for user, interest in interests \n",
            "                           for word in interest.lower().split())\n",
            "This makes it easy to list out the words that occur more than once:\n",
            "for word, count in words_and_counts.most_common(): \n",
            "    if count > 1: \n",
            "        print(word, count)\n",
            "which gives the results you’d expect (unless you expect “scikit-learn” to get\n",
            "split into two words, in which case it doesn’ t give the results you expect):\n",
            "learning 3 \n",
            "java 3 \n",
            "python 3 \n",
            "big 3 \n",
            "data 3 \n",
            "hbase 2 \n",
            "regression 2 \n",
            "cassandra 2 \n",
            "statistics 2 \n",
            "probability 2 \n",
            "hadoop 2 \n",
            "networks 2 \n",
            "machine 2neural 2 \n",
            "scikit-learn 2 \n",
            "r 2\n",
            "W e’ll look at more sophisticated ways to extract topics from data in\n",
            "Chapter 21 .\n",
            "Onward\n",
            "It’ s been a successful first day! Exhausted, you slip out of the building\n",
            "before anyone can ask you for anything else. Get a good night’ s rest,\n",
            "because tomorrow is new employee orientation. (Y es, you went through a\n",
            "full day of work befor e  new employee orientation. T ake it up with HR.)Chapter 2. A Crash Course in\n",
            "Python\n",
            "People ar e still crazy about Python after twenty-five years, which I find\n",
            "har d to believe.\n",
            "— Michael Palin\n",
            "All new employees at DataSciencester are required to go through new\n",
            "employee orientation, the most interesting part of which is a crash course in\n",
            "Python.\n",
            "This is not a comprehensive Python tutorial but instead is intended to\n",
            "highlight the parts of the language that will be most important to us (some\n",
            "of which are often not the focus of Python tutorials). If you have never used\n",
            "Python before, you probably want to supplement this with some sort of\n",
            "beginner tutorial.\n",
            "T h e  Z e n  o f  P y t h o n\n",
            "Python  has a somewhat Zen description of its design principles , which you\n",
            "can also find inside the Python interpreter itself by typing “import this.”\n",
            "One of the most discussed of these is:\n",
            "Ther e should be one—and pr eferably only one—obvious way to do it.\n",
            "Code written in accordance with this “obvious” way (which may not be\n",
            "obvious at all to a newcomer) is often described as “Pythonic.” Although\n",
            "this is not a book about Python, we will occasionally contrast Pythonic and\n",
            "non-Pythonic ways of accomplishing the same things, and we will generally\n",
            "favor Pythonic solutions to our problems.\n",
            "Several others touch on aesthetics:Beautiful is better than ugly . Explicit is better than implicit. Simple is\n",
            "better than complex.\n",
            "and represent ideals that we will strive for in our code.\n",
            "G e t t i n g  P y t h o n\n",
            "N O T E\n",
            "As instructions about how to install things can change, while printed books cannot, up-\n",
            "to-date instructions on how to install Python can be found in the book’ s GitHub repo .\n",
            "If the ones printed here don’ t work for you, check those.\n",
            "Y ou  can download Python from Python.or g . But if you don’ t already have\n",
            "Python, I recommend instead installing the Anaconda  distribution, which\n",
            "already includes most of the libraries that you need to do data science.\n",
            "When I wrote the first version of Data Science fr om Scratch , Python 2.7\n",
            "was still the preferred version of most data scientists. Accordingly , the first\n",
            "edition of the book was based on Python 2.7.\n",
            "In  the last several years, however , pretty much everyone who counts has\n",
            "migrated to Python 3. Recent versions of Python have many features that\n",
            "make it easier to write clean code, and we’ll be taking ample advantage of\n",
            "features that are only available in Python 3.6 or later . This means that you\n",
            "should get Python 3.6 or later . (In addition, many useful libraries are ending\n",
            "support for Python 2.7, which is another reason to switch.)\n",
            "V i r t u a l  E n v i r o n m e n t s\n",
            "Starting  in the next chapter , we’ll be using the matplotlib library to generate\n",
            "plots and charts. This library is not a core part of Python; you have to install\n",
            "it yourself. Every data science project you do will require some\n",
            "combination of external libraries, sometimes with specific versions thatdif fer from the specific versions you used for other projects. If you were to\n",
            "have a single Python installation, these libraries would conflict and cause\n",
            "you all sorts of problems.\n",
            "The standard solution is to use virtual envir onments , which are sandboxed\n",
            "Python environments that maintain their own versions of Python libraries\n",
            "(and, depending on how you set up the environment, of Python itself).\n",
            "I recommended you install the Anaconda Python distribution, so in this\n",
            "section I’m going to explain how Anaconda’ s environments work. If you\n",
            "are not using Anaconda, you can either use the built-in venv  module or\n",
            "install virtualenv . In which case you should follow their instructions\n",
            "instead.\n",
            "T o create an (Anaconda) virtual environment, you just do the following:\n",
            "# create a Python 3.6 environment named \"dsfs\" \n",
            "conda create -n dsfs python=3.6\n",
            "Follow the prompts, and you’ll have a virtual environment called “dsfs,”\n",
            "with the instructions:\n",
            "# \n",
            "# To activate this environment, use: \n",
            "# > source activate dsfs \n",
            "# \n",
            "# To deactivate an active environment, use: \n",
            "# > source deactivate \n",
            "#\n",
            "As indicated, you then activate the environment using:\n",
            "source activate dsfs\n",
            "at which point your command prompt should change to indicate the active\n",
            "environment. On my MacBook the prompt now looks like:\n",
            "(dsfs) ip-10-0-0-198:~ joelg$As long as this environment is active, any libraries you install will be\n",
            "installed only in the dsfs environment. Once you finish this book and go on\n",
            "to your own projects, you should create your own environments for them.\n",
            "Now  that you have your environment, it’ s worth installing IPython , which is\n",
            "a full-featured Python shell:\n",
            "python -m pip install ipython\n",
            "N O T E\n",
            "Anaconda  comes with its own package manager , conda , but you can also just use the\n",
            "standard Python package manager pip , which is what we’ll be doing.\n",
            "The rest of this book will assume that you have created and activated such a\n",
            "Python 3.6 virtual environment (although you can call it whatever you\n",
            "want), and later chapters may rely on the libraries that I told you to install in\n",
            "earlier chapters.\n",
            "As a matter of good discipline, you should always work in a virtual\n",
            "environment, and never using the “base” Python installation.\n",
            "W h i t e s p a c e  F o r m a t t i n g\n",
            "Many  languages use curly braces to delimit blocks of code. Python uses\n",
            "indentation:\n",
            "# The pound sign marks the start of a comment. Python itself \n",
            "# ignores the comments, but they're helpful for anyone reading the code. \n",
            "for i in [1, 2, 3, 4, 5]: \n",
            "    print(i)                    # first line in \"for i\" block \n",
            "    for j in [1, 2, 3, 4, 5]: \n",
            "        print(j)                # first line in \"for j\" block \n",
            "        print(i + j)            # last line in \"for j\" block \n",
            "    print(i)                    # last line in \"for i\" block \n",
            "print(\"done looping\")This makes Python code very readable, but it also means that you have to\n",
            "be very careful with your formatting.\n",
            "W A R N I N G\n",
            "Programmers  will often ar gue over whether to use tabs or spaces for indentation. For\n",
            "many languages it doesn’ t matter that much; however , Python considers tabs and spaces\n",
            "dif ferent indentation and will not be able to run your code if you mix the two. When\n",
            "writing Python you should always use spaces, never tabs. (If you write code in an editor\n",
            "you can configure it so that the T ab key just inserts spaces.)\n",
            "Whitespace is ignored inside parentheses and brackets, which can be\n",
            "helpful for long-winded computations:\n",
            "long_winded_computation = (1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 + 12 + \n",
            "                           13 + 14 + 15 + 16 + 17 + 18 + 19 + 20)\n",
            "and for making code easier to read:\n",
            "list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \n",
            " \n",
            "easier_to_read_list_of_lists = [[1, 2, 3], \n",
            "                                [4, 5, 6], \n",
            "                                [7, 8, 9]]\n",
            "Y ou can also use a backslash to indicate that a statement continues onto the\n",
            "next line, although we’ll rarely do this:\n",
            "two_plus_three = 2 + \\ \n",
            "                 3\n",
            "One consequence of whitespace formatting is that it can be hard to copy and\n",
            "paste code into the Python shell. For example, if you tried to paste the code:\n",
            "for i in [1, 2, 3, 4, 5]: \n",
            " \n",
            "    # notice the blank line \n",
            "    print(i)into the ordinary Python shell, you would receive the complaint:\n",
            "IndentationError: expected an indented block\n",
            "because the interpreter thinks the blank line signals the end of the for\n",
            "loop’ s block.\n",
            "IPython has a magic function called %paste , which correctly pastes\n",
            "whatever is on your clipboard, whitespace and all. This alone is a good\n",
            "reason to use IPython.\n",
            "M o d u l e s\n",
            "Certain  features of Python are not loaded by default. These include both\n",
            "features that are included as part of the language as well as third-party\n",
            "features that you download yourself. In order to use these features, you’ll\n",
            "need to import  the modules that contain them.\n",
            "One approach is to simply import  the module itself:\n",
            "import re \n",
            "my_regex = re.compile(\"[0-9]+\", re.I)\n",
            "Here, re  is the module containing functions and constants for working with\n",
            "regular expressions. After this type of import  you must prefix those\n",
            "functions with re.  in order to access them.\n",
            "If you already had a dif ferent re  in your code, you could use an alias:\n",
            "import re as regex \n",
            "my_regex = regex.compile(\"[0-9]+\", regex.I)\n",
            "Y ou might also do this if your module has an unwieldy name or if you’re\n",
            "going to be typing it a lot. For example, a standard convention when\n",
            "visualizing data with matplotlib is:import matplotlib.pyplot as plt \n",
            " \n",
            "plt.plot(...)\n",
            "If you need a few specific values from a module, you can import them\n",
            "explicitly and use them without qualification:\n",
            "from collections import defaultdict, Counter \n",
            "lookup = defaultdict(int) \n",
            "my_counter = Counter()\n",
            "If you were a bad person, you could import the entire contents of a module\n",
            "into your namespace, which might inadvertently overwrite variables you’ve\n",
            "already defined:\n",
            "match = 10 \n",
            "from re import *    # uh oh, re has a match function \n",
            "print(match)        # \"<function match at 0x10281e6a8>\"\n",
            "However , since you are not a bad person, you won’ t ever do this.\n",
            "F u n c t i o n s\n",
            "A  function is a rule for taking zero or more inputs and returning a\n",
            "corresponding output. In Python, we typically define functions using def :\n",
            "def double(x): \n",
            "    \"\"\" \n",
            "    This is where you put an optional docstring that explains what the \n",
            "    function does. For example, this function multiplies its input by 2. \n",
            "    \"\"\" \n",
            "    return x * 2\n",
            "Python  functions are first-class , which means that we can assign them to\n",
            "variables and pass them into functions just like any other ar guments:\n",
            "def apply_to_one(f): \n",
            "    \"\"\"Calls the function f with 1 as its argument\"\"\" \n",
            "    return f(1)my_double = double             # refers to the previously defined function \n",
            "x = apply_to_one(my_double)    # equals 2\n",
            "It is also easy to create short anonymous functions, or lambdas :\n",
            "y = apply_to_one(lambda x: x + 4)      # equals 5\n",
            "Y ou can assign lambdas to variables, although most people will tell you that\n",
            "you should just use def  instead:\n",
            "another_double = lambda x: 2 * x       # don't do this \n",
            " \n",
            "def another_double(x): \n",
            "    \"\"\"Do this instead\"\"\" \n",
            "    return 2 * x\n",
            "Function parameters can also be given default ar guments, which only need\n",
            "to be specified when you want a value other than the default:\n",
            "def my_print(message = \"my default message\"): \n",
            "    print(message) \n",
            " \n",
            "my_print(\"hello\")   # prints 'hello' \n",
            "my_print()          # prints 'my default message'\n",
            "It is sometimes useful to specify ar guments by name:\n",
            "def full_name(first = \"What's-his-name\", last = \"Something\"): \n",
            "    return first + \" \" + last \n",
            " \n",
            "full_name(\"Joel\", \"Grus\")     # \"Joel Grus\" \n",
            "full_name(\"Joel\")             # \"Joel Something\" \n",
            "full_name(last=\"Grus\")        # \"What's-his-name Grus\"\n",
            "W e will be creating many , many functions.\n",
            "S t r i n g sStrings  can be delimited by single or double quotation marks (but the\n",
            "quotes have to match):\n",
            "single_quoted_string = 'data science' \n",
            "double_quoted_string = \"data science\"\n",
            "Python uses backslashes to encode special characters. For example:\n",
            "tab_string = \"\\t\"       # represents the tab character \n",
            "len(tab_string)         # is 1\n",
            "If you want backslashes as backslashes (which you might in W indows\n",
            "directory names or in regular expressions), you  can create raw  strings using\n",
            "r\"\" :\n",
            "not_tab_string = r\"\\t\"  # represents the characters '\\' and 't' \n",
            "len(not_tab_string)     # is 2\n",
            "Y ou can create  multiline strings using three double quotes:\n",
            "multi_line_string = \"\"\"This is the first line. \n",
            "and this is the second line \n",
            "and this is the third line\"\"\"\n",
            "A  new feature in Python 3.6 is the f-string , which provides a simple way to\n",
            "substitute values into strings. For example, if we had the first name and last\n",
            "name given separately:\n",
            "first_name = \"Joel\" \n",
            "last_name = \"Grus\"\n",
            "we might want to combine them into a full name. There are multiple ways\n",
            "to construct such a full_name  string:\n",
            "full_name1 = first_name + \" \" + last_name             # string addition \n",
            "full_name2 = \"{0} {1}\".format(first_name, last_name)  # string.format\n",
            "but the f-string way is much less unwieldy:full_name3 = f\"{first_name} {last_name}\"\n",
            "and we’ll prefer it throughout the book.\n",
            "E x c e p t i o n s\n",
            "When  something goes wrong, Python raises an exception . Unhandled,\n",
            "exceptions will cause your program to crash. Y ou can handle them using\n",
            "try  and except :\n",
            "try: \n",
            "    print(0 / 0) \n",
            "except ZeroDivisionError: \n",
            "    print(\"cannot divide by zero\")\n",
            "Although in many languages exceptions are considered bad, in Python there\n",
            "is no shame in using them to make your code cleaner , and we will\n",
            "sometimes do so.\n",
            "L i s t s\n",
            "Probably  the most fundamental data structure in Python is the list , which is\n",
            "simply an ordered collection (it is similar to what in other languages might\n",
            "be called an array , but with some added functionality):\n",
            "integer_list = [1, 2, 3] \n",
            "heterogeneous_list = [\"string\", 0.1, True] \n",
            "list_of_lists = [integer_list, heterogeneous_list, []] \n",
            " \n",
            "list_length = len(integer_list)     # equals 3 \n",
            "list_sum    = sum(integer_list)     # equals 6\n",
            "Y ou  can get or set the n th element of a list with square brackets:\n",
            "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \n",
            " \n",
            "zero = x[0]          # equals 0, lists are 0-indexed \n",
            "one = x[1]           # equals 1nine = x[-1]         # equals 9, 'Pythonic' for last element \n",
            "eight = x[-2]        # equals 8, 'Pythonic' for next-to-last element \n",
            "x[0] = -1            # now x is [-1, 1, 2, 3, ..., 9]\n",
            "Y ou  can also use square brackets to slice  lists. The slice i:j  means all\n",
            "elements from i  (inclusive) to j  (not inclusive). If you leave of f the start of\n",
            "the slice, you’ll slice from the beginning of the list, and if you leave of the\n",
            "end of the slice, you’ll slice until the end of the list:\n",
            "first_three = x[:3]                 # [-1, 1, 2] \n",
            "three_to_end = x[3:]                # [3, 4, ..., 9] \n",
            "one_to_four = x[1:5]                # [1, 2, 3, 4] \n",
            "last_three = x[-3:]                 # [7, 8, 9] \n",
            "without_first_and_last = x[1:-1]    # [1, 2, ..., 8] \n",
            "copy_of_x = x[:]                    # [-1, 1, 2, ..., 9]\n",
            "Y ou can similarly slice strings and other “sequential” types.\n",
            "A  slice can take a third ar gument to indicate its stride , which can be\n",
            "negative:\n",
            "every_third = x[::3]                 # [-1, 3, 6, 9] \n",
            "five_to_three = x[5:2:-1]            # [5, 4, 3]\n",
            "Python  has an in  operator to check for list membership:\n",
            "1 in [1, 2, 3]    # True \n",
            "0 in [1, 2, 3]    # False\n",
            "This check involves examining the elements of the list one at a time, which\n",
            "means that you probably shouldn’ t use it unless you know your list is pretty\n",
            "small (or unless you don’ t care how long the check takes).\n",
            "It  is easy to concatenate lists together . If you want to modify a list in place,\n",
            "you can use extend  to add items from another collection:\n",
            "x = [1, 2, 3] \n",
            "x.extend([4, 5, 6])     # x is now [1, 2, 3, 4, 5, 6]If you don’ t want to modify x , you can use list addition:\n",
            "x = [1, 2, 3] \n",
            "y = x + [4, 5, 6]       # y is [1, 2, 3, 4, 5, 6]; x is unchanged\n",
            "More  frequently we will append to lists one item at a time:\n",
            "x = [1, 2, 3] \n",
            "x.append(0)      # x is now [1, 2, 3, 0] \n",
            "y = x[-1]        # equals 0 \n",
            "z = len(x)       # equals 4\n",
            "It’ s  often convenient to unpack  lists when you know how many elements\n",
            "they contain:\n",
            "x, y = [1, 2]    # now x is 1, y is 2\n",
            "although you will get a ValueError  if you don’ t have the same number of\n",
            "elements on both sides.\n",
            "A common idiom is to use an underscore for a value you’re going to throw\n",
            "away:\n",
            "_, y = [1, 2]    # now y == 2, didn't care about the first element\n",
            "T u p l e s\n",
            "T uples  are lists’ immutable cousins. Pretty much anything you can do to a\n",
            "list that doesn’ t involve modifying it, you can do to a tuple. Y ou specify a\n",
            "tuple by using parentheses (or nothing) instead of square brackets:\n",
            "my_list = [1, 2] \n",
            "my_tuple = (1, 2) \n",
            "other_tuple = 3, 4 \n",
            "my_list[1] = 3      # my_list is now [1, 3] \n",
            " \n",
            "try: \n",
            "    my_tuple[1] = 3except TypeError: \n",
            "    print(\"cannot modify a tuple\")\n",
            "T uples are a convenient way to return multiple values from functions:\n",
            "def sum_and_product(x, y): \n",
            "    return (x + y), (x * y) \n",
            " \n",
            "sp = sum_and_product(2, 3)     # sp is (5, 6) \n",
            "s, p = sum_and_product(5, 10)  # s is 15, p is 50\n",
            "T uples (and lists) can also be used for multiple assignment :\n",
            "x, y = 1, 2     # now x is 1, y is 2 \n",
            "x, y = y, x     # Pythonic way to swap variables; now x is 2, y is 1\n",
            "D i c t i o n a r i e s\n",
            "Another  fundamental data structure is a dictionary , which associates values\n",
            "with keys  and allows you to quickly retrieve the value corresponding to a\n",
            "given key:\n",
            "empty_dict = {}                     # Pythonic \n",
            "empty_dict2 = dict()                # less Pythonic \n",
            "grades = {\"Joel\": 80, \"Tim\": 95}    # dictionary literal\n",
            "Y ou can look up the value for a key using square brackets:\n",
            "joels_grade = grades[\"Joel\"]        # equals 80\n",
            "But you’ll get a KeyError  if you ask for a key that’ s not in the dictionary:\n",
            "try: \n",
            "    kates_grade = grades[\"Kate\"] \n",
            "except KeyError: \n",
            "    print(\"no grade for Kate!\")\n",
            "Y ou can check for the existence of a key using in :joel_has_grade = \"Joel\" in grades     # True \n",
            "kate_has_grade = \"Kate\" in grades     # False\n",
            "This membership check is fast even for lar ge dictionaries.\n",
            "Dictionaries  have a get  method that returns a default value (instead of\n",
            "raising an exception) when you look up a key that’ s not in the dictionary:\n",
            "joels_grade = grades.get(\"Joel\", 0)   # equals 80 \n",
            "kates_grade = grades.get(\"Kate\", 0)   # equals 0 \n",
            "no_ones_grade = grades.get(\"No One\")  # default is None\n",
            "Y ou can assign key/value pairs using the same square brackets:\n",
            "grades[\"Tim\"] = 99                    # replaces the old value \n",
            "grades[\"Kate\"] = 100                  # adds a third entry \n",
            "num_students = len(grades)            # equals 3\n",
            "As you saw in Chapter 1 , you can use dictionaries to represent structured\n",
            "data:\n",
            "tweet = { \n",
            "    \"user\" : \"joelgrus\", \n",
            "    \"text\" : \"Data Science is Awesome\", \n",
            "    \"retweet_count\" : 100, \n",
            "    \"hashtags\" : [\"#data\", \"#science\", \"#datascience\", \"#awesome\", \"#yolo\"] \n",
            "}\n",
            "although we’ll soon see a better approach.\n",
            "Besides looking for specific keys, we can look at all of them:\n",
            "tweet_keys   = tweet.keys()     # iterable for the keys \n",
            "tweet_values = tweet.values()   # iterable for the values \n",
            "tweet_items  = tweet.items()    # iterable for the (key, value) tuples \n",
            " \n",
            "\"user\" in tweet_keys            # True, but not Pythonic \n",
            "\"user\" in tweet                 # Pythonic way of checking for keys \n",
            "\"joelgrus\" in tweet_values      # True (slow but the only way to check)Dictionary keys must be “hashable”; in particular , you cannot use lists as\n",
            "keys. If you need a multipart key , you should probably use a tuple or figure\n",
            "out a way to turn the key into a string.\n",
            "defaultdict\n",
            "Imagine  that you’re trying to count the words in a document. An obvious\n",
            "approach is to create a dictionary in which the keys are words and the\n",
            "values are counts. As you check each word, you can increment its count if\n",
            "it’ s already in the dictionary and add it to the dictionary if it’ s not:\n",
            "word_counts = {} \n",
            "for word in document: \n",
            "    if word in word_counts: \n",
            "        word_counts[word] += 1 \n",
            "    else: \n",
            "        word_counts[word] = 1\n",
            "Y ou could also use the “for giveness is better than permission” approach and\n",
            "just handle the exception from trying to look up a missing key:\n",
            "word_counts = {} \n",
            "for word in document: \n",
            "    try: \n",
            "        word_counts[word] += 1 \n",
            "    except KeyError: \n",
            "        word_counts[word] = 1\n",
            "A third approach is to use get , which behaves gracefully for missing keys:\n",
            "word_counts = {} \n",
            "for word in document: \n",
            "    previous_count = word_counts.get(word, 0) \n",
            "    word_counts[word] = previous_count + 1\n",
            "Every one of these is slightly unwieldy , which is why defaultdict  is\n",
            "useful. A defaultdict  is like a regular dictionary , except that when you try\n",
            "to look up a key it doesn’ t contain, it first adds a value for it using a zero-ar gument function you provided when you created it. In order to use\n",
            "defaultdict s, you have to import them from collections :\n",
            "from collections import defaultdict \n",
            " \n",
            "word_counts = defaultdict(int)          # int() produces 0 \n",
            "for word in document: \n",
            "    word_counts[word] += 1\n",
            "They can also be useful with list  or dict , or even your own functions:\n",
            "dd_list = defaultdict(list)             # list() produces an empty list \n",
            "dd_list[2].append(1)                    # now dd_list contains {2: [1]} \n",
            " \n",
            "dd_dict = defaultdict(dict)             # dict() produces an empty dict \n",
            "dd_dict[\"Joel\"][\"City\"] = \"Seattle\"     # {\"Joel\" : {\"City\": Seattle\"}} \n",
            " \n",
            "dd_pair = defaultdict(lambda: [0, 0]) \n",
            "dd_pair[2][1] = 1                       # now dd_pair contains {2: [0, 1]}\n",
            "These will be useful when we’re using dictionaries to “collect” results by\n",
            "some key and don’ t want to have to check every time to see if the key exists\n",
            "yet.\n",
            "C o u n t e r s\n",
            "A Counter  turns  a sequence of values into a defaultdict(int) -like object\n",
            "mapping keys to counts:\n",
            "from collections import Counter \n",
            "c = Counter([0, 1, 2, 0])          # c is (basically) {0: 2, 1: 1, 2: 1}\n",
            "This gives us a very simple way to solve our word_counts  problem:\n",
            "# recall, document is a list of words \n",
            "word_counts = Counter(document)\n",
            "A  Counter  instance has a most_common  method that is frequently useful:# print the 10 most common words and their counts \n",
            "for word, count in word_counts.most_common(10): \n",
            "    print(word, count)\n",
            "S e t s\n",
            "Another  useful data structure is set, which represents a collection of distinct\n",
            "elements. Y ou can define a set by listing its elements between curly braces:\n",
            "primes_below_10 = {2, 3, 5, 7}\n",
            "However , that doesn’ t work for empty set s, as {}  already means “empty\n",
            "dict .” In that case you’ll need to use set()  itself:\n",
            "s = set() \n",
            "s.add(1)       # s is now {1} \n",
            "s.add(2)       # s is now {1, 2} \n",
            "s.add(2)       # s is still {1, 2} \n",
            "x = len(s)     # equals 2 \n",
            "y = 2 in s     # equals True \n",
            "z = 3 in s     # equals False\n",
            "W e’ll use sets for two main reasons. The first is that in  is a very fast\n",
            "operation on sets. If we have a lar ge collection of items that we want to use\n",
            "for a membership test, a set is more appropriate than a list:\n",
            "stopwords_list = [\"a\", \"an\", \"at\"] + hundreds_of_other_words + [\"yet\", \"you\"] \n",
            " \n",
            "\"zip\" in stopwords_list     # False, but have to check every element \n",
            " \n",
            "stopwords_set = set(stopwords_list) \n",
            "\"zip\" in stopwords_set      # very fast to check\n",
            "The  second reason is to find the distinct  items in a collection:\n",
            "item_list = [1, 2, 3, 1, 2, 3] \n",
            "num_items = len(item_list)                # 6 \n",
            "item_set = set(item_list)                 # {1, 2, 3} \n",
            "num_distinct_items = len(item_set)        # 3 \n",
            "distinct_item_list = list(item_set)       # [1, 2, 3]W e’ll use sets less frequently than dictionaries and lists.\n",
            "C o n t r o l  F l o w\n",
            "As  in most programming languages, you can perform an action\n",
            "conditionally using if :\n",
            "if 1 > 2: \n",
            "    message = \"if only 1 were greater than two...\" \n",
            "elif 1 > 3: \n",
            "    message = \"elif stands for 'else if'\" \n",
            "else: \n",
            "    message = \"when all else fails use else (if you want to)\"\n",
            "Y ou  can also write a ternary  if-then-else  on one line, which we will do\n",
            "occasionally:\n",
            "parity = \"even\" if x % 2 == 0 else \"odd\"\n",
            "Python  has a while  loop:\n",
            "x = 0 \n",
            "while x < 10: \n",
            "    print(f\"{x} is less than 10\") \n",
            "    x += 1\n",
            "although more often we’ll use for  and in :\n",
            "# range(10) is the numbers 0, 1, ..., 9 \n",
            "for x in range(10): \n",
            "    print(f\"{x} is less than 10\")\n",
            "If you need more complex logic, you can use continue  and break :\n",
            "for x in range(10): \n",
            "    if x == 3: \n",
            "        continue  # go immediately to the next iteration \n",
            "    if x == 5:break     # quit the loop entirely \n",
            "    print(x)\n",
            "This will print 0 , 1 , 2 , and 4 .\n",
            "T r u t h i n e s s\n",
            "Booleans  in Python work as in most other languages, except that they’re\n",
            "capitalized:\n",
            "one_is_less_than_two = 1 < 2          # equals True \n",
            "true_equals_false = True == False     # equals False\n",
            "Python  uses the value None  to indicate a nonexistent value. It is similar to\n",
            "other languages’ null :\n",
            "x = None \n",
            "assert x == None, \"this is the not the Pythonic way to check for None\" \n",
            "assert x is None, \"this is the Pythonic way to check for None\"\n",
            "Python lets you use any value where it expects a Boolean. The following\n",
            "are all “falsy”:\n",
            "False\n",
            "None\n",
            "[]  (an empty list )\n",
            "{}  (an empty dict )\n",
            "\"\"\n",
            "set()\n",
            "0\n",
            "0.0Pretty much anything else gets treated as True . This allows you to easily\n",
            "use if  statements to test for empty lists, empty strings, empty dictionaries,\n",
            "and so on. It also sometimes causes tricky bugs if you’re not expecting this\n",
            "behavior:\n",
            "s = some_function_that_returns_a_string() \n",
            "if s: \n",
            "    first_char = s[0] \n",
            "else: \n",
            "    first_char = \"\"\n",
            "A shorter (but possibly more confusing) way of doing the same is:\n",
            "first_char = s and s[0]\n",
            "since and  returns its second value when the first is “truthy ,” and the first\n",
            "value when it’ s not. Similarly , if x  is either a number or possibly None :\n",
            "safe_x = x or 0\n",
            "is definitely a number , although:\n",
            "safe_x = x if x is not None else 0\n",
            "is possibly more readable.\n",
            "Python has an all  function, which takes an iterable and returns True\n",
            "precisely when every element is truthy , and an any  function, which returns\n",
            "True  when at least one element is truthy:\n",
            "all([True, 1, {3}])   # True, all are truthy \n",
            "all([True, 1, {}])    # False, {} is falsy \n",
            "any([True, 1, {}])    # True, True is truthy \n",
            "all([])               # True, no falsy elements in the list \n",
            "any([])               # False, no truthy elements in the list\n",
            "S o r t i n gEvery  Python list has a sort  method that sorts it in place. If you don’ t want\n",
            "to mess up your list, you can use the sorted  function, which returns a new\n",
            "list:\n",
            "x = [4, 1, 2, 3] \n",
            "y = sorted(x)     # y is [1, 2, 3, 4], x is unchanged \n",
            "x.sort()          # now x is [1, 2, 3, 4]\n",
            "By default, sort  (and sorted ) sort a list from smallest to lar gest based on\n",
            "naively comparing the elements to one another .\n",
            "If you want elements sorted from lar gest to smallest, you can specify a\n",
            "reverse=True  parameter . And instead of comparing the elements\n",
            "themselves, you can compare the results of a function that you specify with\n",
            "key :\n",
            "# sort the list by absolute value from largest to smallest \n",
            "x = sorted([-4, 1, -2, 3], key=abs, reverse=True)  # is [-4, 3, -2, 1] \n",
            " \n",
            "# sort the words and counts from highest count to lowest \n",
            "wc = sorted(word_counts.items(), \n",
            "            key=lambda word_and_count: word_and_count[1], \n",
            "            reverse=True)\n",
            "L i s t  C o m p r e h e n s i o n s\n",
            "Frequently , you’ll  want to transform a list into another list by choosing only\n",
            "certain elements, by transforming elements, or both. The Pythonic way to\n",
            "do this is with list compr ehensions :\n",
            "even_numbers = [x for x in range(5) if x % 2 == 0]  # [0, 2, 4] \n",
            "squares      = [x * x for x in range(5)]            # [0, 1, 4, 9, 16] \n",
            "even_squares = [x * x for x in even_numbers]        # [0, 4, 16]\n",
            "Y ou can similarly turn lists into dictionaries or sets:\n",
            "square_dict = {x: x * x for x in range(5)}  # {0: 0, 1: 1, 2: 4, 3: 9, 4: 16} \n",
            "square_set  = {x * x for x in [1, -1]}      # {1}If you don’ t need the value from the list, it’ s common to use an underscore\n",
            "as the variable:\n",
            "zeros = [0 for _ in even_numbers]      # has the same length as even_numbers\n",
            "A list comprehension can include multiple for s:\n",
            "pairs = [(x, y) \n",
            "         for x in range(10) \n",
            "         for y in range(10)]   # 100 pairs (0,0) (0,1) ... (9,8), (9,9)\n",
            "and later for s can use the results of earlier ones:\n",
            "increasing_pairs = [(x, y)                       # only pairs with x < y, \n",
            "                    for x in range(10)           # range(lo, hi) equals \n",
            "                    for y in range(x + 1, 10)]   # [lo, lo + 1, ..., hi - 1]\n",
            "W e will use list comprehensions a lot.\n",
            "A u t o m a t e d  T e s t i n g  a n d  a s s e r t\n",
            "As  data scientists, we’ll be writing a lot of code. How can we be confident\n",
            "our code is correct? One way is with types  (discussed shortly), but another\n",
            "way is with automated tests .\n",
            "There are elaborate frameworks for writing and running tests, but in this\n",
            "book we’ll restrict ourselves to using assert  statements, which will cause\n",
            "your code to raise an AssertionError  if your specified condition is not\n",
            "truthy:\n",
            "assert 1 + 1 == 2 \n",
            "assert 1 + 1 == 2, \"1 + 1 should equal 2 but didn't\"\n",
            "As you can see in the second case, you can optionally add a message to be\n",
            "printed if the assertion fails.It’ s not particularly interesting to assert that 1 + 1 = 2. What’ s more\n",
            "interesting is to assert that functions you write are doing what you expect\n",
            "them to:\n",
            "def smallest_item(xs): \n",
            "    return min(xs) \n",
            " \n",
            "assert smallest_item([10, 20, 5, 40]) == 5 \n",
            "assert smallest_item([1, 0, -1, 2]) == -1\n",
            "Throughout the book we’ll be using assert  in this way . It is a good\n",
            "practice, and I strongly encourage you to make liberal use of it in your own\n",
            "code. (If you look at the book’ s code on GitHub, you will see that it\n",
            "contains many , many more assert  statements than are printed in the book.\n",
            "This helps me  be confident that the code I’ve written for you is correct.)\n",
            "Another less common use is to assert things about inputs to functions:\n",
            "def smallest_item(xs): \n",
            "    assert xs, \"empty list has no smallest item\" \n",
            "    return min(xs)\n",
            "W e’ll occasionally do this, but more often we’ll use assert  to check that\n",
            "our code is correct.\n",
            "O b j e c t - O r i e n t e d  P r o g r a m m i n g\n",
            "Like  many languages, Python allows you to define classes  that encapsulate\n",
            "data and the functions that operate on them. W e’ll use them sometimes to\n",
            "make our code cleaner and simpler . It’ s probably simplest to explain them\n",
            "by constructing a heavily annotated example.\n",
            "Here we’ll construct a class representing a “counting clicker ,” the sort that\n",
            "is used at the door to track how many people have shown up for the\n",
            "“advanced topics in data science” meetup.It maintains a count , can be click ed to increment the count, allows you to\n",
            "read_count , and can be reset  back to zero. (In real life one of these rolls\n",
            "over from 9999 to 0000, but we won’ t bother with that.)\n",
            "T o define a class, you use the class  keyword and a PascalCase name:\n",
            "class CountingClicker: \n",
            "    \"\"\"A class can/should have a docstring, just like a function\"\"\"\n",
            "A  class contains zero or more member  functions. By convention, each takes\n",
            "a first parameter , self , that refers to the particular class instance.\n",
            "Normally , a class has a constructor , named __init__ . It takes whatever\n",
            "parameters you need to construct an instance of your class and does\n",
            "whatever setup you need:\n",
            "    def __init__(self, count = 0): \n",
            "        self.count = count\n",
            "Although the constructor has a funny name, we construct instances of the\n",
            "clicker using just the class name:\n",
            "clicker1 = CountingClicker()           # initialized to 0 \n",
            "clicker2 = CountingClicker(100)        # starts with count=100 \n",
            "clicker3 = CountingClicker(count=100)  # more explicit way of doing the same\n",
            "Notice  that the __init__  method name starts and ends with double\n",
            "underscores. These “magic” methods are sometimes called “dunder”\n",
            "methods (double-UNDERscore, get it?) and represent “special” behaviors.\n",
            "N O T E\n",
            "Class methods  whose names start with an underscore are—by convention—considered\n",
            "“private,” and users of the class are not supposed to directly call them. However , Python\n",
            "will not stop  users from calling them.Another such method is __repr__ , which produces the string representation\n",
            "of a class instance:\n",
            "    def __repr__(self): \n",
            "        return f\"CountingClicker(count={self.count})\"\n",
            "And finally we need to implement the public API  of our class:\n",
            "    def click(self, num_times = 1): \n",
            "        \"\"\"Click the clicker some number of times.\"\"\" \n",
            "        self.count += num_times \n",
            " \n",
            "    def read(self): \n",
            "        return self.count \n",
            " \n",
            "    def reset(self): \n",
            "        self.count = 0\n",
            "Having defined it, let’ s use assert  to write some test cases for our clicker:\n",
            "clicker = CountingClicker() \n",
            "assert clicker.read() == 0, \"clicker should start with count 0\" \n",
            "clicker.click() \n",
            "clicker.click() \n",
            "assert clicker.read() == 2, \"after two clicks, clicker should have count 2\" \n",
            "clicker.reset() \n",
            "assert clicker.read() == 0, \"after reset, clicker should be back to 0\"\n",
            "W riting tests like these help us be confident that our code is working the\n",
            "way it’ s designed to, and that it remains doing so whenever we make\n",
            "changes to it.\n",
            "W e’ll also occasionally create subclasses  that inherit  some of their\n",
            "functionality from a parent class. For example, we could create a non-reset-\n",
            "able clicker by using CountingClicker  as the base class and overriding the\n",
            "reset  method to do nothing:\n",
            "# A subclass inherits all the behavior of its parent class. \n",
            "class NoResetClicker(CountingClicker): \n",
            "    # This class has all the same methods as CountingClicker# Except that it has a reset method that does nothing. \n",
            "    def reset(self): \n",
            "        pass \n",
            " \n",
            "clicker2 = NoResetClicker() \n",
            "assert clicker2.read() == 0 \n",
            "clicker2.click() \n",
            "assert clicker2.read() == 1 \n",
            "clicker2.reset() \n",
            "assert clicker2.read() == 1, \"reset shouldn't do anything\"\n",
            "I t e r a b l e s  a n d  G e n e r a t o r s\n",
            "One  nice thing about a list is that you can retrieve specific elements by their\n",
            "indices. But you don’ t always need this! A list of a billion numbers takes up\n",
            "a lot of memory . If you only want the elements one at a time, there’ s no\n",
            "good reason to keep them all around. If you only end up needing the first\n",
            "several elements, generating the entire billion is hugely wasteful.\n",
            "Often all we need is to iterate over the collection using for  and in . In this\n",
            "case we can create generators , which can be iterated over just like lists but\n",
            "generate their values lazily on demand.\n",
            "One way to create generators is with functions and the yield  operator:\n",
            "def generate_range(n): \n",
            "    i = 0 \n",
            "    while i < n: \n",
            "        yield i   # every call to yield produces a value of the generator \n",
            "        i += 1\n",
            "The following loop will consume the yield ed values one at a time until\n",
            "none are left:\n",
            "for i in generate_range(10): \n",
            "    print(f\"i: {i}\")\n",
            "(In fact, range  is itself lazy , so there’ s no point in doing this.)\n",
            "W ith a generator , you can even create an infinite sequence:def natural_numbers(): \n",
            "    \"\"\"returns 1, 2, 3, ...\"\"\" \n",
            "    n = 1 \n",
            "    while True: \n",
            "        yield n \n",
            "        n += 1\n",
            "although you probably shouldn’ t iterate over it without using some kind of\n",
            "break  logic.\n",
            "T I P\n",
            "The flip side of laziness is that you can only iterate through a generator once. If you\n",
            "need to iterate through something multiple times, you’ll need to either re-create the\n",
            "generator each time or use a list. If generating the values is expensive, that might be a\n",
            "good reason to use a list instead.\n",
            "A second way to create generators is by using for  comprehensions wrapped\n",
            "in parentheses:\n",
            "evens_below_20 = (i for i in generate_range(20) if i % 2 == 0)\n",
            "Such a “generator comprehension” doesn’ t do any work until you iterate\n",
            "over it (using for  or next ). W e can use this to build up elaborate data-\n",
            "processing pipelines:\n",
            "# None of these computations *does* anything until we iterate \n",
            "data = natural_numbers() \n",
            "evens = (x for x in data if x % 2 == 0) \n",
            "even_squares = (x ** 2 for x in evens) \n",
            "even_squares_ending_in_six = (x for x in even_squares if x % 10 == 6) \n",
            "# and so on\n",
            "Not  infrequently , when we’re iterating over a list or a generator we’ll want\n",
            "not just the values but also their indices. For this common case Python\n",
            "provides an enumerate  function, which turns values into pairs (index,\n",
            "value) :names = [\"Alice\", \"Bob\", \"Charlie\", \"Debbie\"] \n",
            " \n",
            "# not Pythonic \n",
            "for i in range(len(names)): \n",
            "    print(f\"name {i} is {names[i]}\") \n",
            " \n",
            "# also not Pythonic \n",
            "i = 0 \n",
            "for name in names: \n",
            "    print(f\"name {i} is {names[i]}\") \n",
            "    i += 1 \n",
            " \n",
            "# Pythonic \n",
            "for i, name in enumerate(names): \n",
            "    print(f\"name {i} is {name}\")\n",
            "W e’ll use this a lot.\n",
            "R a n d o m n e s s\n",
            "As  we learn data science, we will frequently need to generate random\n",
            "numbers, which we can do with the random  module:\n",
            "import random \n",
            "random.seed(10)  # this ensures we get the same results every time \n",
            " \n",
            "four_uniform_randoms = [random.random() for _ in range(4)] \n",
            " \n",
            "# [0.5714025946899135,       # random.random() produces numbers \n",
            "#  0.4288890546751146,       # uniformly between 0 and 1. \n",
            "#  0.5780913011344704,       # It's the random function we'll use \n",
            "#  0.20609823213950174]      # most often.\n",
            "The random  module actually produces pseudorandom  (that is,\n",
            "deterministic) numbers based on an internal state that you can set with\n",
            "random.seed  if you want to get reproducible results:\n",
            "random.seed(10)         # set the seed to 10 \n",
            "print(random.random())  # 0.57140259469 \n",
            "random.seed(10)         # reset the seed to 10 \n",
            "print(random.random())  # 0.57140259469 againW e’ll sometimes use random.randrange , which takes either one or two\n",
            "ar guments and returns an element chosen randomly from the corresponding\n",
            "range :\n",
            "random.randrange(10)    # choose randomly from range(10) = [0, 1, ..., 9] \n",
            "random.randrange(3, 6)  # choose randomly from range(3, 6) = [3, 4, 5]\n",
            "There are a few more methods that we’ll sometimes find convenient. For\n",
            "example, random.shuffle  randomly reorders the elements of a list:\n",
            "up_to_ten = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
            "random.shuffle(up_to_ten) \n",
            "print(up_to_ten) \n",
            "# [7, 2, 6, 8, 9, 4, 10, 1, 3, 5]   (your results will probably be different)\n",
            "If you need to randomly pick one element from a list, you can use\n",
            "random.choice :\n",
            "my_best_friend = random.choice([\"Alice\", \"Bob\", \"Charlie\"])     # \"Bob\" for me\n",
            "And if you need to randomly choose a sample of elements without\n",
            "replacement (i.e., with no duplicates), you can use random.sample :\n",
            "lottery_numbers = range(60) \n",
            "winning_numbers = random.sample(lottery_numbers, 6)  # [16, 36, 10, 6, 25, 9]\n",
            "T o choose a sample of elements with  replacement (i.e., allowing duplicates),\n",
            "you can just make multiple calls to random.choice :\n",
            "four_with_replacement = [random.choice(range(10)) for _ in range(4)] \n",
            "print(four_with_replacement)  # [9, 4, 4, 2]\n",
            "R e g u l a r  E x p r e s s i o n s\n",
            "Regular expressions  provide a way of searching text. They are incredibly\n",
            "useful, but also fairly complicated—so much so that there are entire bookswritten about them. W e will get into their details the few times we\n",
            "encounter them; here are a few examples of how to use them in Python:\n",
            "import re \n",
            " \n",
            "re_examples = [                        # All of these are True, because \n",
            "    not re.match(\"a\", \"cat\"),              #  'cat' doesn't start with 'a' \n",
            "    re.search(\"a\", \"cat\"),                 #  'cat' has an 'a' in it \n",
            "    not re.search(\"c\", \"dog\"),             #  'dog' doesn't have a 'c' in it. \n",
            "    3 == len(re.split(\"[ab]\", \"carbs\")),   #  Split on a or b to \n",
            "['c','r','s']. \n",
            "    \"R-D-\" == re.sub(\"[0-9]\", \"-\", \"R2D2\") #  Replace digits with dashes. \n",
            "    ] \n",
            " \n",
            "assert all(re_examples), \"all the regex examples should be True\"\n",
            "One important thing to note is that re.match  checks whether the beginning\n",
            "of a string matches a regular expression, while re.search  checks whether\n",
            "any part  of a string matches a regular expression. At some point you will\n",
            "mix these two up and it will cause you grief.\n",
            "The of ficial documentation  goes into much more detail.\n",
            "F u n c t i o n a l  P r o g r a m m i n g\n",
            "N O T E\n",
            "The  first edition of this book introduced the Python functions partial , map , reduce ,\n",
            "and filter  at this point. On my journey toward enlightenment I have realized that these\n",
            "functions are best avoided, and their uses in the book have been replaced with list\n",
            "comprehensions, for  loops, and other , more Pythonic constructs.\n",
            "z i p  a n d  A r g u m e n t  U n p a c k i n g\n",
            "Often  we will need to zip  two or more iterables together . The zip  function\n",
            "transforms multiple iterables into a single iterable of tuples ofcorresponding function:\n",
            "list1 = ['a', 'b', 'c'] \n",
            "list2 = [1, 2, 3] \n",
            " \n",
            "# zip is lazy, so you have to do something like the following \n",
            "[pair for pair in zip(list1, list2)]    # is [('a', 1), ('b', 2), ('c', 3)]\n",
            "If the lists are dif ferent lengths, zip  stops as soon as the first list ends.\n",
            "Y ou can also “unzip” a list using a strange trick:\n",
            "pairs = [('a', 1), ('b', 2), ('c', 3)] \n",
            "letters, numbers = zip(*pairs)\n",
            "The asterisk ( * ) performs ar gument unpacking , which uses the elements of\n",
            "pairs  as individual ar guments to zip . It ends up the same as if you’d\n",
            "called:\n",
            "letters, numbers = zip(('a', 1), ('b', 2), ('c', 3))\n",
            "Y ou can use ar gument unpacking with any function:\n",
            "def add(a, b): return a + b \n",
            " \n",
            "add(1, 2)      # returns 3 \n",
            "try: \n",
            "    add([1, 2]) \n",
            "except TypeError: \n",
            "    print(\"add expects two inputs\") \n",
            "add(*[1, 2])   # returns 3\n",
            "It is rare that we’ll find this useful, but when we do it’ s a neat trick.\n",
            "a r g s  a n d  k w a r g s\n",
            "Let’ s  say we want to create a higher -order function that takes as input some\n",
            "function f  and returns a new function that for any input returns twice thevalue of f :\n",
            "def doubler(f): \n",
            "    # Here we define a new function that keeps a reference to f \n",
            "    def g(x): \n",
            "        return 2 * f(x) \n",
            " \n",
            "    # And return that new function \n",
            "    return g\n",
            "This works in some cases:\n",
            "def f1(x): \n",
            "    return x + 1 \n",
            " \n",
            "g = doubler(f1) \n",
            "assert g(3) == 8,  \"(3 + 1) * 2 should equal 8\" \n",
            "assert g(-1) == 0, \"(-1 + 1) * 2 should equal 0\"\n",
            "However , it doesn’ t work with functions that take more than a single\n",
            "ar gument:\n",
            "def f2(x, y): \n",
            "    return x + y \n",
            " \n",
            "g = doubler(f2) \n",
            "try: \n",
            "    g(1, 2) \n",
            "except TypeError: \n",
            "    print(\"as defined, g only takes one argument\")\n",
            "What we need is a way to specify a function that takes arbitrary ar guments.\n",
            "W e can do this with ar gument unpacking and a little bit of magic:\n",
            "def magic(*args, **kwargs): \n",
            "    print(\"unnamed args:\", args) \n",
            "    print(\"keyword args:\", kwargs) \n",
            " \n",
            "magic(1, 2, key=\"word\", key2=\"word2\") \n",
            " \n",
            "# prints#  unnamed args: (1, 2) \n",
            "#  keyword args: {'key': 'word', 'key2': 'word2'}\n",
            "That is, when we define a function like this, args  is a tuple of its unnamed\n",
            "ar guments and kwargs  is a dict  of its named ar guments. It works the other\n",
            "way too, if you want to use a list  (or tuple ) and dict  to supply  ar guments\n",
            "to a function:\n",
            "def other_way_magic(x, y, z): \n",
            "    return x + y + z \n",
            " \n",
            "x_y_list = [1, 2] \n",
            "z_dict = {\"z\": 3} \n",
            "assert other_way_magic(*x_y_list, **z_dict) == 6, \"1 + 2 + 3 should be 6\"\n",
            "Y ou could do all sorts of strange tricks with this; we will only use it to\n",
            "produce higher -order functions whose inputs can accept arbitrary\n",
            "ar guments:\n",
            "def doubler_correct(f): \n",
            "    \"\"\"works no matter what kind of inputs f expects\"\"\" \n",
            "    def g(*args, **kwargs): \n",
            "        \"\"\"whatever arguments g is supplied, pass them through to f\"\"\" \n",
            "        return 2 * f(*args, **kwargs) \n",
            "    return g \n",
            " \n",
            "g = doubler_correct(f2) \n",
            "assert g(1, 2) == 6, \"doubler should work now\"\n",
            "As a general rule, your code will be more correct and more readable if you\n",
            "are explicit about what sorts of ar guments your functions require;\n",
            "accordingly , we will use args  and kwargs  only when we have no other\n",
            "option.\n",
            "T y p e  A n n o t a t i o n s\n",
            "Python  is a dynamically typed  language. That means that it in general it\n",
            "doesn’ t care about the types of objects we use, as long as we use them invalid ways:\n",
            "def add(a, b): \n",
            "    return a + b \n",
            " \n",
            "assert add(10, 5) == 15,                  \"+ is valid for numbers\" \n",
            "assert add([1, 2], [3]) == [1, 2, 3],     \"+ is valid for lists\" \n",
            "assert add(\"hi \", \"there\") == \"hi there\", \"+ is valid for strings\" \n",
            " \n",
            "try: \n",
            "    add(10, \"five\") \n",
            "except TypeError: \n",
            "    print(\"cannot add an int to a string\")\n",
            "whereas  in a statically typed  language our functions and objects would have\n",
            "specific types:\n",
            "def add(a: int, b: int) -> int: \n",
            "    return a + b \n",
            " \n",
            "add(10, 5)           # you'd like this to be OK \n",
            "add(\"hi \", \"there\")  # you'd like this to be not OK\n",
            "In fact, recent versions of Python do (sort of) have this functionality . The\n",
            "preceding version of add  with the int  type annotations is valid Python 3.6!\n",
            "However , these type annotations don’ t actually do  anything. Y ou can still\n",
            "use the annotated add  function to add strings, and the call to add(10,\n",
            "\"five\")  will still raise the exact same TypeError .\n",
            "That said, there are still (at least) four good reasons to use type annotations\n",
            "in your Python code:\n",
            "T ypes are an important form of documentation. This is doubly true\n",
            "in a book that is using code to teach you theoretical and\n",
            "mathematical concepts. Compare the following two function stubs:\n",
            "def dot_product(x, y): ...# we have not yet defined Vector, but imagine we had \n",
            "def dot_product(x: Vector, y: Vector) -> float: ...\n",
            "I find the second one exceedingly more informative; hopefully you\n",
            "do too. (At this point I have gotten so used to type hinting that I\n",
            "now find untyped Python dif ficult to read.)\n",
            "There are external tools (the most popular is mypy ) that will read\n",
            "your code, inspect the type annotations, and let you know about\n",
            "type errors befor e you ever run your code . For example, if you ran\n",
            "mypy  over a file containing add(\"hi \", \"there\") , it would warn\n",
            "you:\n",
            "error: Argument 1 to \"add\" has incompatible type \"str\"; expected \n",
            "\"int\"\n",
            "Like assert  testing, this is a good way to find mistakes in your\n",
            "code before you ever run it. The narrative in the book will not\n",
            "involve such a type checker; however , behind the scenes I will be\n",
            "running one, which will help ensure that the book itself is corr ect .\n",
            "Having to think about the types in your code forces you to design\n",
            "cleaner functions and interfaces:\n",
            "from typing import Union \n",
            " \n",
            "def secretly_ugly_function(value, operation): ... \n",
            " \n",
            "def ugly_function(value: int, \n",
            "                  operation: Union[str, int, float, bool]) -> int: \n",
            "    ...\n",
            "Here we have a function whose operation  parameter is allowed to\n",
            "be a string , or an int , or a float , or a bool . It is highly likely\n",
            "that this function is fragile and dif ficult to use, but it becomes farmore clear when the types are made explicit. Doing so, then, will\n",
            "force us to design in a less clunky way , for which our users will\n",
            "thank us.\n",
            "Using types allows your editor to help you with things like\n",
            "autocomplete ( Figure 2-1 ) and to get angry at type errors.\n",
            "Figur e 2-1. VSCode, but likely your editor does the same\n",
            "Sometimes people insist that type hints may be valuable on lar ge projects\n",
            "but are not worth the time for small ones. However , since type hints take\n",
            "almost no additional time to type and allow your editor to save you time, I\n",
            "maintain that they actually allow you to write code more quickly , even for\n",
            "small projects.\n",
            "For all these reasons, all of the code in the remainder of the book will use\n",
            "type annotations. I expect that some readers will be put of f by the use of\n",
            "type annotations; however , I suspect by the end of the book they will have\n",
            "changed their minds.\n",
            "How to W rite T ype Annotations\n",
            "As we’ve seen, for built-in types like int  and bool  and float , you just use\n",
            "the type itself as the annotation. What if you had (say) a list ?def total(xs: list) -> float: \n",
            "    return sum(total)\n",
            "This isn’ t wrong, but the type is not specific enough. It’ s clear we really\n",
            "want xs  to be a list  of floats , not (say) a list  of strings.\n",
            "The typing  module provides a number of parameterized types that we can\n",
            "use to do just this:\n",
            "from typing import List  # note capital L \n",
            " \n",
            "def total(xs: List[float]) -> float: \n",
            "    return sum(total)\n",
            "Up until now we’ve only specified annotations for function parameters and\n",
            "return types. For variables themselves it’ s usually obvious what the type is:\n",
            "# This is how to type-annotate variables when you define them. \n",
            "# But this is unnecessary; it's \"obvious\" x is an int. \n",
            "x: int = 5\n",
            "However , sometimes it’ s not obvious:\n",
            "values = []         # what's my type? \n",
            "best_so_far = None  # what's my type?\n",
            "In such cases we will supply inline type hints:\n",
            "from typing import Optional \n",
            " \n",
            "values: List[int] = [] \n",
            "best_so_far: Optional[float] = None  # allowed to be either a float or None\n",
            "The typing  module contains many other types, only a few of which we’ll\n",
            "ever use:\n",
            "# the type annotations in this snippet are all unnecessary \n",
            "from typing import Dict, Iterable, Tuple \n",
            " \n",
            "# keys are strings, values are intscounts: Dict[str, int] = {'data': 1, 'science': 2} \n",
            " \n",
            "# lists and generators are both iterable \n",
            "if lazy: \n",
            "    evens: Iterable[int] = (x for x in range(10) if x % 2 == 0) \n",
            "else: \n",
            "    evens = [0, 2, 4, 6, 8] \n",
            " \n",
            "# tuples specify a type for each element \n",
            "triple: Tuple[int, float, int] = (10, 2.3, 5)\n",
            "Finally , since Python has first-class functions, we need a type to represent\n",
            "those as well. Here’ s a pretty contrived example:\n",
            "from typing import Callable \n",
            " \n",
            "# The type hint says that repeater is a function that takes \n",
            "# two arguments, a string and an int, and returns a string. \n",
            "def twice(repeater: Callable[[str, int], str], s: str) -> str: \n",
            "    return repeater(s, 2) \n",
            " \n",
            "def comma_repeater(s: str, n: int) -> str: \n",
            "    n_copies = [s for _ in range(n)] \n",
            "    return ', '.join(n_copies) \n",
            " \n",
            "assert twice(comma_repeater, \"type hints\") == \"type hints, type hints\"\n",
            "As type annotations are just Python objects, we can assign them to variables\n",
            "to make them easier to refer to:\n",
            "Number = int \n",
            "Numbers = List[Number] \n",
            " \n",
            "def total(xs: Numbers) -> Number: \n",
            "    return sum(xs)\n",
            "By the time you get to the end of the book, you’ll be quite familiar with\n",
            "reading and writing type annotations, and I hope you’ll use them in your\n",
            "code.\n",
            "W e l c o m e  t o  D a t a S c i e n c e s t e r !This concludes new employee orientation. Oh, and also: try not to embezzle\n",
            "anything.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "There  is no shortage of Python tutorials in the world. The of ficial\n",
            "one  is not a bad place to start.\n",
            "The of ficial IPython tutorial  will  help you get started with IPython,\n",
            "if you decide to use it. Please use it.\n",
            "The mypy  documentation  will tell you more than you ever wanted\n",
            "to know about Python type annotations and type checking.Chapter 3. V isualizing Data\n",
            "I believe that visualization is one of the most powerful means of\n",
            "achieving personal goals.\n",
            "— Harvey Mackay\n",
            "A  fundamental part of the data scientist’ s toolkit is data visualization.\n",
            "Although it is very easy to create visualizations, it’ s much harder to produce\n",
            "good  ones.\n",
            "There are two primary uses for data visualization:\n",
            "T o explor e  data\n",
            "T o communicate  data\n",
            "In this chapter , we will concentrate on building the skills that you’ll need to\n",
            "start exploring your own data and to produce the visualizations we’ll be\n",
            "using throughout the rest of the book. Like most of our chapter topics, data\n",
            "visualization is a rich field of study that deserves its own book.\n",
            "Nonetheless, I’ll try to give you a sense of what makes for a good\n",
            "visualization and what doesn’ t.\n",
            "m a t p l o t l i b\n",
            "A  wide variety of tools exist for visualizing data. W e will be using the\n",
            "matplotlib library , which is widely used (although sort of showing its age).\n",
            "If you are interested in producing elaborate interactive visualizations for the\n",
            "web, it is likely not the right choice, but for simple bar charts, line charts,\n",
            "and scatterplots, it works pretty well.\n",
            "As mentioned earlier , matplotlib is not part of the core Python library . W ith\n",
            "your virtual environment activated (to set one up, go back to “V irtual\n",
            "Environments”  and follow the instructions), install it using this command:python -m pip install matplotlib\n",
            "W e will be using the matplotlib.pyplot  module. In its simplest use,\n",
            "pyplot  maintains an internal state in which you build up a visualization\n",
            "step by step. Once you’re done, you can save it with savefig  or display it\n",
            "with show .\n",
            "For example, making simple plots (like Figure 3-1 ) is pretty simple:\n",
            "from matplotlib import pyplot as plt \n",
            " \n",
            "years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] \n",
            "gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] \n",
            " \n",
            "# create a line chart, years on x-axis, gdp on y-axis \n",
            "plt.plot(years, gdp, color='green', marker='o', linestyle='solid') \n",
            " \n",
            "# add a title \n",
            "plt.title(\"Nominal GDP\") \n",
            " \n",
            "# add a label to the y-axis \n",
            "plt.ylabel(\"Billions of $\") \n",
            "plt.show()Figur e 3-1. A simple line chart\n",
            "Making plots  that look publication-quality good is more complicated and\n",
            "beyond the scope of this chapter . There are many ways you can customize\n",
            "your charts with, for example, axis labels, line styles, and point markers.\n",
            "Rather than attempt a comprehensive treatment of these options, we’ll just\n",
            "use (and call attention to) some of them in our examples.\n",
            "N O T E\n",
            "Although we won’ t be using much of this functionality , matplotlib is capable of\n",
            "producing complicated plots within plots, sophisticated formatting, and interactive\n",
            "visualizations. Check out its documentation  if you want to go deeper than we do in this\n",
            "book.\n",
            "B a r  C h a r t sA  bar chart is a good choice when you want to show how some quantity\n",
            "varies among some discr ete  set of items. For instance, Figure 3-2  shows\n",
            "how many Academy A wards were won by each of a variety of movies:\n",
            "movies = [\"Annie Hall\", \"Ben-Hur\", \"Casablanca\", \"Gandhi\", \"West Side Story\"] \n",
            "num_oscars = [5, 11, 3, 8, 10] \n",
            " \n",
            "# plot bars with left x-coordinates [0, 1, 2, 3, 4], heights [num_oscars] \n",
            "plt.bar(range(len(movies)), num_oscars) \n",
            " \n",
            "plt.title(\"My Favorite Movies\")     # add a title \n",
            "plt.ylabel(\"# of Academy Awards\")   # label the y-axis \n",
            " \n",
            "# label x-axis with movie names at bar centers \n",
            "plt.xticks(range(len(movies)), movies) \n",
            " \n",
            "plt.show()\n",
            "Figur e 3-2. A simple bar chartA bar chart can also be a good choice for plotting histograms of bucketed\n",
            "numeric values, as in Figure 3-3 , in order to visually explore how the values\n",
            "are distributed :\n",
            "from collections import Counter \n",
            "grades = [83, 95, 91, 87, 70, 0, 85, 82, 100, 67, 73, 77, 0] \n",
            " \n",
            "# Bucket grades by decile, but put 100 in with the 90s \n",
            "histogram = Counter(min(grade // 10 * 10, 90) for grade in grades) \n",
            " \n",
            "plt.bar([x + 5 for x in histogram.keys()],  # Shift bars right by 5 \n",
            "        histogram.values(),                 # Give each bar its correct height \n",
            "        10,                                 # Give each bar a width of 10 \n",
            "        edgecolor=(0, 0, 0))                # Black edges for each bar \n",
            " \n",
            "plt.axis([-5, 105, 0, 5])                  # x-axis from -5 to 105, \n",
            "                                           # y-axis from 0 to 5 \n",
            " \n",
            "plt.xticks([10 * i for i in range(11)])    # x-axis labels at 0, 10, ..., 100 \n",
            "plt.xlabel(\"Decile\") \n",
            "plt.ylabel(\"# of Students\") \n",
            "plt.title(\"Distribution of Exam 1 Grades\") \n",
            "plt.show()Figur e 3-3. Using a bar chart for a histogram\n",
            "The third ar gument to plt.bar  specifies the bar width. Here we chose a\n",
            "width of 10, to fill the entire decile. W e also shifted the bars right by 5, so\n",
            "that, for example, the “10” bar (which corresponds to the decile 10–20)\n",
            "would have its center at 15 and hence occupy the correct range. W e also\n",
            "added a black edge to each bar to make them visually distinct.\n",
            "The call to plt.axis  indicates that we want the x-axis to range from –5 to\n",
            "105 (just to leave a little space on the left and right), and that the y-axis\n",
            "should range from 0 to 5. And the call to plt.xticks  puts x-axis labels at\n",
            "0, 10, 20, …, 100.\n",
            "Be judicious when using plt.axis . When creating bar charts it is\n",
            "considered especially bad form for your y-axis not to start at 0, since this is\n",
            "an easy way to mislead people ( Figure 3-4 ):mentions = [500, 505] \n",
            "years = [2017, 2018] \n",
            " \n",
            "plt.bar(years, mentions, 0.8) \n",
            "plt.xticks(years) \n",
            "plt.ylabel(\"# of times I heard someone say 'data science'\") \n",
            " \n",
            "# if you don't do this, matplotlib will label the x-axis 0, 1 \n",
            "# and then add a +2.013e3 off in the corner (bad matplotlib!) \n",
            "plt.ticklabel_format(useOffset=False) \n",
            " \n",
            "# misleading y-axis only shows the part above 500 \n",
            "plt.axis([2016.5, 2018.5, 499, 506]) \n",
            "plt.title(\"Look at the 'Huge' Increase!\") \n",
            "plt.show()\n",
            "Figur e 3-4. A chart with a misleading y-axis\n",
            "In Figure 3-5 , we use more sensible axes, and it looks far less impressive:\n",
            "plt.axis([2016.5, 2018.5, 0, 550]) \n",
            "plt.title(\"Not So Huge Anymore\")plt.show()\n",
            "Figur e 3-5. The same chart with a nonmisleading y-axis\n",
            "L i n e  C h a r t s\n",
            "As  we saw already , we can make line charts using plt.plot . These are a\n",
            "good choice for showing tr ends , as illustrated in Figure 3-6 :\n",
            "variance     = [1, 2, 4, 8, 16, 32, 64, 128, 256] \n",
            "bias_squared = [256, 128, 64, 32, 16, 8, 4, 2, 1] \n",
            "total_error  = [x + y for x, y in zip(variance, bias_squared)] \n",
            "xs = [i for i, _ in enumerate(variance)] \n",
            " \n",
            "# We can make multiple calls to plt.plot \n",
            "# to show multiple series on the same chart \n",
            "plt.plot(xs, variance,     'g-',  label='variance')    # green solid line \n",
            "plt.plot(xs, bias_squared, 'r-.', label='bias^2')      # red dot-dashed line \n",
            "plt.plot(xs, total_error,  'b:',  label='total error') # blue dotted line# Because we've assigned labels to each series, \n",
            "# we can get a legend for free (loc=9 means \"top center\") \n",
            "plt.legend(loc=9) \n",
            "plt.xlabel(\"model complexity\") \n",
            "plt.xticks([]) \n",
            "plt.title(\"The Bias-Variance Tradeoff\") \n",
            "plt.show()\n",
            "Figur e 3-6. Several line charts with a legend\n",
            "S c a t t e r p l o t s\n",
            "A  scatterplot is the right choice for visualizing the relationship between two\n",
            "paired sets of data. For example, Figure 3-7  illustrates the relationship\n",
            "between the number of friends your users have and the number of minutes\n",
            "they spend on the site every day:friends = [ 70,  65,  72,  63,  71,  64,  60,  64,  67] \n",
            "minutes = [175, 170, 205, 120, 220, 130, 105, 145, 190] \n",
            "labels =  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'] \n",
            " \n",
            "plt.scatter(friends, minutes) \n",
            " \n",
            "# label each point \n",
            "for label, friend_count, minute_count in zip(labels, friends, minutes): \n",
            "    plt.annotate(label, \n",
            "        xy=(friend_count, minute_count), # Put the label with its point \n",
            "        xytext=(5, -5),                  # but slightly offset \n",
            "        textcoords='offset points') \n",
            " \n",
            "plt.title(\"Daily Minutes vs. Number of Friends\") \n",
            "plt.xlabel(\"# of friends\") \n",
            "plt.ylabel(\"daily minutes spent on the site\") \n",
            "plt.show()\n",
            "Figur e 3-7. A scatterplot of friends and time on the site\n",
            "If you’re scattering comparable variables, you might get a misleading\n",
            "picture if you let matplotlib choose the scale, as in Figure 3-8 .Figur e 3-8. A scatterplot with uncomparable axes\n",
            "test_1_grades = [ 99, 90, 85, 97, 80] \n",
            "test_2_grades = [100, 85, 60, 90, 70] \n",
            " \n",
            "plt.scatter(test_1_grades, test_2_grades) \n",
            "plt.title(\"Axes Aren't Comparable\") \n",
            "plt.xlabel(\"test 1 grade\") \n",
            "plt.ylabel(\"test 2 grade\") \n",
            "plt.show()\n",
            "If we include a call to plt.axis(\"equal\") , the plot ( Figure 3-9 ) more\n",
            "accurately shows that most of the variation occurs on test 2.\n",
            "That’ s enough to get you started doing visualization. W e’ll learn much more\n",
            "about visualization throughout the book.Figur e 3-9. The same scatterplot with equal axes\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "The  matplotlib Gallery  will give you a good idea of the sorts of\n",
            "things you can do with matplotlib (and how to do them).\n",
            "seaborn  is  built on top of matplotlib and allows you to easily\n",
            "produce prettier (and more complex) visualizations.\n",
            "Altair  is a newer  Python library for creating declarative\n",
            "visualizations.\n",
            "D3.js  is a JavaScript  library for producing sophisticated interactive\n",
            "visualizations for the web. Although it is not in Python, it is widely\n",
            "used, and it is well worth your while to be familiar with it.\n",
            "Bokeh  is a  library that brings D3-style visualizations into Python.Chapter 4. Linear Algebra\n",
            "Is ther e anything mor e useless or less useful than algebra?\n",
            "— Billy Connolly\n",
            "Linear algebra  is the branch of mathematics that deals with vector spaces .\n",
            "Although I can’ t hope to teach you linear algebra in a brief chapter , it\n",
            "underpins a lar ge number of data science concepts and techniques, which\n",
            "means I owe it to you to at least try . What we learn in this chapter we’ll use\n",
            "heavily throughout the rest of the book.\n",
            "V e c t o r s\n",
            "Abstractly , vectors  are objects  that can be added together to form new\n",
            "vectors and that can be multiplied by scalars  (i.e., numbers), also to form\n",
            "new vectors.\n",
            "Concretely (for us), vectors are points in some finite-dimensional space.\n",
            "Although you might not think of your data as vectors, they are often a\n",
            "useful way to represent numeric data.\n",
            "For example, if you have the heights, weights, and ages of a lar ge number\n",
            "of people, you can treat your data as three-dimensional vectors [height,\n",
            "weight, age] . If you’re teaching a class with four exams, you can treat\n",
            "student grades as four -dimensional vectors [exam1, exam2, exam3,\n",
            "exam4] .\n",
            "The simplest from-scratch approach is to represent vectors as lists of\n",
            "numbers. A list of three numbers corresponds to a vector in three-\n",
            "dimensional space, and vice versa.\n",
            "W e’ll accomplish this with a type alias that says a Vector  is just a list  of\n",
            "float s:from typing import List \n",
            " \n",
            "Vector = List[float] \n",
            " \n",
            "height_weight_age = [70,  # inches, \n",
            "                     170, # pounds, \n",
            "                     40 ] # years \n",
            " \n",
            "grades = [95,   # exam1 \n",
            "          80,   # exam2 \n",
            "          75,   # exam3 \n",
            "          62 ]  # exam4\n",
            "W e’ll  also want to perform arithmetic  on vectors. Because Python list s\n",
            "aren’ t vectors (and hence provide no facilities for vector arithmetic), we’ll\n",
            "need to build these arithmetic tools ourselves. So let’ s start with that.\n",
            "T o begin with, we’ll frequently need to add two vectors. V ectors add\n",
            "componentwise . This means that if two vectors v  and w  are the same length,\n",
            "their sum is just the vector whose first element is v[0] + w[0] , whose\n",
            "second element is v[1] + w[1] , and so on. (If they’re not the same length,\n",
            "then we’re not allowed to add them.)\n",
            "For example, adding the vectors [1, 2]  and [2, 1]  results in [1 + 2, 2\n",
            "+ 1]  or [3, 3] , as shown in Figure 4-1 .Figur e 4-1. Adding two vectors\n",
            "W e can easily implement this by zip -ing the vectors together and using a\n",
            "list comprehension to add the corresponding elements:\n",
            "def add(v: Vector, w: Vector) -> Vector: \n",
            "    \"\"\"Adds corresponding elements\"\"\" \n",
            "    assert len(v) == len(w), \"vectors must be the same length\" \n",
            " \n",
            "    return [v_i + w_i for v_i, w_i in zip(v, w)] \n",
            " \n",
            "assert add([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n",
            "Similarly , to subtract two vectors we just subtract the corresponding\n",
            "elements:\n",
            "def subtract(v: Vector, w: Vector) -> Vector: \n",
            "    \"\"\"Subtracts corresponding elements\"\"\" \n",
            "    assert len(v) == len(w), \"vectors must be the same length\"return [v_i - w_i for v_i, w_i in zip(v, w)] \n",
            " \n",
            "assert subtract([5, 7, 9], [4, 5, 6]) == [1, 2, 3]\n",
            "W e’ll also sometimes want to componentwise sum a list of vectors—that is,\n",
            "create a new vector whose first element is the sum of all the first elements,\n",
            "whose second element is the sum of all the second elements, and so on:\n",
            "def vector_sum(vectors: List[Vector]) -> Vector: \n",
            "    \"\"\"Sums all corresponding elements\"\"\" \n",
            "    # Check that vectors is not empty \n",
            "    assert vectors, \"no vectors provided!\" \n",
            " \n",
            "    # Check the vectors are all the same size \n",
            "    num_elements = len(vectors[0]) \n",
            "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\" \n",
            " \n",
            "    # the i-th element of the result is the sum of every vector[i] \n",
            "    return [sum(vector[i] for vector in vectors) \n",
            "            for i in range(num_elements)] \n",
            " \n",
            "assert vector_sum([[1, 2], [3, 4], [5, 6], [7, 8]]) == [16, 20]\n",
            "W e’ll also need to be able to multiply a vector by a scalar , which we do\n",
            "simply by multiplying each element of the vector by that number:\n",
            "def scalar_multiply(c: float, v: Vector) -> Vector: \n",
            "    \"\"\"Multiplies every element by c\"\"\" \n",
            "    return [c * v_i for v_i in v] \n",
            " \n",
            "assert scalar_multiply(2, [1, 2, 3]) == [2, 4, 6]\n",
            "This allows us to compute the componentwise means of a list of (same-\n",
            "sized) vectors:\n",
            "def vector_mean(vectors: List[Vector]) -> Vector: \n",
            "    \"\"\"Computes the element-wise average\"\"\" \n",
            "    n = len(vectors) \n",
            "    return scalar_multiply(1/n, vector_sum(vectors)) \n",
            " \n",
            "assert vector_mean([[1, 2], [3, 4], [5, 6]]) == [3, 4]A  less obvious tool is the dot pr oduct . The dot product of two vectors is the\n",
            "sum of their componentwise products:\n",
            "def dot(v: Vector, w: Vector) -> float: \n",
            "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\" \n",
            "    assert len(v) == len(w), \"vectors must be same length\" \n",
            " \n",
            "    return sum(v_i * w_i for v_i, w_i in zip(v, w)) \n",
            " \n",
            "assert dot([1, 2, 3], [4, 5, 6]) == 32  # 1 * 4 + 2 * 5 + 3 * 6\n",
            "If w  has magnitude 1, the dot product measures how far the vector v  extends\n",
            "in the w  direction. For example, if w = [1, 0] , then dot(v, w)  is just the\n",
            "first component of v . Another way of saying this is that it’ s the length of the\n",
            "vector you’d get if you pr ojected  v  onto w  ( Figure 4-2 ).\n",
            "Figur e 4-2. The dot pr oduct as vector pr ojectionUsing this, it’ s  easy to compute a vector ’ s sum of squar es :\n",
            "def sum_of_squares(v: Vector) -> float: \n",
            "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\" \n",
            "    return dot(v, v) \n",
            " \n",
            "assert sum_of_squares([1, 2, 3]) == 14  # 1 * 1 + 2 * 2 + 3 * 3\n",
            "which  we can use to compute its magnitude  (or length):\n",
            "import math \n",
            " \n",
            "def magnitude(v: Vector) -> float: \n",
            "    \"\"\"Returns the magnitude (or length) of v\"\"\" \n",
            "    return math.sqrt(sum_of_squares(v))   # math.sqrt is square root function \n",
            " \n",
            "assert magnitude([3, 4]) == 5\n",
            "W e now have all the pieces we need to compute the distance between two\n",
            "vectors, defined as:\n",
            "√(v1−w1)2+...+(vn−wn)2\n",
            "In code:\n",
            "def squared_distance(v: Vector, w: Vector) -> float: \n",
            "    \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\" \n",
            "    return sum_of_squares(subtract(v, w)) \n",
            " \n",
            "def distance(v: Vector, w: Vector) -> float: \n",
            "    \"\"\"Computes the distance between v and w\"\"\" \n",
            "    return math.sqrt(squared_distance(v, w))\n",
            "This is possibly clearer if we write it as (the equivalent):\n",
            "def distance(v: Vector, w: Vector) -> float: \n",
            "    return magnitude(subtract(v, w))\n",
            "That should be plenty to get us started. W e’ll be using these functions\n",
            "heavily throughout the book.N O T E\n",
            "Using  lists as vectors is great for exposition but terrible for performance.\n",
            "In  production code, you would want to use the NumPy library , which includes a high-\n",
            "performance array class with all sorts of arithmetic operations included.\n",
            "M a t r i c e s\n",
            "A matrix  is  a two-dimensional collection of numbers. W e will represent\n",
            "matrices as lists of lists, with each inner list having the same size and\n",
            "representing a r ow  of the matrix. If A  is a matrix, then A[i][j]  is the\n",
            "element in the i th row and the j th column. Per mathematical convention, we\n",
            "will frequently use capital letters to represent matrices. For example:\n",
            "# Another type alias \n",
            "Matrix = List[List[float]] \n",
            " \n",
            "A = [[1, 2, 3],  # A has 2 rows and 3 columns \n",
            "     [4, 5, 6]] \n",
            " \n",
            "B = [[1, 2],     # B has 3 rows and 2 columns \n",
            "     [3, 4], \n",
            "     [5, 6]]\n",
            "N O T E\n",
            "In mathematics, you would usually name the first row of the matrix “row 1” and the first\n",
            "column “column 1.” Because we’re representing matrices with Python list s, which are\n",
            "zero-indexed, we’ll call the first row of a matrix “row 0” and the first column “column\n",
            "0.”\n",
            "Given this list-of-lists representation, the matrix A  has len(A)  rows and\n",
            "len(A[0])  columns, which we consider its shape :\n",
            "from typing import Tupledef shape(A: Matrix) -> Tuple[int, int]: \n",
            "    \"\"\"Returns (# of rows of A, # of columns of A)\"\"\" \n",
            "    num_rows = len(A) \n",
            "    num_cols = len(A[0]) if A else 0   # number of elements in first row \n",
            "    return num_rows, num_cols \n",
            " \n",
            "assert shape([[1, 2, 3], [4, 5, 6]]) == (2, 3)  # 2 rows, 3 columns\n",
            "If a matrix has n  rows and k  columns, we will refer to it as an n × k matrix .\n",
            "W e can (and sometimes will) think of each row of an n × k  matrix as a\n",
            "vector of length k , and each column as a vector of length n :\n",
            "def get_row(A: Matrix, i: int) -> Vector: \n",
            "    \"\"\"Returns the i-th row of A (as a Vector)\"\"\" \n",
            "    return A[i]             # A[i] is already the ith row \n",
            " \n",
            "def get_column(A: Matrix, j: int) -> Vector: \n",
            "    \"\"\"Returns the j-th column of A (as a Vector)\"\"\" \n",
            "    return [A_i[j]          # jth element of row A_i \n",
            "            for A_i in A]   # for each row A_i\n",
            "W e’ll also want to be able to create a matrix given its shape and a function\n",
            "for generating its elements. W e can do this using a nested list\n",
            "comprehension:\n",
            "from typing import Callable \n",
            " \n",
            "def make_matrix(num_rows: int, \n",
            "                num_cols: int, \n",
            "                entry_fn: Callable[[int, int], float]) -> Matrix: \n",
            "    \"\"\" \n",
            "    Returns a num_rows x num_cols matrix \n",
            "    whose (i,j)-th entry is entry_fn(i, j) \n",
            "    \"\"\" \n",
            "    return [[entry_fn(i, j)             # given i, create a list \n",
            "             for j in range(num_cols)]  #   [entry_fn(i, 0), ... ] \n",
            "            for i in range(num_rows)]   # create one list for each i\n",
            "Given  this function, you could make a 5 × 5 identity matrix  (with 1s on the\n",
            "diagonal and 0s elsewhere) like so:def identity_matrix(n: int) -> Matrix: \n",
            "    \"\"\"Returns the n x n identity matrix\"\"\" \n",
            "    return make_matrix(n, n, lambda i, j: 1 if i == j else 0) \n",
            " \n",
            "assert identity_matrix(5) == [[1, 0, 0, 0, 0], \n",
            "                              [0, 1, 0, 0, 0], \n",
            "                              [0, 0, 1, 0, 0], \n",
            "                              [0, 0, 0, 1, 0], \n",
            "                              [0, 0, 0, 0, 1]]\n",
            "Matrices will be important to us for several reasons.\n",
            "First, we can use a matrix to represent a dataset consisting of multiple\n",
            "vectors, simply by considering each vector as a row of the matrix. For\n",
            "example, if you had the heights, weights, and ages of 1,000 people, you\n",
            "could put them in a 1,000 × 3 matrix:\n",
            "data = [[70, 170, 40], \n",
            "        [65, 120, 26], \n",
            "        [77, 250, 19], \n",
            "        # .... \n",
            "       ]\n",
            "Second, as we’ll see later , we can use an n × k  matrix to represent a linear\n",
            "function that maps k -dimensional vectors to n -dimensional vectors. Several\n",
            "of our techniques and concepts will involve such functions.\n",
            "Third, matrices can be used to represent binary relationships. In Chapter 1 ,\n",
            "we represented the edges of a network as a collection of pairs (i, j) . An\n",
            "alternative representation would be to create a matrix A  such that A[i][j]\n",
            "is 1 if nodes i  and j  are connected and 0 otherwise.\n",
            "Recall that before we had:\n",
            "friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4), \n",
            "               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n",
            "W e could also represent this as:\n",
            "#            user 0  1  2  3  4  5  6  7  8  9 \n",
            "#friend_matrix = [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0],  # user 0 \n",
            "                 [1, 0, 1, 1, 0, 0, 0, 0, 0, 0],  # user 1 \n",
            "                 [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],  # user 2 \n",
            "                 [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],  # user 3 \n",
            "                 [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],  # user 4 \n",
            "                 [0, 0, 0, 0, 1, 0, 1, 1, 0, 0],  # user 5 \n",
            "                 [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],  # user 6 \n",
            "                 [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],  # user 7 \n",
            "                 [0, 0, 0, 0, 0, 0, 1, 1, 0, 1],  # user 8 \n",
            "                 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]  # user 9\n",
            "If there are very few connections, this is a much more inef ficient\n",
            "representation, since you end up having to store a lot of zeros. However ,\n",
            "with the matrix representation it is much quicker to check whether two\n",
            "nodes are connected—you just have to do a matrix lookup instead of\n",
            "(potentially) inspecting every edge:\n",
            "assert friend_matrix[0][2] == 1, \"0 and 2 are friends\" \n",
            "assert friend_matrix[0][8] == 0, \"0 and 8 are not friends\"\n",
            "Similarly , to find a node’ s connections, you only need to inspect the column\n",
            "(or the row) corresponding to that node:\n",
            "# only need to look at one row \n",
            "friends_of_five = [i \n",
            "                   for i, is_friend in enumerate(friend_matrix[5]) \n",
            "                   if is_friend]\n",
            "W ith a small graph you could just add a list of connections to each node\n",
            "object to speed up this process; but for a lar ge, evolving graph that would\n",
            "probably be too expensive and dif ficult to maintain.\n",
            "W e’ll revisit matrices throughout the book.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "Linear algebra  is widely used by data scientists (frequently\n",
            "implicitly , and not infrequently by people who don’ t understand it).It wouldn’ t be a bad idea to read a textbook. Y ou can find several\n",
            "freely available online:\n",
            "Linear Algebra , by Jim Hef feron (Saint Michael’ s\n",
            "College)\n",
            "Linear Algebra , by David Cherney , T om Denton, Rohit\n",
            "Thomas, and Andrew W aldron (UC Davis)\n",
            "If you are feeling adventurous, Linear Algebra Done\n",
            "W r ong , by Ser gei T reil (Brown University), is a more\n",
            "advanced introduction.\n",
            "All  of the machinery we built in this chapter you get for free if you\n",
            "use NumPy . (Y ou get a lot more too, including much better\n",
            "performance.)Chapter 5. Statistics\n",
            "Facts ar e stubborn, but statistics ar e mor e pliable.\n",
            "— Mark T wain\n",
            "Statistics  refers  to the mathematics and techniques with which we\n",
            "understand data. It is a rich, enormous field, more suited to a shelf (or\n",
            "room) in a library than a chapter in a book, and so our discussion will\n",
            "necessarily not be a deep one. Instead, I’ll try to teach you just enough to be\n",
            "dangerous, and pique your interest just enough that you’ll go of f and learn\n",
            "more.\n",
            "D e s c r i b i n g  a  S i n g l e  S e t  o f  D a t a\n",
            "Through  a combination of word of mouth and luck, DataSciencester has\n",
            "grown to dozens of members, and the VP of Fundraising asks you for some\n",
            "sort of description of how many friends your members have that he can\n",
            "include in his elevator pitches.\n",
            "Using techniques from Chapter 1 , you are easily able to produce this data.\n",
            "But now you are faced with the problem of how to describe  it.\n",
            "One obvious description of any dataset is simply the data itself:\n",
            "num_friends = [100, 49, 41, 40, 25, \n",
            "               # ... and lots more \n",
            "              ]\n",
            "For a small enough dataset, this might even be the best description. But for\n",
            "a lar ger dataset, this is unwieldy and probably opaque. (Imagine staring at a\n",
            "list of 1 million numbers.) For that reason, we use statistics to distill and\n",
            "communicate relevant features of our data.As  a first approach, you put the friend counts into a histogram using\n",
            "Counter  and plt.bar  ( Figure 5-1 ):\n",
            "from collections import Counter \n",
            "import matplotlib.pyplot as plt \n",
            " \n",
            "friend_counts = Counter(num_friends) \n",
            "xs = range(101)                         # largest value is 100 \n",
            "ys = [friend_counts[x] for x in xs]     # height is just # of friends \n",
            "plt.bar(xs, ys) \n",
            "plt.axis([0, 101, 0, 25]) \n",
            "plt.title(\"Histogram of Friend Counts\") \n",
            "plt.xlabel(\"# of friends\") \n",
            "plt.ylabel(\"# of people\") \n",
            "plt.show()\n",
            "Figur e 5-1. A histogram of friend countsUnfortunately , this  chart is still too dif ficult to slip into conversations. So\n",
            "you start generating some statistics. Probably the simplest statistic is the\n",
            "number of data points:\n",
            "num_points = len(num_friends)               # 204\n",
            "Y ou’re  probably also interested in the lar gest and smallest values:\n",
            "largest_value = max(num_friends)            # 100 \n",
            "smallest_value = min(num_friends)           # 1\n",
            "which  are just special cases of wanting to know the values in specific\n",
            "positions:\n",
            "sorted_values = sorted(num_friends) \n",
            "smallest_value = sorted_values[0]           # 1 \n",
            "second_smallest_value = sorted_values[1]    # 1 \n",
            "second_largest_value = sorted_values[-2]    # 49\n",
            "But we’re only getting started.\n",
            "Central T endencies\n",
            "Usually , we’ll  want some notion of where our data is centered. Most\n",
            "commonly we’ll use the mean  (or average), which is just the sum of the\n",
            "data divided by its count:\n",
            "def mean(xs: List[float]) -> float: \n",
            "    return sum(xs) / len(xs) \n",
            " \n",
            "mean(num_friends)   # 7.333333\n",
            "If you have two data points, the mean is simply the point halfway between\n",
            "them. As you add more points, the mean shifts around, but it always\n",
            "depends on the value of every point. For example, if you have 10 data\n",
            "points, and you increase the value of any of them by 1, you increase the\n",
            "mean by 0.1.W e’ll also sometimes be interested  in the median , which is the middle-most\n",
            "value (if the number of data points is odd) or the average of the two middle-\n",
            "most values (if the number of data points is even).\n",
            "For instance, if we have five data points in a sorted vector x , the median is\n",
            "x[5 // 2]  or x[2] . If we have six data points, we want the average of\n",
            "x[2]  (the third point) and x[3]  (the fourth point).\n",
            "Notice that—unlike the mean—the median doesn’ t fully depend on every\n",
            "value in your data. For example, if you make the lar gest point lar ger (or the\n",
            "smallest point smaller), the middle points remain unchanged, which means\n",
            "so does the median.\n",
            "W e’ll write dif ferent functions for the even and odd cases and combine\n",
            "them:\n",
            "# The underscores indicate that these are \"private\" functions, as they're \n",
            "# intended to be called by our median function but not by other people \n",
            "# using our statistics library. \n",
            "def _median_odd(xs: List[float]) -> float: \n",
            "    \"\"\"If len(xs) is odd, the median is the middle element\"\"\" \n",
            "    return sorted(xs)[len(xs) // 2] \n",
            " \n",
            "def _median_even(xs: List[float]) -> float: \n",
            "    \"\"\"If len(xs) is even, it's the average of the middle two elements\"\"\" \n",
            "    sorted_xs = sorted(xs) \n",
            "    hi_midpoint = len(xs) // 2  # e.g. length 4 => hi_midpoint 2 \n",
            "    return (sorted_xs[hi_midpoint - 1] + sorted_xs[hi_midpoint]) / 2 \n",
            " \n",
            "def median(v: List[float]) -> float: \n",
            "    \"\"\"Finds the 'middle-most' value of v\"\"\" \n",
            "    return _median_even(v) if len(v) % 2 == 0 else _median_odd(v) \n",
            " \n",
            "assert median([1, 10, 2, 9, 5]) == 5 \n",
            "assert median([1, 9, 2, 10]) == (2 + 9) / 2\n",
            "And now we can compute the median number of friends:\n",
            "print(median(num_friends))  # 6Clearly , the mean is simpler to compute, and it varies smoothly as our data\n",
            "changes. If we have n  data points and one of them increases by some small\n",
            "amount e , then necessarily the mean will increase by e  / n . (This makes the\n",
            "mean amenable to all sorts of calculus tricks.) In order to find the median,\n",
            "however , we have to sort our data. And changing one of our data points by a\n",
            "small amount e  might increase the median by e , by some number less than\n",
            "e , or not at all (depending on the rest of the data).\n",
            "N O T E\n",
            "There are, in fact, nonobvious tricks to ef ficiently compute medians  without sorting the\n",
            "data. However , they are beyond the scope of this book, so we  have to sort the data.\n",
            "At the same time, the mean is very sensitive to outliers in our data. If our\n",
            "friendliest user had 200 friends (instead of 100), then the mean would rise\n",
            "to 7.82, while the median would stay the same. If outliers are likely to be\n",
            "bad data (or otherwise unrepresentative of whatever phenomenon we’re\n",
            "trying to understand), then the mean can sometimes give us a misleading\n",
            "picture. For example, the story is often told that in the mid-1980s, the major\n",
            "at the University of North Carolina with the highest average starting salary\n",
            "was geography , mostly because of NBA star (and outlier) Michael Jordan.\n",
            "A  generalization of the median is the quantile , which represents the value\n",
            "under which a certain percentile of the data lies (the median represents the\n",
            "value under which 50% of the data lies):\n",
            "def quantile(xs: List[float], p: float) -> float: \n",
            "    \"\"\"Returns the pth-percentile value in x\"\"\" \n",
            "    p_index = int(p * len(xs)) \n",
            "    return sorted(xs)[p_index] \n",
            " \n",
            "assert quantile(num_friends, 0.10) == 1 \n",
            "assert quantile(num_friends, 0.25) == 3 \n",
            "assert quantile(num_friends, 0.75) == 9 \n",
            "assert quantile(num_friends, 0.90) == 13Less  commonly you might want to look at the mode , or most common\n",
            "value(s):\n",
            "def mode(x: List[float]) -> List[float]: \n",
            "    \"\"\"Returns a list, since there might be more than one mode\"\"\" \n",
            "    counts = Counter(x) \n",
            "    max_count = max(counts.values()) \n",
            "    return [x_i for x_i, count in counts.items() \n",
            "            if count == max_count] \n",
            " \n",
            "assert set(mode(num_friends)) == {1, 6}\n",
            "But most frequently we’ll just use the mean.\n",
            "Dispersion\n",
            "Dispersion  refers  to measures of how spread out our data is. T ypically\n",
            "they’re statistics for which values near zero signify not spr ead out at all  and\n",
            "for which lar ge values (whatever that means) signify very spr ead out . For\n",
            "instance, a very simple measure is the range , which is just the dif ference\n",
            "between the lar gest and smallest elements:\n",
            "# \"range\" already means something in Python, so we'll use a different name \n",
            "def data_range(xs: List[float]) -> float: \n",
            "    return max(xs) - min(xs) \n",
            " \n",
            "assert data_range(num_friends) == 99\n",
            "The range is zero precisely when the max  and min  are equal, which can only\n",
            "happen if the elements of x  are all the same, which means the data is as\n",
            "undispersed as possible. Conversely , if the range is lar ge, then the max  is\n",
            "much lar ger than the min  and the data is more spread out.\n",
            "Like the median, the range doesn’ t really depend on the whole dataset. A\n",
            "dataset whose points are all either 0 or 100 has the same range as a dataset\n",
            "whose values are 0, 100, and lots of 50s. But it seems like the first dataset\n",
            "“should” be more spread out.A more complex measure of dispersion  is the variance , which is computed\n",
            "as:\n",
            "from scratch.linear_algebra import sum_of_squares \n",
            " \n",
            "def de_mean(xs: List[float]) -> List[float]: \n",
            "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\" \n",
            "    x_bar = mean(xs) \n",
            "    return [x - x_bar for x in xs] \n",
            " \n",
            "def variance(xs: List[float]) -> float: \n",
            "    \"\"\"Almost the average squared deviation from the mean\"\"\" \n",
            "    assert len(xs) >= 2, \"variance requires at least two elements\" \n",
            " \n",
            "    n = len(xs) \n",
            "    deviations = de_mean(xs) \n",
            "    return sum_of_squares(deviations) / (n - 1) \n",
            " \n",
            "assert 81.54 < variance(num_friends) < 81.55\n",
            "N O T E\n",
            "This looks like it is almost the average squared deviation from the mean, except that\n",
            "we’re dividing by n - 1  instead of n . In fact, when we’re dealing with a sample from a\n",
            "lar ger population, x_bar  is only an estimate  of the actual mean, which means that on\n",
            "average (x_i - x_bar) ** 2  is an underestimate of x_i ’ s squared deviation from the\n",
            "mean, which is why we divide by n - 1  instead of n . See W ikipedia .\n",
            "Now , whatever units our data is in (e.g., “friends”), all of our measures of\n",
            "central tendency are in that same unit. The range will similarly be in that\n",
            "same unit. The variance, on the other hand, has units that are the squar e  of\n",
            "the original units (e.g., “friends squared”). As it can be hard to make sense\n",
            "of these, we often look  instead at the standar d deviation :\n",
            "import math \n",
            " \n",
            "def standard_deviation(xs: List[float]) -> float: \n",
            "    \"\"\"The standard deviation is the square root of the variance\"\"\" \n",
            "    return math.sqrt(variance(xs))assert 9.02 < standard_deviation(num_friends) < 9.04\n",
            "Both the range and the standard deviation have the same outlier problem\n",
            "that we saw earlier for the mean. Using the same example, if our friendliest\n",
            "user had instead 200 friends, the standard deviation would be 14.89—more\n",
            "than 60% higher!\n",
            "A more robust alternative computes the dif ference between the 75th\n",
            "percentile value and the 25th percentile value:\n",
            "def interquartile_range(xs: List[float]) -> float: \n",
            "    \"\"\"Returns the difference between the 75%-ile and the 25%-ile\"\"\" \n",
            "    return quantile(xs, 0.75) - quantile(xs, 0.25) \n",
            " \n",
            "assert interquartile_range(num_friends) == 6\n",
            "which is quite plainly unaf fected by a small number of outliers.\n",
            "C o r r e l a t i o n\n",
            "DataSciencester ’ s VP of Growth  has a theory that the amount of time\n",
            "people spend on the site is related to the number of friends they have on the\n",
            "site (she’ s not a VP for nothing), and she’ s asked you to verify this.\n",
            "After digging through traf fic logs, you’ve come up with a list called\n",
            "daily_minutes  that shows how many minutes per day each user spends on\n",
            "DataSciencester , and you’ve ordered it so that its elements correspond to\n",
            "the elements of our previous num_friends  list. W e’d like to investigate the\n",
            "relationship between these two metrics.\n",
            "W e’ll  first look at covariance , the paired analogue of variance. Whereas\n",
            "variance measures how a single variable deviates from its mean, covariance\n",
            "measures how two variables vary in tandem from their means:\n",
            "from scratch.linear_algebra import dot \n",
            " \n",
            "def covariance(xs: List[float], ys: List[float]) -> float:assert len(xs) == len(ys), \"xs and ys must have same number of elements\" \n",
            " \n",
            "    return dot(de_mean(xs), de_mean(ys)) / (len(xs) - 1) \n",
            " \n",
            "assert 22.42 < covariance(num_friends, daily_minutes) < 22.43 \n",
            "assert 22.42 / 60 < covariance(num_friends, daily_hours) < 22.43 / 60\n",
            "Recall that dot  sums up the products of corresponding pairs of elements.\n",
            "When corresponding elements of x  and y  are either both above their means\n",
            "or both below their means, a positive number enters the sum. When one is\n",
            "above its mean and the other below , a negative number enters the sum.\n",
            "Accordingly , a “lar ge” positive covariance means that x  tends to be lar ge\n",
            "when y  is lar ge and small when y  is small. A “lar ge” negative covariance\n",
            "means the opposite—that x  tends to be small when y  is lar ge and vice versa.\n",
            "A covariance close to zero means that no such relationship exists.\n",
            "Nonetheless, this number can be hard to interpret, for a couple of reasons:\n",
            "Its units are the product of the inputs’ units (e.g., friend-minutes-\n",
            "per -day), which can be hard to make sense of. (What’ s a “friend-\n",
            "minute-per -day”?)\n",
            "If each user had twice as many friends (but the same number of\n",
            "minutes), the covariance would be twice as lar ge. But in a sense,\n",
            "the variables would be just as interrelated. Said dif ferently , it’ s hard\n",
            "to say what counts as a “lar ge” covariance.\n",
            "For this reason, it’ s more common to look at the corr elation , which divides\n",
            "out the standard deviations of both variables:\n",
            "def correlation(xs: List[float], ys: List[float]) -> float: \n",
            "    \"\"\"Measures how much xs and ys vary in tandem about their means\"\"\" \n",
            "    stdev_x = standard_deviation(xs) \n",
            "    stdev_y = standard_deviation(ys) \n",
            "    if stdev_x > 0 and stdev_y > 0: \n",
            "        return covariance(xs, ys) / stdev_x / stdev_y \n",
            "    else: \n",
            "        return 0    # if no variation, correlation is zeroassert 0.24 < correlation(num_friends, daily_minutes) < 0.25 \n",
            "assert 0.24 < correlation(num_friends, daily_hours) < 0.25\n",
            "The correlation  is unitless and always lies between –1 (perfect\n",
            "anticorrelation) and 1 (perfect correlation). A number like 0.25 represents a\n",
            "relatively weak positive correlation.\n",
            "However , one thing we neglected to do was examine our data. Check out\n",
            "Figure 5-2 .\n",
            "Figur e 5-2. Corr elation with an outlier\n",
            "The person with 100 friends (who spends only 1 minute per day on the site)\n",
            "is a huge outlier , and correlation can be very sensitive to outliers. What\n",
            "happens if we ignore him?\n",
            "outlier = num_friends.index(100)    # index of outliernum_friends_good = [x \n",
            "                    for i, x in enumerate(num_friends) \n",
            "                    if i != outlier] \n",
            " \n",
            "daily_minutes_good = [x \n",
            "                      for i, x in enumerate(daily_minutes) \n",
            "                      if i != outlier] \n",
            " \n",
            "daily_hours_good = [dm / 60 for dm in daily_minutes_good] \n",
            " \n",
            "assert 0.57 < correlation(num_friends_good, daily_minutes_good) < 0.58 \n",
            "assert 0.57 < correlation(num_friends_good, daily_hours_good) < 0.58\n",
            "W ithout the outlier , there is a much stronger correlation ( Figure 5-3 ).\n",
            "Figur e 5-3. Corr elation after r emoving the outlier\n",
            "Y ou investigate further and discover that the outlier was actually an internal\n",
            "test  account that no one ever bothered to remove. So you feel justified in\n",
            "excluding it.S i m p s o n ’ s  P a r a d o x\n",
            "One  not uncommon surprise when analyzing data is Simpson’ s paradox , in\n",
            "which correlations can  be misleading when confounding  variables are\n",
            "ignored.\n",
            "For example, imagine that you can identify all of your members as either\n",
            "East Coast data scientists or W est Coast data scientists. Y ou decide to\n",
            "examine which coast’ s data scientists are friendlier:\n",
            "Coast # of members A vg. # of friends\n",
            "W est Coast 101 8.2\n",
            "East Coast 103 6.5\n",
            "It certainly looks like the W est Coast data scientists are friendlier than the\n",
            "East Coast data scientists. Y our coworkers advance all sorts of theories as to\n",
            "why this might be: maybe it’ s the sun, or the cof fee, or the or ganic produce,\n",
            "or the laid-back Pacific vibe?\n",
            "But when playing with the data, you discover something very strange. If\n",
            "you look only at people with PhDs, the East Coast data scientists have more\n",
            "friends on average. And if you look only at people without PhDs, the East\n",
            "Coast data scientists also have more friends on average!\n",
            "Coast Degree # of members A vg. # of friends\n",
            "W est Coast PhD 35 3.1\n",
            "East Coast PhD 70 3.2\n",
            "W est Coast No PhD 66 10.9\n",
            "East Coast No PhD 33 13.4\n",
            "Once you account for the users’ degrees, the correlation goes in the\n",
            "opposite direction! Bucketing the data as East Coast/W est Coast disguisedthe fact that the East Coast data scientists skew much more heavily toward\n",
            "PhD types.\n",
            "This phenomenon crops up in the real world with some regularity . The key\n",
            "issue is that correlation is measuring the relationship between your two\n",
            "variables all else being equal . If your dataclasses are assigned at random, as\n",
            "they might be in a well-designed experiment, “all else being equal” might\n",
            "not be a terrible assumption. But when there is a deeper pattern to class\n",
            "assignments, “all else being equal” can be an awful assumption.\n",
            "The only real way to avoid this is by knowing your data  and by doing what\n",
            "you can to make sure you’ve checked for possible confounding factors.\n",
            "Obviously , this is not always possible. If you didn’ t have data on the\n",
            "educational attainment of these 200 data scientists, you might simply\n",
            "conclude that there was something inherently more sociable about the W est\n",
            "Coast.\n",
            "S o m e  O t h e r  C o r r e l a t i o n a l  C a v e a t s\n",
            "A  correlation of zero indicates that there is no linear relationship between\n",
            "the two variables. However , there may be other sorts of relationships. For\n",
            "example, if:\n",
            "x = [-2, -1, 0, 1, 2] \n",
            "y = [ 2,  1, 0, 1, 2]\n",
            "then x  and y  have zero correlation. But they certainly have a relationship—\n",
            "each element of y  equals the absolute value of the corresponding element of\n",
            "x . What they don’ t have is a relationship in which knowing how x_i\n",
            "compares to mean(x)  gives us information about how y_i  compares to\n",
            "mean(y) . That is the sort of relationship that correlation looks for .\n",
            "In addition, correlation tells you nothing about how lar ge the relationship is.\n",
            "The variables:x = [-2, -1, 0, 1, 2] \n",
            "y = [99.98, 99.99, 100, 100.01, 100.02]\n",
            "are perfectly correlated, but (depending on what you’re measuring) it’ s\n",
            "quite possible that this relationship isn’ t all that interesting.\n",
            "C o r r e l a t i o n  a n d  C a u s a t i o n\n",
            "Y ou  have probably heard at some point that “correlation is not causation,”\n",
            "most likely from someone looking at data that posed a challenge to parts of\n",
            "his worldview that he was reluctant to question. Nonetheless, this is an\n",
            "important point—if x  and y  are strongly correlated, that might mean that x\n",
            "causes y , that y  causes x , that each causes the other , that some third factor\n",
            "causes both, or nothing at all.\n",
            "Consider the relationship between num_friends  and daily_minutes . It’ s\n",
            "possible that having more friends on the site causes  DataSciencester users\n",
            "to spend more time on the site. This might be the case if each friend posts a\n",
            "certain amount of content each day , which means that the more friends you\n",
            "have, the more time it takes to stay current with their updates.\n",
            "However , it’ s also possible that the more time users spend ar guing in the\n",
            "DataSciencester forums, the more they encounter and befriend like-minded\n",
            "people. That is, spending more time on the site causes  users to have more\n",
            "friends.\n",
            "A third possibility is that the users who are most passionate about data\n",
            "science spend more time on the site (because they find it more interesting)\n",
            "and more actively collect data science friends (because they don’ t want to\n",
            "associate with anyone else).\n",
            "One way to feel more confident about causality is by conducting\n",
            "randomized trials. If you can randomly split your users into two groups with\n",
            "similar demographics and give one of the groups a slightly dif ferent\n",
            "experience, then you can often feel pretty good that the dif ferent\n",
            "experiences are causing the dif ferent outcomes.For instance, if you don’ t mind being angrily accused of\n",
            "https://www .nytimes.com/2014/06/30/technology/facebook-tinkers-with-\n",
            "users-emotions-in-news-feed-experiment-stirring-outcry .html?\n",
            "r=0[experimenting on your users], you could randomly choose a subset of\n",
            "your users and show them content fr om only a fraction of their friends. If\n",
            "this subset subsequently spent less time on the site, this would give you\n",
            "some confidence that having mor e friends _causes  more time to be spent on\n",
            "the site.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "SciPy , pandas , and StatsModels  all  come with a wide variety of\n",
            "statistical functions.\n",
            "Statistics  is important . (Or maybe statistics ar e  important?) If you\n",
            "want to be a better data scientist, it would be a good idea to read a\n",
            "statistics textbook. Many are freely available online, including:\n",
            "Intr oductory Statistics , by Douglas Shafer and Zhiyi\n",
            "Zhang (Saylor Foundation)\n",
            "OnlineStatBook , by David Lane (Rice University)\n",
            "Intr oductory Statistics , by OpenStax (OpenStax College)Chapter 6. Probability\n",
            "The laws of pr obability , so true in general, so fallacious in particular .\n",
            "— Edward Gibbon\n",
            "It  is hard to do data science without some sort of understanding of\n",
            "pr obability  and its mathematics. As with our treatment of statistics in\n",
            "Chapter 5 , we’ll wave our hands a lot and elide many of the technicalities.\n",
            "For  our purposes you should think of probability as a way of quantifying\n",
            "the uncertainty associated with events  chosen from some universe  of events.\n",
            "Rather than getting technical about what these terms mean, think of rolling\n",
            "a die. The universe consists of all possible outcomes. And any subset of\n",
            "these outcomes is an event; for example, “the die rolls a 1” or “the die rolls\n",
            "an even number .”\n",
            "Notationally , we write P ( E ) to mean “the probability of the event E .”\n",
            "W e’ll use probability theory to build models. W e’ll use probability theory to\n",
            "evaluate models. W e’ll use probability theory all over the place.\n",
            "One could, were one so inclined, get really deep into the philosophy of what\n",
            "probability theory means . (This is best done over beers.) W e won’ t be doing\n",
            "that.\n",
            "D e p e n d e n c e  a n d  I n d e p e n d e n c e\n",
            "Roughly  speaking, we say that two events E  and F  are dependent  if\n",
            "knowing something about whether E  happens gives us information about\n",
            "whether F  happens (and vice versa). Otherwise, they are independent .\n",
            "For instance, if we flip a fair coin twice, knowing whether the first flip is\n",
            "heads gives us no information about whether the second flip is heads. These\n",
            "events are independent. On the other hand, knowing whether the first flip is\n",
            "heads certainly gives us information about whether both flips are tails. (Ifthe first flip is heads, then definitely it’ s not the case that both flips are\n",
            "tails.) These two events are dependent.\n",
            "Mathematically , we say that two events E  and F  are independent if the\n",
            "probability that they both happen is the product of the probabilities that\n",
            "each one happens:\n",
            "P(E,F)=P(E)P(F)\n",
            "In the example, the probability of “first flip heads” is 1/2, and the\n",
            "probability of “both flips tails” is 1/4, but the probability of “first flip heads\n",
            "and  both flips tails” is 0.\n",
            "C o n d i t i o n a l  P r o b a b i l i t y\n",
            "When  two events E  and F  are independent, then by definition we have:\n",
            "P(E,F)=P(E)P(F)\n",
            "If they are not necessarily independent (and if the probability of F  is not\n",
            "zero), then we define the probability of E  “conditional on F ” as:\n",
            "P(E|F)=P(E,F)/P(F)\n",
            "Y ou should think of this as the probability that E  happens, given that we\n",
            "know that F  happens.\n",
            "W e often rewrite this as:\n",
            "P(E,F)=P(E|F)P(F)\n",
            "When E  and F  are independent, you can check that this gives:\n",
            "P(E|F)=P(E)\n",
            "which is the mathematical way of expressing that knowing F  occurred gives\n",
            "us no additional information about whether E  occurred.One common tricky example involves a family with two (unknown)\n",
            "children. If we assume that:\n",
            "Each child is equally likely to be a boy or a girl.\n",
            "The gender of the second child is independent of the gender of the\n",
            "first child.\n",
            "Then the event “no girls” has probability 1/4, the event “one girl, one boy”\n",
            "has probability 1/2, and the event “two girls” has probability 1/4.\n",
            "Now we can ask what is the probability of the event “both children are\n",
            "girls” ( B ) conditional on the event “the older child is a girl” ( G )? Using the\n",
            "definition of conditional probability:\n",
            "P(B|G)=P(B,G)/P(G)=P(B)/P(G)=1/2\n",
            "since the event B  and G  (“both children are girls and  the older child is a\n",
            "girl”) is just the event B . (Once you know that both children are girls, it’ s\n",
            "necessarily true that the older child is a girl.)\n",
            "Most likely this result accords with your intuition.\n",
            "W e could also ask about the probability of the event “both children are\n",
            "girls” conditional on the event “at least one of the children is a girl” ( L ).\n",
            "Surprisingly , the answer is dif ferent from before!\n",
            "As before, the event B  and L  (“both children are girls and  at least one of the\n",
            "children is a girl”) is just the event B . This means we have:\n",
            "P(B|L)=P(B,L)/P(L)=P(B)/P(L)=1/3\n",
            "How can this be the case? W ell, if all you know is that at least one of the\n",
            "children is a girl, then it is twice as likely that the family has one boy and\n",
            "one girl than that it has both girls.\n",
            "W e can check this by “generating” a lot of families:\n",
            "import enum, random# An Enum is a typed set of enumerated values. We can use them \n",
            "# to make our code more descriptive and readable. \n",
            "class Kid(enum.Enum): \n",
            "    BOY = 0 \n",
            "    GIRL = 1 \n",
            " \n",
            "def random_kid() -> Kid: \n",
            "    return random.choice([Kid.BOY, Kid.GIRL]) \n",
            " \n",
            "both_girls = 0 \n",
            "older_girl = 0 \n",
            "either_girl = 0 \n",
            " \n",
            "random.seed(0) \n",
            " \n",
            "for _ in range(10000): \n",
            "    younger = random_kid() \n",
            "    older = random_kid() \n",
            "    if older == Kid.GIRL: \n",
            "        older_girl += 1 \n",
            "    if older == Kid.GIRL and younger == Kid.GIRL: \n",
            "        both_girls += 1 \n",
            "    if older == Kid.GIRL or younger == Kid.GIRL: \n",
            "        either_girl += 1 \n",
            " \n",
            "print(\"P(both | older):\", both_girls / older_girl)     # 0.514 ~ 1/2 \n",
            "print(\"P(both | either): \", both_girls / either_girl)  # 0.342 ~ 1/3\n",
            "B a y e s ’ s  T h e o r e m\n",
            "One  of the data scientist’ s best friends is Bayes’ s theorem, which is a way\n",
            "of “reversing” conditional probabilities. Let’ s say we need to know the\n",
            "probability of some event E  conditional on some other event F  occurring.\n",
            "But we only have information about the probability of F  conditional on E\n",
            "occurring. Using the definition of conditional probability twice tells us that:\n",
            "P(E|F)=P(E,F)/P(F)=P(F|E)P(E)/P(F)\n",
            "The event F  can be split into the two mutually exclusive events “ F  and E ”\n",
            "and “ F  and not E .” If we write ¬E  for “not E ” (i.e., “ E  doesn’ t happen”),\n",
            "then:P(F)=P(F,E)+P(F,¬E)\n",
            "so that:\n",
            "P(E|F)=P(F|E)P(E)/[P(F|E)P(E)+P(F|¬E)P(¬E)]\n",
            "which is how Bayes’ s theorem is often stated.\n",
            "This theorem often gets used to demonstrate why data scientists are smarter\n",
            "than doctors. Imagine a certain disease that af fects 1 in every 10,000\n",
            "people. And imagine that there is a test for this disease that gives the correct\n",
            "result (“diseased” if you have the disease, “nondiseased” if you don’ t) 99%\n",
            "of the time.\n",
            "What does a positive test mean? Let’ s use T  for the event “your test is\n",
            "positive” and D  for the event “you have the disease.” Then Bayes’ s theorem\n",
            "says that the probability that you have the disease, conditional on testing\n",
            "positive, is:\n",
            "P(D|T)=P(T|D)P(D)/[P(T|D)P(D)+P(T|¬D)P(¬D)]\n",
            "Here we know that P(T|D) , the probability that someone with the disease\n",
            "tests positive, is 0.99. P ( D ), the probability that any given person has the\n",
            "disease, is 1/10,000 = 0.0001. P(T|¬D) , the probability that someone\n",
            "without the disease tests positive, is 0.01. And P(¬D) , the probability that\n",
            "any given person doesn’ t have the disease, is 0.9999. If you substitute these\n",
            "numbers into Bayes’ s theorem, you find:\n",
            "P(D|T)=0.98%\n",
            "That is, less than 1% of the people who test positive actually have the\n",
            "disease.N O T E\n",
            "This assumes that people take the test more or less at random. If only people with\n",
            "certain symptoms take the test, we would instead have to condition on the event\n",
            "“positive test and  symptoms” and the number would likely be a lot higher .\n",
            "A more intuitive way to see this is to imagine a population of 1 million\n",
            "people. Y ou’d expect 100 of them to have the disease, and 99 of those 100\n",
            "to test positive. On the other hand, you’d expect 999,900 of them not to\n",
            "have the disease, and 9,999 of those to test positive. That means you’d\n",
            "expect only 99 out of (99 + 9999) positive testers to actually have the\n",
            "disease.\n",
            "R a n d o m  V a r i a b l e s\n",
            "A random variable  is  a variable whose possible values have an associated\n",
            "probability distribution. A very simple random variable equals 1 if a coin\n",
            "flip turns up heads and 0 if the flip turns up tails. A more complicated one\n",
            "might measure the number of heads you observe when flipping a coin 10\n",
            "times or a value picked from range(10)  where each number is equally\n",
            "likely .\n",
            "The associated distribution gives the probabilities that the variable realizes\n",
            "each of its possible values. The coin flip variable equals 0 with probability\n",
            "0.5 and 1 with probability 0.5. The range(10)  variable has a distribution\n",
            "that assigns probability 0.1 to each of the numbers from 0 to 9.\n",
            "W e will sometimes talk about the expected value  of a random variable,\n",
            "which is the average of its values weighted by their probabilities. The coin\n",
            "flip variable has an expected value of 1/2 (= 0 * 1/2 + 1 * 1/2), and the\n",
            "range(10)  variable has an expected value of 4.5.\n",
            "Random variables can be conditioned  on events just as other events can.\n",
            "Going back to the two-child example from “Conditional Probability” , if X  isthe random variable representing the number of girls, X  equals 0 with\n",
            "probability 1/4, 1 with probability 1/2, and 2 with probability 1/4.\n",
            "W e can define a new random variable Y  that gives the number of girls\n",
            "conditional on at least one of the children being a girl. Then Y  equals 1 with\n",
            "probability 2/3 and 2 with probability 1/3. And a variable Z  that’ s the\n",
            "number of girls conditional on the older child being a girl equals 1 with\n",
            "probability 1/2 and 2 with probability 1/2.\n",
            "For the most part, we will be using random variables implicitly  in what we\n",
            "do without calling special attention to them. But if you look deeply you’ll\n",
            "see them.\n",
            "C o n t i n u o u s  D i s t r i b u t i o n s\n",
            "A  coin flip corresponds to a discr ete distribution —one that associates\n",
            "positive probability with discrete outcomes. Often we’ll want to model\n",
            "distributions across a continuum of outcomes. (For our purposes, these\n",
            "outcomes will always be real numbers, although that’ s not always the case\n",
            "in real life.) For example,  the uniform distribution  puts equal weight  on all\n",
            "the numbers between 0 and 1.\n",
            "Because there are infinitely many numbers between 0 and 1, this means that\n",
            "the weight it assigns to individual points must necessarily be zero. For  this\n",
            "reason, we represent a continuous distribution with a pr obability density\n",
            "function  (PDF) such that the probability of seeing a value in a certain\n",
            "interval equals the integral of the density function over the interval.\n",
            "N O T E\n",
            "If your integral calculus is rusty , a simpler way of understanding this is that if a\n",
            "distribution has density function f , then the probability of seeing a value between x  and x\n",
            "+ h  is approximately h * f(x)  if h  is small.\n",
            "The density function for the uniform distribution is just:def uniform_pdf(x: float) -> float: \n",
            "    return 1 if 0 <= x < 1 else 0\n",
            "The probability that a random variable following that distribution is\n",
            "between 0.2 and 0.3 is 1/10, as you’d expect. Python’ s random.random  is a\n",
            "(pseudo)random variable with a uniform density .\n",
            "W e  will often be more interested in the cumulative distribution function\n",
            "(CDF), which gives the probability that a random variable is less than or\n",
            "equal to a certain value. It’ s not hard to create the CDF for the uniform\n",
            "distribution ( Figure 6-1 ):\n",
            "def uniform_cdf(x: float) -> float: \n",
            "    \"\"\"Returns the probability that a uniform random variable is <= x\"\"\" \n",
            "    if x < 0:   return 0    # uniform random is never less than 0 \n",
            "    elif x < 1: return x    # e.g. P(X <= 0.4) = 0.4 \n",
            "    else:       return 1    # uniform random is always less than 1Figur e 6-1. The uniform CDF\n",
            "T h e  N o r m a l  D i s t r i b u t i o n\n",
            "The  normal distribution is the classic bell curve–shaped distribution and is\n",
            "completely determined by two parameters: its mean μ  (mu) and its standard\n",
            "deviation σ  (sigma). The mean indicates where the bell is centered, and the\n",
            "standard deviation how “wide” it is.\n",
            "It has the PDF:\n",
            "f(x|μ,σ)= exp(− )\n",
            "which we can implement as:1\n",
            "√2πσ(x−μ)2\n",
            "2σ2import math \n",
            "SQRT_TWO_PI = math.sqrt(2 * math.pi) \n",
            " \n",
            "def normal_pdf(x: float, mu: float = 0, sigma: float = 1) -> float: \n",
            "    return (math.exp(-(x-mu) ** 2 / 2 / sigma ** 2) / (SQRT_TWO_PI * sigma))\n",
            "In Figure 6-2 , we plot some of these PDFs to see what they look like:\n",
            "import matplotlib.pyplot as plt \n",
            "xs = [x / 10.0 for x in range(-50, 50)] \n",
            "plt.plot(xs,[normal_pdf(x,sigma=1) for x in xs],'-',label='mu=0,sigma=1') \n",
            "plt.plot(xs,[normal_pdf(x,sigma=2) for x in xs],'--',label='mu=0,sigma=2') \n",
            "plt.plot(xs,[normal_pdf(x,sigma=0.5) for x in xs],':',label='mu=0,sigma=0.5') \n",
            "plt.plot(xs,[normal_pdf(x,mu=-1)   for x in xs],'-.',label='mu=-1,sigma=1') \n",
            "plt.legend() \n",
            "plt.title(\"Various Normal pdfs\") \n",
            "plt.show()\n",
            "Figur e 6-2. V arious normal PDFsWhen μ  = 0 and σ  = 1, it’ s called  the standar d normal distribution . If Z  is a\n",
            "standard normal random variable, then it turns out that:\n",
            "X=σZ+μ\n",
            "is also normal but with mean μ  and standard deviation σ . Conversely , if X  is\n",
            "a normal random variable with mean μ  and standard deviation σ ,\n",
            "Z=(X−μ)/σ\n",
            "is a standard normal variable.\n",
            "The CDF for the normal distribution cannot be written in an “elementary”\n",
            "manner , but we can write it using Python’ s math.erf  error function :\n",
            "def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -> float: \n",
            "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
            "Again, in Figure 6-3 , we plot a few CDFs:\n",
            "xs = [x / 10.0 for x in range(-50, 50)] \n",
            "plt.plot(xs,[normal_cdf(x,sigma=1) for x in xs],'-',label='mu=0,sigma=1') \n",
            "plt.plot(xs,[normal_cdf(x,sigma=2) for x in xs],'--',label='mu=0,sigma=2') \n",
            "plt.plot(xs,[normal_cdf(x,sigma=0.5) for x in xs],':',label='mu=0,sigma=0.5') \n",
            "plt.plot(xs,[normal_cdf(x,mu=-1) for x in xs],'-.',label='mu=-1,sigma=1') \n",
            "plt.legend(loc=4) # bottom right \n",
            "plt.title(\"Various Normal cdfs\") \n",
            "plt.show()Figur e 6-3. V arious normal CDFs\n",
            "Sometimes we’ll need to invert normal_cdf  to find the value corresponding\n",
            "to a specified probability . There’ s no simple way to compute its inverse, but\n",
            "normal_cdf  is continuous and strictly increasing, so we can use a binary\n",
            "sear ch :\n",
            "def inverse_normal_cdf(p: float, \n",
            "                       mu: float = 0, \n",
            "                       sigma: float = 1, \n",
            "                       tolerance: float = 0.00001) -> float: \n",
            "    \"\"\"Find approximate inverse using binary search\"\"\" \n",
            " \n",
            "    # if not standard, compute standard and rescale \n",
            "    if mu != 0 or sigma != 1: \n",
            "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance) \n",
            " \n",
            "    low_z = -10.0                      # normal_cdf(-10) is (very close to) 0 \n",
            "    hi_z  =  10.0                      # normal_cdf(10)  is (very close to) 1while hi_z - low_z > tolerance: \n",
            "        mid_z = (low_z + hi_z) / 2     # Consider the midpoint \n",
            "        mid_p = normal_cdf(mid_z)      # and the CDF's value there \n",
            "        if mid_p < p: \n",
            "            low_z = mid_z              # Midpoint too low, search above it \n",
            "        else: \n",
            "            hi_z = mid_z               # Midpoint too high, search below it \n",
            " \n",
            "    return mid_z\n",
            "The function repeatedly bisects intervals until it narrows in on a Z  that’ s\n",
            "close enough to the desired probability .\n",
            "T h e  C e n t r a l  L i m i t  T h e o r e m\n",
            "One  reason the normal distribution is so useful is the central limit theor em ,\n",
            "which says (in essence) that a random variable defined as the average of a\n",
            "lar ge number of independent and identically distributed random variables is\n",
            "itself approximately normally distributed.\n",
            "In particular , if x1,...,xn  are random variables with mean μ  and standard\n",
            "deviation σ , and if n  is lar ge, then:\n",
            "(x1+...+xn)\n",
            "is approximately normally distributed with mean μ  and standard deviation \n",
            "σ/√n . Equivalently (but often more usefully),\n",
            "is approximately normally distributed with mean 0 and standard deviation\n",
            "1.\n",
            "An  easy way to illustrate this is by looking at binomial  random variables,\n",
            "which have two parameters n  and p . A Binomial( n , p ) random variable is1\n",
            "n\n",
            "(x1+...+xn)−μn\n",
            "σ√nsimply the sum of n  independent Bernoulli( p ) random variables, each of\n",
            "which equals 1 with probability p  and 0 with probability 1 – p :\n",
            "def bernoulli_trial(p: float) -> int: \n",
            "    \"\"\"Returns 1 with probability p and 0 with probability 1-p\"\"\" \n",
            "    return 1 if random.random() < p else 0 \n",
            " \n",
            "def binomial(n: int, p: float) -> int: \n",
            "    \"\"\"Returns the sum of n bernoulli(p) trials\"\"\" \n",
            "    return sum(bernoulli_trial(p) for _ in range(n))\n",
            "The mean of a Bernoulli( p ) variable is p , and its standard deviation is \n",
            "√p(1−p) . The central limit theorem says that as n  gets lar ge, a\n",
            "Binomial( n , p ) variable is approximately a normal random variable with\n",
            "mean μ=np  and standard deviation σ=√np(1−p) . If we plot both,\n",
            "you can easily see the resemblance:\n",
            "from collections import Counter \n",
            " \n",
            "def binomial_histogram(p: float, n: int, num_points: int) -> None: \n",
            "    \"\"\"Picks points from a Binomial(n, p) and plots their histogram\"\"\" \n",
            "    data = [binomial(n, p) for _ in range(num_points)] \n",
            " \n",
            "    # use a bar chart to show the actual binomial samples \n",
            "    histogram = Counter(data) \n",
            "    plt.bar([x - 0.4 for x in histogram.keys()], \n",
            "            [v / num_points for v in histogram.values()], \n",
            "            0.8, \n",
            "            color='0.75') \n",
            " \n",
            "    mu = p * n \n",
            "    sigma = math.sqrt(n * p * (1 - p)) \n",
            " \n",
            "    # use a line chart to show the normal approximation \n",
            "    xs = range(min(data), max(data) + 1) \n",
            "    ys = [normal_cdf(i + 0.5, mu, sigma) - normal_cdf(i - 0.5, mu, sigma) \n",
            "          for i in xs] \n",
            "    plt.plot(xs,ys) \n",
            "    plt.title(\"Binomial Distribution vs. Normal Approximation\") \n",
            "    plt.show()For example, when you call make_hist(0.75, 100, 10000) , you get the\n",
            "graph in Figure 6-4 .\n",
            "Figur e 6-4. The output fr om binomial_histogram\n",
            "The moral of this approximation is that if you want to know the probability\n",
            "that (say) a fair coin turns up more than 60 heads in 100 flips, you can\n",
            "estimate it as the probability that a Normal(50,5) is greater than 60, which is\n",
            "easier than computing the Binomial(100,0.5) CDF . (Although in most\n",
            "applications you’d probably be using statistical software that would gladly\n",
            "compute whatever probabilities you want.)\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "scipy .stats  contains PDF  and CDF functions for most of the\n",
            "popular probability distributions.Remember  how , at the end of Chapter 5 , I said that it would be a\n",
            "good idea to study a statistics textbook? It would also be a good\n",
            "idea to study a probability textbook. The best one I know that’ s\n",
            "available online is Intr oduction to Pr obability , by Charles M.\n",
            "Grinstead and J. Laurie Snell (American Mathematical Society).Chapter 7. Hypothesis and\n",
            "Inference\n",
            "It is the mark of a truly intelligent person to be moved by statistics.\n",
            "— Geor ge Bernard Shaw\n",
            "What will we do with all this statistics and probability theory? The science\n",
            "part of data science frequently involves forming and testing hypotheses\n",
            "about our data and the processes that generate it.\n",
            "S t a t i s t i c a l  H y p o t h e s i s  T e s t i n g\n",
            "Often, as  data scientists, we’ll want to test whether a certain hypothesis is\n",
            "likely to be true. For our purposes, hypotheses are assertions like “this coin\n",
            "is fair” or “data scientists prefer Python to R” or “people are more likely to\n",
            "navigate away from the page without ever reading the content if we pop up\n",
            "an irritating interstitial advertisement with a tiny , hard-to-find close button”\n",
            "that can be translated into statistics about data. Under various assumptions,\n",
            "those statistics can be thought of as observations of random variables from\n",
            "known distributions, which allows us to make statements about how likely\n",
            "those assumptions are to hold.\n",
            "In the classical setup, we  have a null hypothesis , H0 , that represents some\n",
            "default position, and some alternative hypothesis, H1 , that we’d like to\n",
            "compare it with. W e use statistics to decide whether we can reject H0  as\n",
            "false or not. This will probably make more sense with an example.\n",
            "E x a m p l e :  F l i p p i n g  a  C o i n\n",
            "Imagine  we have a coin and we want to test whether it’ s fair . W e’ll make\n",
            "the assumption that the coin has some probability p  of landing heads, and soour null hypothesis is that the coin is fair—that is, that p  = 0.5. W e’ll test\n",
            "this against the alternative hypothesis p  ≠ 0.5.\n",
            "In particular , our test will involve flipping the coin some number , n , times\n",
            "and counting the number of heads, X . Each coin flip is a Bernoulli trial,\n",
            "which means that X  is a Binomial( n , p ) random variable, which (as we saw\n",
            "in Chapter 6 ) we can approximate using the normal distribution:\n",
            "from typing import Tuple \n",
            "import math \n",
            " \n",
            "def normal_approximation_to_binomial(n: int, p: float) -> Tuple[float, float]: \n",
            "    \"\"\"Returns mu and sigma corresponding to a Binomial(n, p)\"\"\" \n",
            "    mu = p * n \n",
            "    sigma = math.sqrt(p * (1 - p) * n) \n",
            "    return mu, sigma\n",
            "Whenever a random variable follows a normal distribution, we can use\n",
            "normal_cdf  to figure out the probability that its realized value lies within\n",
            "or outside a particular interval:\n",
            "from scratch.probability import normal_cdf \n",
            " \n",
            "# The normal cdf _is_ the probability the variable is below a threshold \n",
            "normal_probability_below = normal_cdf \n",
            " \n",
            "# It's above the threshold if it's not below the threshold \n",
            "def normal_probability_above(lo: float, \n",
            "                             mu: float = 0, \n",
            "                             sigma: float = 1) -> float: \n",
            "    \"\"\"The probability that an N(mu, sigma) is greater than lo.\"\"\" \n",
            "    return 1 - normal_cdf(lo, mu, sigma) \n",
            " \n",
            "# It's between if it's less than hi, but not less than lo \n",
            "def normal_probability_between(lo: float, \n",
            "                               hi: float, \n",
            "                               mu: float = 0, \n",
            "                               sigma: float = 1) -> float: \n",
            "    \"\"\"The probability that an N(mu, sigma) is between lo and hi.\"\"\" \n",
            "    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma) \n",
            " \n",
            "# It's outside if it's not between \n",
            "def normal_probability_outside(lo: float,hi: float, \n",
            "                               mu: float = 0, \n",
            "                               sigma: float = 1) -> float: \n",
            "    \"\"\"The probability that an N(mu, sigma) is not between lo and hi.\"\"\" \n",
            "    return 1 - normal_probability_between(lo, hi, mu, sigma)\n",
            "W e can also do the reverse—find either the nontail region or the\n",
            "(symmetric) interval around the mean that accounts for a certain level of\n",
            "likelihood. For example, if we want to find an interval centered at the mean\n",
            "and containing 60% probability , then we find the cutof fs where the upper\n",
            "and lower tails each contain 20% of the probability (leaving 60%):\n",
            "from scratch.probability import inverse_normal_cdf \n",
            " \n",
            "def normal_upper_bound(probability: float, \n",
            "                       mu: float = 0, \n",
            "                       sigma: float = 1) -> float: \n",
            "    \"\"\"Returns the z for which P(Z <= z) = probability\"\"\" \n",
            "    return inverse_normal_cdf(probability, mu, sigma) \n",
            " \n",
            "def normal_lower_bound(probability: float, \n",
            "                       mu: float = 0, \n",
            "                       sigma: float = 1) -> float: \n",
            "    \"\"\"Returns the z for which P(Z >= z) = probability\"\"\" \n",
            "    return inverse_normal_cdf(1 - probability, mu, sigma) \n",
            " \n",
            "def normal_two_sided_bounds(probability: float, \n",
            "                            mu: float = 0, \n",
            "                            sigma: float = 1) -> Tuple[float, float]: \n",
            "    \"\"\" \n",
            "    Returns the symmetric (about the mean) bounds \n",
            "    that contain the specified probability \n",
            "    \"\"\" \n",
            "    tail_probability = (1 - probability) / 2 \n",
            " \n",
            "    # upper bound should have tail_probability above it \n",
            "    upper_bound = normal_lower_bound(tail_probability, mu, sigma) \n",
            " \n",
            "    # lower bound should have tail_probability below it \n",
            "    lower_bound = normal_upper_bound(tail_probability, mu, sigma) \n",
            " \n",
            "    return lower_bound, upper_boundIn particular , let’ s say that we choose to flip the coin n  = 1,000 times. If our\n",
            "hypothesis of fairness is true, X  should be distributed approximately\n",
            "normally with mean 500 and standard deviation 15.8:\n",
            "mu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5)\n",
            "W e  need to make a decision about significance —how willing  we are to\n",
            "make a type 1 err or  (“false positive”), in which we reject H0  even though\n",
            "it’ s true. For reasons lost to the annals of history , this willingness is often\n",
            "set at 5% or 1%. Let’ s choose 5%.\n",
            "Consider the test that rejects H0  if X  falls outside the bounds given by:\n",
            "# (469, 531) \n",
            "lower_bound, upper_bound = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
            "Assuming p  really equals 0.5 (i.e., H0  is true), there is just a 5% chance we\n",
            "observe an X  that lies outside this interval, which is the exact significance\n",
            "we wanted. Said dif ferently , if H0  is true, then, approximately 19 times out\n",
            "of 20, this test will give the correct result.\n",
            "W e  are also often interested in the power  of a test, which is the probability\n",
            "of not making a type 2 err or  (“false negative”), in which we fail to reject\n",
            "H0  even though it’ s false. In order to measure this, we have to specify what\n",
            "exactly H0  being false means . (Knowing merely that p  is not  0.5 doesn’ t\n",
            "give us a ton of information about the distribution of X .) In particular , let’ s\n",
            "check what happens if p  is really 0.55, so that the coin is slightly biased\n",
            "toward heads.\n",
            "In that case, we can calculate the power of the test with:\n",
            "# 95% bounds based on assumption p is 0.5 \n",
            "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0) \n",
            " \n",
            "# actual mu and sigma based on p = 0.55 \n",
            "mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55) \n",
            " \n",
            "# a type 2 error means we fail to reject the null hypothesis, \n",
            "# which will happen when X is still in our original intervaltype_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1) \n",
            "power = 1 - type_2_probability      # 0.887\n",
            "Imagine instead that our null hypothesis was that the coin is not biased\n",
            "toward heads, or that p≤0.5 . In that case we want a one-sided test  that\n",
            "rejects the null hypothesis when X  is much lar ger than 500 but not when X\n",
            "is smaller than 500. So, a 5% significance test involves using\n",
            "normal_probability_below  to find the cutof f below which 95% of the\n",
            "probability lies:\n",
            "hi = normal_upper_bound(0.95, mu_0, sigma_0) \n",
            "# is 526 (< 531, since we need more probability in the upper tail) \n",
            " \n",
            "type_2_probability = normal_probability_below(hi, mu_1, sigma_1) \n",
            "power = 1 - type_2_probability      # 0.936\n",
            "This is a more powerful test, since it no longer rejects H0  when X  is below\n",
            "469 (which is very unlikely to happen if H1  is true) and instead rejects H0\n",
            "when X  is between 526 and 531 (which is somewhat likely to happen if H1\n",
            "is true).\n",
            "p - V a l u e s\n",
            "An  alternative way of thinking about the preceding test involves p-values .\n",
            "Instead of choosing bounds based on some probability cutof f, we compute\n",
            "the probability—assuming H0  is true—that we would see a value at least as\n",
            "extreme as the one we actually observed.\n",
            "For our two-sided test of whether the coin is fair , we compute:\n",
            "def two_sided_p_value(x: float, mu: float = 0, sigma: float = 1) -> float: \n",
            "    \"\"\" \n",
            "    How likely are we to see a value at least as extreme as x (in either \n",
            "    direction) if our values are from an N(mu, sigma)? \n",
            "    \"\"\" \n",
            "    if x >= mu: \n",
            "        # x is greater than the mean, so the tail is everything greater than x \n",
            "        return 2 * normal_probability_above(x, mu, sigma) \n",
            "    else:# x is less than the mean, so the tail is everything less than x \n",
            "        return 2 * normal_probability_below(x, mu, sigma)\n",
            "If we were to see 530 heads, we would compute:\n",
            "two_sided_p_value(529.5, mu_0, sigma_0)   # 0.062\n",
            "N O T E\n",
            "Why  did we use a value of 529.5  rather than using 530 ? This is what’ s called a\n",
            "continuity corr ection . It reflects the fact that normal_probability_between(529.5,\n",
            "530.5, mu_0, sigma_0)  is a better estimate of the probability of seeing 530 heads\n",
            "than normal_probability_between(530, 531, mu_0, sigma_0)  is.\n",
            "Correspondingly , normal_probability_above(529.5, mu_0, sigma_0)  is a better\n",
            "estimate of the probability of seeing at least 530 heads. Y ou may have noticed that we\n",
            "also used this in the code that produced Figure 6-4 .\n",
            "One way to convince yourself that this is a sensible estimate is with a\n",
            "simulation:\n",
            "import random \n",
            " \n",
            "extreme_value_count = 0 \n",
            "for _ in range(1000): \n",
            "    num_heads = sum(1 if random.random() < 0.5 else 0    # Count # of heads \n",
            "                    for _ in range(1000))                # in 1000 flips, \n",
            "    if num_heads >= 530 or num_heads <= 470:             # and count how often \n",
            "        extreme_value_count += 1                         # the # is 'extreme' \n",
            " \n",
            "# p-value was 0.062 => ~62 extreme values out of 1000 \n",
            "assert 59 < extreme_value_count < 65, f\"{extreme_value_count}\"\n",
            "Since the p -value is greater than our 5% significance, we don’ t reject the\n",
            "null. If we instead saw 532 heads, the p -value would be:\n",
            "two_sided_p_value(531.5, mu_0, sigma_0)   # 0.0463which is smaller than the 5% significance, which means we would reject the\n",
            "null. It’ s the exact same test as before. It’ s just a dif ferent way of\n",
            "approaching the statistics.\n",
            "Similarly , we would have:\n",
            "upper_p_value = normal_probability_above \n",
            "lower_p_value = normal_probability_below\n",
            "For our one-sided test, if we saw 525 heads we would compute:\n",
            "upper_p_value(524.5, mu_0, sigma_0) # 0.061\n",
            "which means we wouldn’ t reject the null. If we saw 527 heads, the\n",
            "computation would be:\n",
            "upper_p_value(526.5, mu_0, sigma_0) # 0.047\n",
            "and we would reject the null.\n",
            "W A R N I N G\n",
            "Make sure your data is roughly normally distributed before using\n",
            "normal_probability_above  to compute p -values. The annals of bad data science are\n",
            "filled with examples of people opining that the chance of some observed event\n",
            "occurring at random is one in a million, when what they really mean is “the chance,\n",
            "assuming the data is distributed normally ,” which is fairly meaningless if the data isn’ t.\n",
            "There are various statistical tests for normality , but even plotting the data is a good start.\n",
            "C o n f i d e n c e  I n t e r v a l s\n",
            "W e’ve  been testing hypotheses about the value of the heads probability p ,\n",
            "which is a parameter  of the unknown “heads” distribution. When this is the\n",
            "case, a third approach is to construct a confidence interval  around the\n",
            "observed value of the parameter .For example, we can estimate the probability of the unfair coin by looking\n",
            "at the average value of the Bernoulli variables corresponding to each flip—\n",
            "1 if heads, 0 if tails. If we observe 525 heads out of 1,000 flips, then we\n",
            "estimate p  equals 0.525.\n",
            "How confident  can we be about this estimate? W ell, if we knew the exact\n",
            "value of p , the central limit theorem (recall “The Central Limit Theorem” )\n",
            "tells us that the average of those Bernoulli variables should be\n",
            "approximately normal, with mean p  and standard deviation:\n",
            "math.sqrt(p * (1 - p) / 1000)\n",
            "Here we don’ t know p , so instead we use our estimate:\n",
            "p_hat = 525 / 1000 \n",
            "mu = p_hat \n",
            "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)   # 0.0158\n",
            "This is not entirely justified, but people seem to do it anyway . Using the\n",
            "normal approximation, we conclude that we are “95% confident” that the\n",
            "following interval contains the true parameter p :\n",
            "normal_two_sided_bounds(0.95, mu, sigma)        # [0.4940, 0.5560]\n",
            "N O T E\n",
            "This is a statement about the interval , not about p . Y ou should understand it as the\n",
            "assertion that if you were to repeat the experiment many times, 95% of the time the\n",
            "“true” parameter (which is the same every time) would lie within the observed\n",
            "confidence interval (which might be dif ferent every time).\n",
            "In particular , we do not conclude that the coin is unfair , since 0.5 falls\n",
            "within our confidence interval.\n",
            "If instead we’d seen 540 heads, then we’d have:p_hat = 540 / 1000 \n",
            "mu = p_hat \n",
            "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000) # 0.0158 \n",
            "normal_two_sided_bounds(0.95, mu, sigma) # [0.5091, 0.5709]\n",
            "Here, “fair coin” doesn’ t lie in the confidence interval. (The “fair coin”\n",
            "hypothesis doesn’ t pass a test that you’d expect it to pass 95% of the time if\n",
            "it were true.)\n",
            "p - H a c k i n g\n",
            "A  procedure that erroneously rejects the null hypothesis only 5% of the\n",
            "time will—by definition—5% of the time erroneously reject the null\n",
            "hypothesis:\n",
            "from typing import List \n",
            " \n",
            "def run_experiment() -> List[bool]: \n",
            "    \"\"\"Flips a fair coin 1000 times, True = heads, False = tails\"\"\" \n",
            "    return [random.random() < 0.5 for _ in range(1000)] \n",
            " \n",
            "def reject_fairness(experiment: List[bool]) -> bool: \n",
            "    \"\"\"Using the 5% significance levels\"\"\" \n",
            "    num_heads = len([flip for flip in experiment if flip]) \n",
            "    return num_heads < 469 or num_heads > 531 \n",
            " \n",
            "random.seed(0) \n",
            "experiments = [run_experiment() for _ in range(1000)] \n",
            "num_rejections = len([experiment \n",
            "                      for experiment in experiments \n",
            "                      if reject_fairness(experiment)]) \n",
            " \n",
            "assert num_rejections == 46\n",
            "What this means is that if you’re setting out to find “significant” results,\n",
            "you usually can. T est enough hypotheses against your dataset, and one of\n",
            "them will almost certainly appear significant. Remove the right outliers, and\n",
            "you can probably get your p -value below 0.05. (W e did something vaguely\n",
            "similar in “Correlation” ; did you notice?)This is sometimes called p-hacking  and is in some ways a consequence of\n",
            "the “inference from p -values framework.” A good article criticizing this\n",
            "approach is “The Earth Is Round” , by Jacob Cohen.\n",
            "If you want to do good science , you should determine your hypotheses\n",
            "before looking at the data, you should clean your data without the\n",
            "hypotheses in mind, and you should keep in mind that p -values are not\n",
            "substitutes for common sense. (An alternative approach is discussed in\n",
            "“Bayesian Inference” .)\n",
            "E x a m p l e :  R u n n i n g  a n  A / B  T e s t\n",
            "One  of your primary responsibilities at DataSciencester is experience\n",
            "optimization, which is a euphemism for trying to get people to click on\n",
            "advertisements. One of your advertisers has developed a new ener gy drink\n",
            "tar geted at data scientists, and the VP of Advertisements wants your help\n",
            "choosing between advertisement A (“tastes great!”) and advertisement B\n",
            "(“less bias!”).\n",
            "Being a scientist , you decide to run an experiment  by randomly showing\n",
            "site visitors one of the two advertisements and tracking how many people\n",
            "click on each one.\n",
            "If 990 out of 1,000 A-viewers click their ad, while only 10 out of 1,000 B-\n",
            "viewers click their ad, you can be pretty confident that A is the better ad.\n",
            "But what if the dif ferences are not so stark? Here’ s where you’d use\n",
            "statistical inference.\n",
            "Let’ s say that NA  people see ad A, and that nA  of them click it. W e can\n",
            "think of each ad view as a Bernoulli trial where pA  is the probability that\n",
            "someone clicks ad A. Then (if NA  is lar ge, which it is here) we know that \n",
            "nA/NA  is approximately a normal random variable with mean pA  and\n",
            "standard deviation σA=√pA(1−pA)/NA .\n",
            "Similarly , nB/NB  is approximately a normal random variable with mean \n",
            "pB  and standard deviation σB=√pB(1−pB)/NB . W e can express thisin code as:\n",
            "def estimated_parameters(N: int, n: int) -> Tuple[float, float]: \n",
            "    p = n / N \n",
            "    sigma = math.sqrt(p * (1 - p) / N) \n",
            "    return p, sigma\n",
            "If we assume those two normals are independent (which seems reasonable,\n",
            "since the individual Bernoulli trials ought to be), then their dif ference\n",
            "should also be normal with mean pB−pA  and standard deviation \n",
            "√σ2\n",
            "A+σ2\n",
            "B.\n",
            "N O T E\n",
            "This is sort of cheating. The math only works out exactly like this if you know  the\n",
            "standard deviations. Here we’re estimating them from the data, which means that we\n",
            "really should be using a t -distribution. But for lar ge enough datasets, it’ s close enough\n",
            "that it doesn’ t make much of a dif ference.\n",
            "This means we can test the null hypothesis  that pA  and pB  are the same (that\n",
            "is, that pA−pB  is 0) by using the statistic:\n",
            "def a_b_test_statistic(N_A: int, n_A: int, N_B: int, n_B: int) -> float: \n",
            "    p_A, sigma_A = estimated_parameters(N_A, n_A) \n",
            "    p_B, sigma_B = estimated_parameters(N_B, n_B) \n",
            "    return (p_B - p_A) / math.sqrt(sigma_A ** 2 + sigma_B ** 2)\n",
            "which should approximately be a standard normal.\n",
            "For example, if “tastes great” gets 200 clicks out of 1,000 views and “less\n",
            "bias” gets 180 clicks out of 1,000 views, the statistic equals:\n",
            "z = a_b_test_statistic(1000, 200, 1000, 180)    # -1.14\n",
            "The probability of seeing such a lar ge dif ference if the means were actually\n",
            "equal would be:two_sided_p_value(z)                            # 0.254\n",
            "which is lar ge enough that we can’ t conclude there’ s much of a dif ference.\n",
            "On the other hand, if “less bias” only got 150 clicks, we’d have:\n",
            "z = a_b_test_statistic(1000, 200, 1000, 150)    # -2.94 \n",
            "two_sided_p_value(z)                            # 0.003\n",
            "which means there’ s only a 0.003 probability we’d see such a lar ge\n",
            "dif ference if the ads were equally ef fective.\n",
            "B a y e s i a n  I n f e r e n c e\n",
            "The  procedures we’ve looked at have involved making probability\n",
            "statements about our tests : e.g., “There’ s only a 3% chance you’d observe\n",
            "such an extreme statistic if our null hypothesis were true.”\n",
            "An  alternative approach to inference involves treating the unknown\n",
            "parameters themselves as random variables. The analyst (that’ s you) starts\n",
            "with a prior distribution  for the parameters and then uses the observed data\n",
            "and Bayes’ s theorem to get an updated posterior distribution  for the\n",
            "parameters. Rather than making probability judgments about the tests, you\n",
            "make probability judgments about the parameters.\n",
            "For  example, when the unknown parameter is a probability (as in our coin-\n",
            "flipping example), we often use a prior from the Beta distribution , which\n",
            "puts all its probability between 0 and 1:\n",
            "def B(alpha: float, beta: float) -> float: \n",
            "    \"\"\"A normalizing constant so that the total probability is 1\"\"\" \n",
            "    return math.gamma(alpha) * math.gamma(beta) / math.gamma(alpha + beta) \n",
            " \n",
            "def beta_pdf(x: float, alpha: float, beta: float) -> float: \n",
            "    if x <= 0 or x >= 1:          # no weight outside of [0, 1] \n",
            "        return 0 \n",
            "    return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta)\n",
            "Generally speaking, this distribution centers its weight at:alpha / (alpha + beta)\n",
            "and the lar ger alpha  and beta  are, the “tighter” the distribution is.\n",
            "For example, if alpha  and beta  are both 1, it’ s just the uniform distribution\n",
            "(centered at 0.5, very dispersed). If alpha  is much lar ger than beta , most of\n",
            "the weight is near 1. And if alpha  is much smaller than beta , most of the\n",
            "weight is near 0. Figure 7-1  shows several dif ferent Beta distributions.\n",
            "Figur e 7-1. Example Beta distributions\n",
            "Say we assume a prior distribution on p . Maybe we don’ t want to take a\n",
            "stand on whether the coin is fair , and we choose alpha  and beta  to both\n",
            "equal 1. Or maybe we have a strong belief that the coin lands heads 55% of\n",
            "the time, and we choose alpha  equals 55, beta  equals 45.Then we flip our coin a bunch of times and see h  heads and t  tails. Bayes’ s\n",
            "theorem (and some mathematics too tedious for us to go through here) tells\n",
            "us that the posterior distribution for p  is again a Beta distribution, but with\n",
            "parameters alpha + h  and beta + t .\n",
            "N O T E\n",
            "It is no coincidence that the posterior distribution was again a Beta distribution. The\n",
            "number of heads is given by a  Binomial distribution, and the Beta is the conjugate prior\n",
            "to the Binomial distribution. This means that whenever you update a Beta prior using\n",
            "observations from the corresponding binomial, you will get back a Beta posterior .\n",
            "Let’ s say you flip the coin 10 times and see only 3 heads. If you started with\n",
            "the uniform prior (in some sense refusing to take a stand about the coin’ s\n",
            "fairness), your posterior distribution would be a Beta(4, 8), centered around\n",
            "0.33. Since you considered all probabilities equally likely , your best guess\n",
            "is close to the observed probability .\n",
            "If you started with a Beta(20, 20) (expressing a belief that the coin was\n",
            "roughly fair), your posterior distribution would be a Beta(23, 27), centered\n",
            "around 0.46, indicating a revised belief that maybe the coin is slightly\n",
            "biased toward tails.\n",
            "And if you started with a Beta(30, 10) (expressing a belief that the coin was\n",
            "biased to flip 75% heads), your posterior distribution would be a Beta(33,\n",
            "17), centered around 0.66. In that case you’d still believe in a heads bias,\n",
            "but less strongly than you did initially . These three dif ferent posteriors are\n",
            "plotted in Figure 7-2 .Figur e 7-2. Posteriors arising fr om differ ent priors\n",
            "If you flipped the coin more and more times, the prior would matter less\n",
            "and less until eventually you’d have (nearly) the same posterior distribution\n",
            "no matter which prior you started with.\n",
            "For example, no matter how biased you initially thought the coin was, it\n",
            "would be hard to maintain that belief after seeing 1,000 heads out of 2,000\n",
            "flips (unless you are a lunatic who picks something like a Beta(1000000,1)\n",
            "prior).\n",
            "What’ s interesting is that this allows us to make probability statements\n",
            "about hypotheses: “Based on the prior and the observed data, there is only a\n",
            "5% likelihood the coin’ s heads probability is between 49% and 51%.” This\n",
            "is philosophically very dif ferent from a statement like “If the coin were fair ,\n",
            "we would expect to observe data so extreme only 5% of the time.”Using Bayesian inference to test hypotheses is considered somewhat\n",
            "controversial—in part because the mathematics can get somewhat\n",
            "complicated, and in part because of the subjective nature of choosing a\n",
            "prior . W e won’ t use it any further in this book, but it’ s good to know about.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "W e’ve barely  scratched the surface of what you should know about\n",
            "statistical inference. The books recommended at the end of\n",
            "Chapter 5  go into a lot more detail.\n",
            "Coursera  of fers a Data Analysis and Statistical Inference  course\n",
            "that covers many of these topics.Chapter 8. Gradient Descent\n",
            "Those who boast of their descent, brag on what they owe to others.\n",
            "— Seneca\n",
            "Frequently when doing data science, we’ll be trying to the find the best\n",
            "model for a certain situation. And usually “best” will mean something like\n",
            "“minimizes the error of its predictions” or “maximizes the likelihood of the\n",
            "data.” In other words, it will represent the solution to some sort of\n",
            "optimization problem.\n",
            "This means we’ll need to solve a number of optimization problems. And in\n",
            "particular , we’ll need to solve them from scratch. Our approach will be a\n",
            "technique called gradient descent , which lends itself pretty well to a from-\n",
            "scratch treatment. Y ou might not find it super -exciting in and of itself, but it\n",
            "will enable us to do exciting things throughout the book, so bear with me.\n",
            "T h e  I d e a  B e h i n d  G r a d i e n t  D e s c e n t\n",
            "Suppose  we have some function f  that takes as input a vector of real\n",
            "numbers and outputs a single real number . One simple such function is:\n",
            "from scratch.linear_algebra import Vector, dot \n",
            " \n",
            "def sum_of_squares(v: Vector) -> float: \n",
            "    \"\"\"Computes the sum of squared elements in v\"\"\" \n",
            "    return dot(v, v)\n",
            "W e’ll frequently need to maximize or minimize such functions. That is, we\n",
            "need to find the input v  that produces the lar gest (or smallest) possible\n",
            "value.\n",
            "For functions like ours, the gradient  (if you remember your calculus, this is\n",
            "the vector of partial derivatives) gives the input direction in which thefunction most quickly increases. (If you don’ t remember your calculus, take\n",
            "my word for it or look it up on the internet.)\n",
            "Accordingly , one approach to maximizing a function is to pick a random\n",
            "starting point, compute the gradient, take a small step in the direction of the\n",
            "gradient (i.e., the direction that causes the function to increase the most),\n",
            "and repeat with the new starting point. Similarly , you can try to minimize a\n",
            "function by taking small steps in the opposite  direction, as shown in\n",
            "Figure 8-1 .\n",
            "Figur e 8-1. Finding a minimum using gradient descent\n",
            "N O T E\n",
            "If a function has a unique global minimum, this procedure is likely to find it. If a\n",
            "function has multiple (local) minima, this procedure might “find” the wrong one of\n",
            "them, in which case you might rerun the procedure from dif ferent starting points. If a\n",
            "function has no minimum, then it’ s possible the procedure might go on forever .E s t i m a t i n g  t h e  G r a d i e n t\n",
            "If f  is a function  of one variable, its derivative at a point x  measures how\n",
            "f(x)  changes when we make a very small change to x . The derivative is\n",
            "defined as the limit of the dif ference quotients:\n",
            "from typing import Callable \n",
            " \n",
            "def difference_quotient(f: Callable[[float], float], \n",
            "                        x: float, \n",
            "                        h: float) -> float: \n",
            "    return (f(x + h) - f(x)) / h\n",
            "as h  approaches zero.\n",
            "(Many a would-be calculus student has been stymied by the mathematical\n",
            "definition of limit, which is beautiful but can seem somewhat forbidding.\n",
            "Here we’ll cheat and simply say that “limit” means what you think it\n",
            "means.)\n",
            "The derivative is the slope of the tangent line at (x,f(x)) , while the\n",
            "dif ference quotient is the slope of the not-quite-tangent line that runs\n",
            "through (x+h,f(x+h)) . As h  gets smaller and smaller , the not-quite-\n",
            "tangent line gets closer and closer to the tangent line ( Figure 8-2 ).Figur e 8-2. Appr oximating a derivative with a differ ence quotient\n",
            "For many functions it’ s easy to exactly calculate derivatives. For example,\n",
            "the square  function:\n",
            "def square(x: float) -> float: \n",
            "    return x * x\n",
            "has the derivative:\n",
            "def derivative(x: float) -> float: \n",
            "    return 2 * x\n",
            "which is easy for us to check by explicitly computing the dif ference\n",
            "quotient and taking the limit. (Doing so requires nothing more than high\n",
            "school algebra.)\n",
            "What if you couldn’ t (or didn’ t want to) find the gradient? Although we\n",
            "can’ t take limits in Python, we can estimate derivatives by evaluating thedif ference quotient for a very small e . Figure 8-3  shows the results of one\n",
            "such estimation:\n",
            "xs = range(-10, 11) \n",
            "actuals = [derivative(x) for x in xs] \n",
            "estimates = [difference_quotient(square, x, h=0.001) for x in xs] \n",
            " \n",
            "# plot to show they're basically the same \n",
            "import matplotlib.pyplot as plt \n",
            "plt.title(\"Actual Derivatives vs. Estimates\") \n",
            "plt.plot(xs, actuals, 'rx', label='Actual')       # red  x \n",
            "plt.plot(xs, estimates, 'b+', label='Estimate')   # blue + \n",
            "plt.legend(loc=9) \n",
            "plt.show()\n",
            "Figur e 8-3. Goodness of differ ence quotient appr oximation\n",
            "When f  is  a function of many variables, it has multiple partial derivatives ,\n",
            "each indicating how f  changes when we make small changes in just one ofthe input variables.\n",
            "W e calculate its i th partial derivative by treating it as a function of just its\n",
            "i th variable, holding the other variables fixed:\n",
            "def partial_difference_quotient(f: Callable[[Vector], float], \n",
            "                                v: Vector, \n",
            "                                i: int, \n",
            "                                h: float) -> float: \n",
            "    \"\"\"Returns the i-th partial difference quotient of f at v\"\"\" \n",
            "    w = [v_j + (h if j == i else 0)    # add h to just the ith element of v \n",
            "         for j, v_j in enumerate(v)] \n",
            " \n",
            "    return (f(w) - f(v)) / h\n",
            "after which we can estimate the gradient the same way:\n",
            "def estimate_gradient(f: Callable[[Vector], float], \n",
            "                      v: Vector, \n",
            "                      h: float = 0.0001): \n",
            "    return [partial_difference_quotient(f, v, i, h) \n",
            "            for i in range(len(v))]\n",
            "N O T E\n",
            "A major drawback to this “estimate using dif ference quotients” approach is that it’ s\n",
            "computationally expensive. If v  has length n , estimate_gradient  has to evaluate f  on\n",
            "2 n  dif ferent inputs. If you’re repeatedly estimating gradients, you’re doing a whole lot\n",
            "of extra work. In everything we do, we’ll use math to calculate our gradient functions\n",
            "explicitly .\n",
            "U s i n g  t h e  G r a d i e n t\n",
            "It’ s  easy to see that the sum_of_squares  function is smallest when its input\n",
            "v  is a vector of zeros. But imagine we didn’ t know that. Let’ s use gradients\n",
            "to find the minimum among all three-dimensional vectors. W e’ll just pick a\n",
            "random starting point and then take tiny steps in the opposite direction of\n",
            "the gradient until we reach a point where the gradient is very small:import random \n",
            "from scratch.linear_algebra import distance, add, scalar_multiply \n",
            " \n",
            "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector: \n",
            "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\" \n",
            "    assert len(v) == len(gradient) \n",
            "    step = scalar_multiply(step_size, gradient) \n",
            "    return add(v, step) \n",
            " \n",
            "def sum_of_squares_gradient(v: Vector) -> Vector: \n",
            "    return [2 * v_i for v_i in v]\n",
            "# pick a random starting point \n",
            "v = [random.uniform(-10, 10) for i in range(3)] \n",
            " \n",
            "for epoch in range(1000): \n",
            "    grad = sum_of_squares_gradient(v)    # compute the gradient at v \n",
            "    v = gradient_step(v, grad, -0.01)    # take a negative gradient step \n",
            "    print(epoch, v) \n",
            " \n",
            "assert distance(v, [0, 0, 0]) < 0.001    # v should be close to 0\n",
            "If you run this, you’ll find that it always ends up with a v  that’ s very close\n",
            "to [0,0,0] . The more epochs you run it for , the closer it will get.\n",
            "C h o o s i n g  t h e  R i g h t  S t e p  S i z e\n",
            "Although  the rationale for moving against the gradient is clear , how far to\n",
            "move is not. Indeed, choosing the right step size is more of an art than a\n",
            "science. Popular options include:\n",
            "Using a fixed step size\n",
            "Gradually shrinking the step size over time\n",
            "At each step, choosing the step size that minimizes the value of the\n",
            "objective function\n",
            "The last approach sounds great but is, in practice, a costly computation. T o\n",
            "keep things simple, we’ll mostly just use a fixed step size. The step size that\n",
            "“works” depends on the problem—too small, and your gradient descent willtake forever; too big, and you’ll take giant steps that might make the\n",
            "function you care about get lar ger or even be undefined. So we’ll need to\n",
            "experiment.\n",
            "U s i n g  G r a d i e n t  D e s c e n t  t o  F i t  M o d e l s\n",
            "In  this book, we’ll be using gradient descent to fit parameterized models to\n",
            "data. In the usual case, we’ll have some dataset and some (hypothesized)\n",
            "model for the data that depends (in a dif ferentiable way) on one or more\n",
            "parameters. W e’ll also have a loss  function that measures how well the\n",
            "model fits our data. (Smaller is better .)\n",
            "If we think of our data as being fixed, then our loss function tells us how\n",
            "good or bad any particular model parameters are. This means we can use\n",
            "gradient descent to find the model parameters that make the loss as small as\n",
            "possible. Let’ s look at a simple example:\n",
            "# x ranges from -50 to 49, y is always 20 * x + 5 \n",
            "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]\n",
            "In this case we know  the parameters of the linear relationship between x  and\n",
            "y , but imagine we’d like to learn them from the data. W e’ll use gradient\n",
            "descent to find the slope and intercept that minimize the average squared\n",
            "error .\n",
            "W e’ll start of f with a function that determines the gradient based on the\n",
            "error from a single data point:\n",
            "def linear_gradient(x: float, y: float, theta: Vector) -> Vector: \n",
            "    slope, intercept = theta \n",
            "    predicted = slope * x + intercept    # The prediction of the model. \n",
            "    error = (predicted - y)              # error is (predicted - actual). \n",
            "    squared_error = error ** 2           # We'll minimize squared error \n",
            "    grad = [2 * error * x, 2 * error]    # using its gradient. \n",
            "    return gradLet’ s think about what that gradient means. Imagine for some x  our\n",
            "prediction is too lar ge. In that case the error  is positive. The second\n",
            "gradient term, 2 * error , is positive, which reflects the fact that small\n",
            "increases in the intercept will make the (already too lar ge) prediction even\n",
            "lar ger , which will cause the squared error (for this x ) to get even bigger .\n",
            "The first gradient term, 2 * error * x , has the same sign as x . Sure\n",
            "enough, if x  is positive, small increases in the slope will again make the\n",
            "prediction (and hence the error) lar ger . If x  is negative, though, small\n",
            "increases in the slope will make the prediction (and hence the error) smaller .\n",
            "Now , that computation  was for a single data point. For the whole dataset\n",
            "we’ll look at the mean squar ed err or . And the gradient of the mean squared\n",
            "error is just the mean of the individual gradients.\n",
            "So, here’ s what we’re going to do:\n",
            "1 . Start with a random value for theta .\n",
            "2 . Compute the mean of the gradients.\n",
            "3 . Adjust theta  in that direction.\n",
            "4 . Repeat.\n",
            "After a lot of epochs  (what we call each pass through the dataset), we\n",
            "should learn something like the correct parameters:\n",
            "from scratch.linear_algebra import vector_mean \n",
            " \n",
            "# Start with random values for slope and intercept \n",
            "theta = [random.uniform(-1, 1), random.uniform(-1, 1)] \n",
            " \n",
            "learning_rate = 0.001 \n",
            " \n",
            "for epoch in range(5000): \n",
            "    # Compute the mean of the gradients \n",
            "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs]) \n",
            "    # Take a step in that direction \n",
            "    theta = gradient_step(theta, grad, -learning_rate) \n",
            "    print(epoch, theta)slope, intercept = theta \n",
            "assert 19.9 < slope < 20.1,   \"slope should be about 20\" \n",
            "assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
            "M i n i b a t c h  a n d  S t o c h a s t i c  G r a d i e n t  D e s c e n t\n",
            "One  drawback of the preceding approach is that we had to evaluate the\n",
            "gradients on the entire dataset before we could take a gradient step and\n",
            "update our parameters. In this case it was fine, because our dataset was only\n",
            "100 pairs and the gradient computation was cheap.\n",
            "Y our models, however , will frequently have lar ge datasets and expensive\n",
            "gradient computations. In that case you’ll want to take gradient steps more\n",
            "often.\n",
            "W e  can do this using a technique called minibatch gradient descent , in\n",
            "which we compute the gradient (and take a gradient step) based on a\n",
            "“minibatch” sampled from the lar ger dataset:\n",
            "from typing import TypeVar, List, Iterator \n",
            " \n",
            "T = TypeVar('T')  # this allows us to type \"generic\" functions \n",
            " \n",
            "def minibatches(dataset: List[T], \n",
            "                batch_size: int, \n",
            "                shuffle: bool = True) -> Iterator[List[T]]: \n",
            "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\" \n",
            "    # start indexes 0, batch_size, 2 * batch_size, ... \n",
            "    batch_starts = [start for start in range(0, len(dataset), batch_size)] \n",
            " \n",
            "    if shuffle: random.shuffle(batch_starts)  # shuffle the batches \n",
            " \n",
            "    for start in batch_starts: \n",
            "        end = start + batch_size \n",
            "        yield dataset[start:end]N O T E\n",
            "The TypeVar(T)  allows us to create a “generic” function. It says that our dataset  can\n",
            "be a list of any single type— str s, int s, list s, whatever—but whatever that type is, the\n",
            "outputs will be batches of it.\n",
            "Now we can solve our problem again using minibatches:\n",
            "theta = [random.uniform(-1, 1), random.uniform(-1, 1)] \n",
            " \n",
            "for epoch in range(1000): \n",
            "    for batch in minibatches(inputs, batch_size=20): \n",
            "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch]) \n",
            "        theta = gradient_step(theta, grad, -learning_rate) \n",
            "    print(epoch, theta) \n",
            " \n",
            "slope, intercept = theta \n",
            "assert 19.9 < slope < 20.1,   \"slope should be about 20\" \n",
            "assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
            "Another  variation is stochastic gradient descent , in which you take gradient\n",
            "steps based on one training example at a time:\n",
            "theta = [random.uniform(-1, 1), random.uniform(-1, 1)] \n",
            " \n",
            "for epoch in range(100): \n",
            "    for x, y in inputs: \n",
            "        grad = linear_gradient(x, y, theta) \n",
            "        theta = gradient_step(theta, grad, -learning_rate) \n",
            "    print(epoch, theta) \n",
            " \n",
            "slope, intercept = theta \n",
            "assert 19.9 < slope < 20.1,   \"slope should be about 20\" \n",
            "assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
            "On this problem, stochastic gradient descent finds the optimal parameters in\n",
            "a much smaller number of epochs. But there are always tradeof fs. Basing\n",
            "gradient steps on small minibatches (or on single data points) allows you to\n",
            "take more of them, but the gradient for a single point might lie in a very\n",
            "dif ferent direction from the gradient for the dataset as a whole.In addition, if we weren’ t doing our linear algebra from scratch, there would\n",
            "be performance gains from “vectorizing” our computations across batches\n",
            "rather than computing the gradient one point at a time.\n",
            "Throughout the book, we’ll play around to find optimal batch sizes and step\n",
            "sizes.\n",
            "N O T E\n",
            "The terminology for the various flavors of gradient descent is not uniform. The\n",
            "“compute the gradient for the whole dataset” approach is often  called batch gradient\n",
            "descent , and some people say stochastic gradient descent  when referring to the\n",
            "minibatch version (of which the one-point-at-a-time version is a special case).\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "Keep reading! W e’ll be using gradient descent to solve problems\n",
            "throughout the rest of the book.\n",
            "At  this point, you’re undoubtedly sick of me recommending that\n",
            "you read textbooks. If it’ s any consolation, Active Calculus 1.0 , by\n",
            "Matthew Boelkins, David Austin, and Steven Schlicker (Grand\n",
            "V alley State University Libraries), seems nicer than the calculus\n",
            "textbooks I learned from.\n",
            "Sebastian Ruder has an epic blog post  comparing gradient descent\n",
            "and its many variants.Chapter 9. Getting Data\n",
            "T o write it, it took thr ee months; to conceive it, thr ee minutes; to collect\n",
            "the data in it, all my life.\n",
            "— F . Scott Fitzgerald\n",
            "In order to be a data scientist you need data. In fact, as a data scientist you\n",
            "will spend an embarrassingly lar ge fraction of your time acquiring,\n",
            "cleaning, and transforming data. In a pinch, you can always type the data in\n",
            "yourself (or if you have minions, make them do it), but usually this is not a\n",
            "good use of your time. In this chapter , we’ll look at dif ferent ways of\n",
            "getting data into Python and into the right formats.\n",
            "s t d i n  a n d  s t d o u t\n",
            "If  you run your Python scripts at the command line, you can pipe  data\n",
            "through them using sys.stdin  and sys.stdout . For example, here is a\n",
            "script that reads in lines of text and spits back out the ones that match a\n",
            "regular expression:\n",
            "# egrep.py \n",
            "import sys, re \n",
            " \n",
            "# sys.argv is the list of command-line arguments \n",
            "# sys.argv[0] is the name of the program itself \n",
            "# sys.argv[1] will be the regex specified at the command line \n",
            "regex = sys.argv[1] \n",
            " \n",
            "# for every line passed into the script \n",
            "for line in sys.stdin: \n",
            "    # if it matches the regex, write it to stdout \n",
            "    if re.search(regex, line): \n",
            "        sys.stdout.write(line)And here’ s one that counts the lines it receives and then writes out the\n",
            "count:\n",
            "# line_count.py \n",
            "import sys \n",
            " \n",
            "count = 0 \n",
            "for line in sys.stdin: \n",
            "    count += 1 \n",
            " \n",
            "# print goes to sys.stdout \n",
            "print(count)\n",
            "Y ou could then use these to count how many lines of a file contain numbers.\n",
            "In W indows, you’d use:\n",
            "type SomeFile.txt | python egrep.py \"[0-9]\" | python line_count.py\n",
            "whereas in a Unix system you’d use:\n",
            "cat SomeFile.txt | python egrep.py \"[0-9]\" | python line_count.py\n",
            "The | is the pipe character , which means “use the output of the left\n",
            "command as the input of the right command.” Y ou can build pretty\n",
            "elaborate data-processing pipelines this way .\n",
            "N O T E\n",
            "If you are using W indows, you can probably leave out the python  part of this command:\n",
            "type SomeFile.txt | egrep.py \"[0-9]\" | line_count.py\n",
            "If you are on a Unix system, doing so requires a couple more steps . First add a\n",
            "“shebang” as the first line of your script #!/usr/bin/env python . Then, at the\n",
            "command line, use chmod x egrep.py++ to make the file executable.Similarly , here’ s a script that counts the words in its input and writes out the\n",
            "most common ones:\n",
            "# most_common_words.py \n",
            "import sys \n",
            "from collections import Counter \n",
            " \n",
            "# pass in number of words as first argument \n",
            "try: \n",
            "    num_words = int(sys.argv[1]) \n",
            "except: \n",
            "    print(\"usage: most_common_words.py num_words\") \n",
            "    sys.exit(1)   # nonzero exit code indicates error \n",
            " \n",
            "counter = Counter(word.lower()                      # lowercase words \n",
            "                  for line in sys.stdin \n",
            "                  for word in line.strip().split()  # split on spaces \n",
            "                  if word)                          # skip empty 'words' \n",
            " \n",
            "for word, count in counter.most_common(num_words): \n",
            "    sys.stdout.write(str(count)) \n",
            "    sys.stdout.write(\"\\t\") \n",
            "    sys.stdout.write(word) \n",
            "    sys.stdout.write(\"\\n\")\n",
            "after which you could do something like:\n",
            "$ cat the_bible.txt | python most_common_words.py 10 \n",
            "36397 the \n",
            "30031 and \n",
            "20163 of \n",
            "7154 to \n",
            "6484 in \n",
            "5856 that \n",
            "5421 he \n",
            "5226 his \n",
            "5060 unto \n",
            "4297 shall\n",
            "(If you are using W indows, then use type  instead of cat .)N O T E\n",
            "If you are a seasoned Unix programmer , you are probably familiar with a wide variety\n",
            "of command-line tools (for example, egrep ) that are built into your operating system\n",
            "and are preferable to building your own from scratch. Still, it’ s good to know you can if\n",
            "you need to.\n",
            "R e a d i n g  F i l e s\n",
            "Y ou  can also explicitly read from and write to files directly in your code.\n",
            "Python makes working with files pretty simple.\n",
            "The Basics of T ext Files\n",
            "The  first step to working with a text file is to obtain a file object  using open :\n",
            "# 'r' means read-only, it's assumed if you leave it out \n",
            "file_for_reading = open('reading_file.txt', 'r') \n",
            "file_for_reading2 = open('reading_file.txt') \n",
            " \n",
            "# 'w' is write -- will destroy the file if it already exists! \n",
            "file_for_writing = open('writing_file.txt', 'w') \n",
            " \n",
            "# 'a' is append -- for adding to the end of the file \n",
            "file_for_appending = open('appending_file.txt', 'a') \n",
            " \n",
            "# don't forget to close your files when you're done \n",
            "file_for_writing.close()\n",
            "Because it is easy to for get to close your files, you should always use them\n",
            "in a with  block, at the end of which they will be closed automatically:\n",
            "with open(filename) as f: \n",
            "    data = function_that_gets_data_from(f) \n",
            " \n",
            "# at this point f has already been closed, so don't try to use it \n",
            "process(data)If you need to read a whole text file, you can just iterate over the lines of\n",
            "the file using for :\n",
            "starts_with_hash = 0 \n",
            " \n",
            "with open('input.txt') as f: \n",
            "    for line in f:                  # look at each line in the file \n",
            "        if re.match(\"^#\",line):     # use a regex to see if it starts with '#' \n",
            "            starts_with_hash += 1   # if it does, add 1 to the count\n",
            "Every line you get this way ends in a newline character , so you’ll often\n",
            "want to strip  it before doing anything with it.\n",
            "For example, imagine you have a file full of email addresses, one per line,\n",
            "and you need to generate a histogram of the domains. The rules for\n",
            "correctly extracting domains are somewhat subtle—see, e.g., the Public\n",
            "Suf fix List —but a good first approximation is to just take the parts of the\n",
            "email addresses that come after the @  (this gives the wrong answer for\n",
            "email addresses like joel@mail.datasciencester .com , but for the purposes of\n",
            "this example we’re willing to live with that):\n",
            "def get_domain(email_address: str) -> str: \n",
            "    \"\"\"Split on '@' and return the last piece\"\"\" \n",
            "    return email_address.lower().split(\"@\")[-1] \n",
            " \n",
            "# a couple of tests \n",
            "assert get_domain('joelgrus@gmail.com') == 'gmail.com' \n",
            "assert get_domain('joel@m.datasciencester.com') == 'm.datasciencester.com' \n",
            " \n",
            "from collections import Counter \n",
            " \n",
            "with open('email_addresses.txt', 'r') as f: \n",
            "    domain_counts = Counter(get_domain(line.strip()) \n",
            "                            for line in f \n",
            "                            if \"@\" in line)\n",
            "Delimited Files\n",
            "The  hypothetical email addresses file we just processed had one address per\n",
            "line. More frequently you’ll work with files with lots of data on each line.These files are very often either comma-separated  or tab-separated : each\n",
            "line has several fields, with a comma or a tab indicating where one field\n",
            "ends and the next field starts.\n",
            "This starts to get complicated when you have fields with commas and tabs\n",
            "and newlines in them (which you inevitably will). For this reason, you\n",
            "should never try to parse them yourself. Instead, you should use  Python’ s\n",
            "csv  module (or the pandas library , or some other library that’ s designed to\n",
            "read comma-separated or tab-delimited files).\n",
            "W A R N I N G\n",
            "Never parse a comma-separated file yourself. Y ou will screw up the edge cases!\n",
            "If your file has no headers (which means you probably want each row as a\n",
            "list , and which places the burden on you to know what’ s in each column),\n",
            "you can use csv.reader  to iterate over the rows, each of which will be an\n",
            "appropriately split list.\n",
            "For example, if we had a tab-delimited file of stock prices:\n",
            "6/20/2014   AAPL    90.91 \n",
            "6/20/2014   MSFT    41.68 \n",
            "6/20/2014   FB  64.5 \n",
            "6/19/2014   AAPL    91.86 \n",
            "6/19/2014   MSFT    41.51 \n",
            "6/19/2014   FB  64.34\n",
            "we could process them with:\n",
            "import csv \n",
            " \n",
            "with open('tab_delimited_stock_prices.txt') as f: \n",
            "    tab_reader = csv.reader(f, delimiter='\\t') \n",
            "    for row in tab_reader: \n",
            "        date = row[0] \n",
            "        symbol = row[1]closing_price = float(row[2]) \n",
            "        process(date, symbol, closing_price)\n",
            "If your file has headers:\n",
            "date:symbol:closing_price \n",
            "6/20/2014:AAPL:90.91 \n",
            "6/20/2014:MSFT:41.68 \n",
            "6/20/2014:FB:64.5\n",
            "you can either skip the header row with an initial call to reader.next , or\n",
            "get each row as a dict  (with the headers as keys) by using\n",
            "csv.DictReader :\n",
            "with open('colon_delimited_stock_prices.txt') as f: \n",
            "    colon_reader = csv.DictReader(f, delimiter=':') \n",
            "    for dict_row in colon_reader: \n",
            "        date = dict_row[\"date\"] \n",
            "        symbol = dict_row[\"symbol\"] \n",
            "        closing_price = float(dict_row[\"closing_price\"]) \n",
            "        process(date, symbol, closing_price)\n",
            "Even if your file doesn’ t have headers, you can still use DictReader  by\n",
            "passing it the keys as a fieldnames  parameter .\n",
            "Y ou can similarly write out delimited data using csv.writer :\n",
            "todays_prices = {'AAPL': 90.91, 'MSFT': 41.68, 'FB': 64.5 } \n",
            " \n",
            "with open('comma_delimited_stock_prices.txt', 'w') as f: \n",
            "    csv_writer = csv.writer(f, delimiter=',') \n",
            "    for stock, price in todays_prices.items(): \n",
            "        csv_writer.writerow([stock, price])\n",
            "csv.writer  will do the right thing if your fields themselves have commas\n",
            "in them. Y our own hand-rolled writer probably won’ t. For example, if you\n",
            "attempt:\n",
            "results = [[\"test1\", \"success\", \"Monday\"], \n",
            "           [\"test2\", \"success, kind of\", \"Tuesday\"],[\"test3\", \"failure, kind of\", \"Wednesday\"], \n",
            "           [\"test4\", \"failure, utter\", \"Thursday\"]] \n",
            " \n",
            "# don't do this! \n",
            "with open('bad_csv.txt', 'w') as f: \n",
            "    for row in results: \n",
            "        f.write(\",\".join(map(str, row))) # might have too many commas in it! \n",
            "        f.write(\"\\n\")                    # row might have newlines as well!\n",
            "Y ou will end up with a .csv  file that looks like this:\n",
            "test1,success,Monday \n",
            "test2,success, kind of,Tuesday \n",
            "test3,failure, kind of,Wednesday \n",
            "test4,failure, utter,Thursday\n",
            "and that no one will ever be able to make sense of.\n",
            "S c r a p i n g  t h e  W e b\n",
            "Another  way to get data is by scraping it from web pages. Fetching web\n",
            "pages, it turns out, is pretty easy; getting meaningful structured information\n",
            "out of them less so.\n",
            "HTML and the Parsing Thereof\n",
            "Pages  on the web are written in HTML, in which text is (ideally) marked up\n",
            "into elements and their attributes:\n",
            "<html> \n",
            "  <head> \n",
            "    <title>A web page</title> \n",
            "  </head> \n",
            "  <body> \n",
            "    <p id=\"author\">Joel Grus</p> \n",
            "    <p id=\"subject\">Data Science</p> \n",
            "  </body> \n",
            "</html>In a perfect world, where all web pages were marked up semantically for\n",
            "our benefit, we would be able to extract data using rules like “find the <p>\n",
            "element whose id  is subject  and return the text it contains.” In the actual\n",
            "world, HTML is not generally well formed, let alone annotated. This means\n",
            "we’ll need help making sense of it.\n",
            "T o  get data out of HTML, we will use the Beautiful Soup library , which\n",
            "builds a tree out of the various elements on a web page and provides a\n",
            "simple interface for accessing them. As I write this, the latest version is\n",
            "Beautiful Soup 4.6.0, which is what we’ll be using. W e’ll  also be using the\n",
            "Requests library , which is a much nicer way of making HTTP requests than\n",
            "anything that’ s built into Python.\n",
            "Python’ s built-in HTML parser is not that lenient, which means that it\n",
            "doesn’ t always cope well with HTML that’ s not perfectly formed. For that\n",
            "reason, we’ll also install the html5lib  parser .\n",
            "Making sure you’re in the correct virtual environment, install the libraries:\n",
            "python -m pip install beautifulsoup4 requests html5lib\n",
            "T o use Beautiful Soup, we pass a string containing HTML into the\n",
            "BeautifulSoup  function. In our examples, this will be the result of a call to\n",
            "requests.get :\n",
            "from bs4 import BeautifulSoup \n",
            "import requests \n",
            " \n",
            "# I put the relevant HTML file on GitHub. In order to fit \n",
            "# the URL in the book I had to split it across two lines. \n",
            "# Recall that whitespace-separated strings get concatenated. \n",
            "url = (\"https://raw.githubusercontent.com/\" \n",
            "       \"joelgrus/data/master/getting-data.html\") \n",
            "html = requests.get(url).text \n",
            "soup = BeautifulSoup(html, 'html5lib')\n",
            "after which we can get pretty far using a few simple methods.W e’ll typically work with Tag  objects, which correspond to the tags\n",
            "representing the structure of an HTML page.\n",
            "For example, to find the first <p>  tag (and its contents), you can use:\n",
            "first_paragraph = soup.find('p')        # or just soup.p\n",
            "Y ou can get the text contents of a Tag  using its text  property:\n",
            "first_paragraph_text = soup.p.text \n",
            "first_paragraph_words = soup.p.text.split()\n",
            "And you can extract a tag’ s attributes by treating it like a dict :\n",
            "first_paragraph_id = soup.p['id']       # raises KeyError if no 'id' \n",
            "first_paragraph_id2 = soup.p.get('id')  # returns None if no 'id'\n",
            "Y ou can get multiple tags at once as follows:\n",
            "all_paragraphs = soup.find_all('p')  # or just soup('p') \n",
            "paragraphs_with_ids = [p for p in soup('p') if p.get('id')]\n",
            "Frequently , you’ll want to find tags with a specific class :\n",
            "important_paragraphs = soup('p', {'class' : 'important'}) \n",
            "important_paragraphs2 = soup('p', 'important') \n",
            "important_paragraphs3 = [p for p in soup('p') \n",
            "                         if 'important' in p.get('class', [])]\n",
            "And you can combine these methods to implement more elaborate logic.\n",
            "For example, if you want to find every <span>  element that is contained\n",
            "inside a <div>  element, you could do this:\n",
            "# Warning: will return the same <span> multiple times \n",
            "# if it sits inside multiple <div>s. \n",
            "# Be more clever if that's the case. \n",
            "spans_inside_divs = [span \n",
            "                     for div in soup('div')     # for each <div> on the page \n",
            "                     for span in div('span')]   # find each <span> inside itJust this handful of features will allow us to do quite a lot. If you end up\n",
            "needing to do more complicated things (or if you’re just curious), check the\n",
            "documentation .\n",
            "Of course, the important data won’ t typically be labeled as\n",
            "class=\"important\" . Y ou’ll need to carefully inspect the source HTML,\n",
            "reason through your selection logic, and worry about edge cases to make\n",
            "sure your data is correct. Let’ s look at an example.\n",
            "Example: Keeping T abs on Congress\n",
            "The  VP of Policy at DataSciencester is worried about potential regulation of\n",
            "the data science industry and asks you to quantify what Congress is saying\n",
            "on the topic. In particular , he wants you to find all the representatives who\n",
            "have press releases about “data.”\n",
            "At the time of publication, there is a page with links to all of the\n",
            "representatives’ websites at https://www .house.gov/r epr esentatives .\n",
            "And if you “view source,” all of the links to the websites look like:\n",
            "<td> \n",
            "  <a href=\"https://jayapal.house.gov\">Jayapal, Pramila</a> \n",
            "</td>\n",
            "Let’ s start by collecting all of the URLs linked to from that page:\n",
            "from bs4 import BeautifulSoup \n",
            "import requests \n",
            " \n",
            "url = \"https://www.house.gov/representatives\" \n",
            "text = requests.get(url).text \n",
            "soup = BeautifulSoup(text, \"html5lib\") \n",
            " \n",
            "all_urls = [a['href'] \n",
            "            for a in soup('a') \n",
            "            if a.has_attr('href')] \n",
            " \n",
            "print(len(all_urls))  # 965 for me, way too manyThis returns way too many URLs. If you look at them, the ones we want\n",
            "start with either http://  or https:// , have some kind of name, and end with\n",
            "either .house.gov  or .house.gov/ .\n",
            "This is a good place to use a regular expression:\n",
            "import re \n",
            " \n",
            "# Must start with http:// or https:// \n",
            "# Must end with .house.gov or .house.gov/ \n",
            "regex = r\"^https?://.*\\.house\\.gov/?$\" \n",
            " \n",
            "# Let's write some tests! \n",
            "assert re.match(regex, \"http://joel.house.gov\") \n",
            "assert re.match(regex, \"https://joel.house.gov\") \n",
            "assert re.match(regex, \"http://joel.house.gov/\") \n",
            "assert re.match(regex, \"https://joel.house.gov/\") \n",
            "assert not re.match(regex, \"joel.house.gov\") \n",
            "assert not re.match(regex, \"http://joel.house.com\") \n",
            "assert not re.match(regex, \"https://joel.house.gov/biography\") \n",
            " \n",
            "# And now apply \n",
            "good_urls = [url for url in all_urls if re.match(regex, url)] \n",
            " \n",
            "print(len(good_urls))  # still 862 for me\n",
            "That’ s still way too many , as there are only 435 representatives. If you look\n",
            "at the list, there are a lot of duplicates. Let’ s use set  to get rid of them:\n",
            "good_urls = list(set(good_urls)) \n",
            " \n",
            "print(len(good_urls))  # only 431 for me\n",
            "There are always a couple of House seats empty , or maybe there’ s a\n",
            "representative without a website. In any case, this is good enough. When we\n",
            "look at the sites, most of them have a link to press releases. For example:\n",
            "html = requests.get('https://jayapal.house.gov').text \n",
            "soup = BeautifulSoup(html, 'html5lib') \n",
            " \n",
            "# Use a set because the links might appear multiple times. \n",
            "links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}print(links) # {'/media/press-releases'}\n",
            "Notice that this is a relative link, which means we need to remember the\n",
            "originating site. Let’ s do some scraping:\n",
            "from typing import Dict, Set \n",
            " \n",
            "press_releases: Dict[str, Set[str]] = {} \n",
            " \n",
            "for house_url in good_urls: \n",
            "    html = requests.get(house_url).text \n",
            "    soup = BeautifulSoup(html, 'html5lib') \n",
            "    pr_links = {a['href'] for a in soup('a') if 'press releases' \n",
            "                                             in a.text.lower()} \n",
            "    print(f\"{house_url}: {pr_links}\") \n",
            "    press_releases[house_url] = pr_links\n",
            "N O T E\n",
            "Normally  it is impolite to scrape a site freely like this. Most sites will have a r obots.txt\n",
            "file that indicates how frequently you may scrape the site (and which paths you’re not\n",
            "supposed to scrape), but since it’ s Congress we don’ t need to be particularly polite.\n",
            "If you watch these as they scroll by , you’ll see a lot of /media/pr ess-r eleases\n",
            "and media-center/pr ess-r eleases , as well as various other addresses. One of\n",
            "these URLs is https://jayapal.house.gov/media/pr ess-r eleases .\n",
            "Remember that our goal is to find out which congresspeople have press\n",
            "releases mentioning “data.” W e’ll write a slightly more general function\n",
            "that checks whether a page of press releases mentions any given term.\n",
            "If you visit the site and view the source, it seems like there’ s a snippet from\n",
            "each press release inside a <p>  tag, so we’ll use that as our first attempt:\n",
            "def paragraph_mentions(text: str, keyword: str) -> bool: \n",
            "    \"\"\" \n",
            "    Returns True if a <p> inside the text mentions {keyword} \n",
            "    \"\"\" \n",
            "    soup = BeautifulSoup(text, 'html5lib')paragraphs = [p.get_text() for p in soup('p')] \n",
            " \n",
            "    return any(keyword.lower() in paragraph.lower() \n",
            "               for paragraph in paragraphs)\n",
            "Let’ s write a quick test for it:\n",
            "text = \"\"\"<body><h1>Facebook</h1><p>Twitter</p>\"\"\" \n",
            "assert paragraph_mentions(text, \"twitter\")       # is inside a <p> \n",
            "assert not paragraph_mentions(text, \"facebook\")  # not inside a <p>\n",
            "At last we’re ready to find the relevant congresspeople and give their names\n",
            "to the VP:\n",
            "for house_url, pr_links in press_releases.items(): \n",
            "    for pr_link in pr_links: \n",
            "        url = f\"{house_url}/{pr_link}\" \n",
            "        text = requests.get(url).text \n",
            " \n",
            "        if paragraph_mentions(text, 'data'): \n",
            "            print(f\"{house_url}\") \n",
            "            break  # done with this house_url\n",
            "When I run this I get a list of about 20 representatives. Y our results will\n",
            "probably be dif ferent.\n",
            "N O T E\n",
            "If you look at the various “press releases” pages, most of them are paginated with only 5\n",
            "or 10 press releases per page. This means that we only retrieved the few most recent\n",
            "press releases for each congressperson. A more thorough solution would have iterated\n",
            "over the pages and retrieved the full text of each press release.\n",
            "U s i n g  A P I s\n",
            "Many  websites and web services provide application pr ogramming\n",
            "interfaces  (APIs), which allow you to explicitly request data in a structured\n",
            "format. This saves you the trouble of having to scrape them!JSON and XML\n",
            "Because  HTTP is a protocol for transferring text , the data you request\n",
            "through a web API needs to be serialized  into a  string format. Often  this\n",
            "serialization uses JavaScript Object Notation  (JSON). JavaScript objects\n",
            "look quite similar to Python dict s, which makes their string representations\n",
            "easy to interpret:\n",
            "{ \"title\" : \"Data Science Book\", \n",
            "  \"author\" : \"Joel Grus\", \n",
            "  \"publicationYear\" : 2019, \n",
            "  \"topics\" : [ \"data\", \"science\", \"data science\"] }\n",
            "W e  can parse JSON using Python’ s json  module. In particular , we will use\n",
            "its loads  function, which deserializes a string representing a JSON object\n",
            "into a Python object:\n",
            "import json \n",
            "serialized = \"\"\"{ \"title\" : \"Data Science Book\", \n",
            "                  \"author\" : \"Joel Grus\", \n",
            "                  \"publicationYear\" : 2019, \n",
            "                  \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\" \n",
            " \n",
            "# parse the JSON to create a Python dict \n",
            "deserialized = json.loads(serialized) \n",
            "assert deserialized[\"publicationYear\"] == 2019 \n",
            "assert \"data science\" in deserialized[\"topics\"]\n",
            "Sometimes an API provider hates you and provides only responses in XML:\n",
            "<Book> \n",
            "  <Title>Data Science Book</Title> \n",
            "  <Author>Joel Grus</Author> \n",
            "  <PublicationYear>2014</PublicationYear> \n",
            "  <Topics> \n",
            "    <Topic>data</Topic> \n",
            "    <Topic>science</Topic> \n",
            "    <Topic>data science</Topic> \n",
            "  </Topics> \n",
            "</Book>Y ou can use Beautiful Soup to get data from XML similarly to how we used\n",
            "it to get data from HTML; check its documentation for details.\n",
            "Using an Unauthenticated API\n",
            "Most  APIs these days require that you first authenticate yourself before you\n",
            "can use them. While we don’ t begrudge them this policy , it creates a lot of\n",
            "extra boilerplate that muddies up our exposition. Accordingly , we’ll start by\n",
            "taking a look at GitHub’ s API , with which you can do some simple things\n",
            "unauthenticated:\n",
            "import requests, json \n",
            " \n",
            "github_user = \"joelgrus\" \n",
            "endpoint = f\"https://api.github.com/users/{github_user}/repos\" \n",
            " \n",
            "repos = json.loads(requests.get(endpoint).text)\n",
            "At this point repos  is a list  of Python dict s, each representing a public\n",
            "repository in my GitHub account. (Feel free to substitute your username\n",
            "and get your GitHub repository data instead. Y ou do have a GitHub\n",
            "account, right?)\n",
            "W e can use this to figure out which months and days of the week I’m most\n",
            "likely to create a repository . The only issue is that the dates in the response\n",
            "are strings:\n",
            "\"created_at\": \"2013-07-05T02:02:28Z\"\n",
            "Python doesn’ t come with a great date parser , so we’ll need to install one:\n",
            "python -m pip install python-dateutil\n",
            "from which you’ll probably only ever need the dateutil.parser.parse\n",
            "function:\n",
            "from collections import Counter \n",
            "from dateutil.parser import parsedates = [parse(repo[\"created_at\"]) for repo in repos] \n",
            "month_counts = Counter(date.month for date in dates) \n",
            "weekday_counts = Counter(date.weekday() for date in dates)\n",
            "Similarly , you can get the languages of my last five repositories:\n",
            "last_5_repositories = sorted(repos, \n",
            "                             key=lambda r: r[\"pushed_at\"], \n",
            "                             reverse=True)[:5] \n",
            " \n",
            "last_5_languages = [repo[\"language\"] \n",
            "                    for repo in last_5_repositories]\n",
            "T ypically we won’ t be working with APIs at this low “make the requests\n",
            "and parse the responses ourselves” level. One of the benefits of using\n",
            "Python is that someone has already built a library for pretty much any API\n",
            "you’re interested in accessing. When they’re done well, these libraries can\n",
            "save you a lot of the trouble of figuring out the hairier details of API access.\n",
            "(When they’re not done well, or when it turns out they’re based on defunct\n",
            "versions of the corresponding APIs, they can cause you enormous\n",
            "headaches.)\n",
            "Nonetheless, you’ll occasionally have to roll your own API access library\n",
            "(or , more likely , debug why someone else’ s isn’ t working), so it’ s good to\n",
            "know some of the details.\n",
            "Finding APIs\n",
            "If you need data from a specific site, look for a “developers” or “API”\n",
            "section of the site for details, and try searching the web for “python\n",
            "<sitename> api” to find a library .\n",
            "There are libraries for the Y elp API, for the Instagram API, for the Spotify\n",
            "API, and so on.\n",
            "If you’re looking for a list of APIs that have Python wrappers, there’ s a nice\n",
            "one from Real Python on GitHub .And if you can’ t find what you need, there’ s always scraping, the last refuge\n",
            "of the data scientist.\n",
            "E x a m p l e :  U s i n g  t h e  T w i t t e r  A P I s\n",
            "T witter  is a fantastic source of data to work with. Y ou can use it to get real-\n",
            "time news. Y ou can use it to measure reactions to current events. Y ou can\n",
            "use it to find links related to specific topics. Y ou can use it for pretty much\n",
            "anything you can imagine, just as long as you can get access to its data. And\n",
            "you can get access to its data through its APIs.\n",
            "T o  interact with the T witter APIs, we’ll be using the T wython library\n",
            "(python -m pip install twython ). There are quite a few Python T witter\n",
            "libraries out there, but this is the one that I’ve had the most success working\n",
            "with. Y ou are encouraged to explore the others as well!\n",
            "Getting Credentials\n",
            "In order to use T witter ’ s APIs, you need to get some credentials (for which\n",
            "you need a T witter account, which you should have anyway so that you can\n",
            "be part of the lively and friendly T witter #datascience community).\n",
            "W A R N I N G\n",
            "Like all instructions that relate to websites that I don’ t control, these may become\n",
            "obsolete at some point but will hopefully work for a while. (Although they have already\n",
            "changed multiple times since I originally started writing this book, so good luck!)\n",
            "Here are the steps:\n",
            "1 . Go to https://developer .twitter .com/ .\n",
            "2 . If you are not signed in, click “Sign in” and enter your T witter\n",
            "username and password.\n",
            "3 . Click Apply to apply for a developer account.4 . Request access for your own personal use.\n",
            "5 . Fill out the application. It requires 300 words (really) on why you\n",
            "need access, so to get over the limit you could tell them about this\n",
            "book and how much you’re enjoying it.\n",
            "6 . W ait some indefinite amount of time.\n",
            "7 . If you know someone who works at T witter , email them and ask\n",
            "them if they can expedite your application. Otherwise, keep\n",
            "waiting.\n",
            "8 . Once you get approved, go back to developer .twitter .com , find the\n",
            "“Apps” section, and click “Create an app.”\n",
            "9 . Fill out all the required fields (again, if you need extra characters\n",
            "for the description, you could talk about this book and how\n",
            "edifying you’re finding it).\n",
            "10 . Click CREA TE.\n",
            "Now your app should have a “Keys and tokens” tab with a “Consumer API\n",
            "keys” section that lists an “API key” and an “API secret key .” T ake note of\n",
            "those keys; you’ll need them. (Also, keep them secret! They’re like\n",
            "passwords.)\n",
            "C A U T I O N\n",
            "Don’ t share the keys, don’ t publish them in your book, and don’ t check them into your\n",
            "public GitHub repository . One simple solution is to store them in a cr edentials.json  file\n",
            "that doesn’ t get checked in, and to have your code use json.loads  to retrieve them.\n",
            "Another solution is to store them in environment variables and use os.environ  to\n",
            "retrieve them.\n",
            "Using T wython\n",
            "The trickiest part of using the T witter API is authenticating yourself.\n",
            "(Indeed, this is the trickiest part of using a lot of APIs.) API providers wantto make sure that you’re authorized to access their data and that you don’ t\n",
            "exceed their usage limits. They also want to know who’ s accessing their\n",
            "data.\n",
            "Authentication is kind of a pain. There is a simple way , OAuth 2, that\n",
            "suf fices when you just want to do simple searches. And there is a complex\n",
            "way , OAuth 1, that’ s required when you want to perform actions (e.g.,\n",
            "tweeting) or (in particular for us) connect to the T witter stream.\n",
            "So we’re stuck with the more complicated way , which we’ll try to automate\n",
            "as much as we can.\n",
            "First, you need your API key and API secret key (sometimes known as the\n",
            "consumer key and consumer secret, respectively). I’ll be getting mine from\n",
            "environment variables, but feel free to substitute in yours however you\n",
            "wish:\n",
            "import os \n",
            " \n",
            "# Feel free to plug your key and secret in directly \n",
            "CONSUMER_KEY = os.environ.get(\"TWITTER_CONSUMER_KEY\") \n",
            "CONSUMER_SECRET = os.environ.get(\"TWITTER_CONSUMER_SECRET\")\n",
            "Now we can instantiate the client:\n",
            "import webbrowser \n",
            "from twython import Twython \n",
            " \n",
            "# Get a temporary client to retrieve an authentication URL \n",
            "temp_client = Twython(CONSUMER_KEY, CONSUMER_SECRET) \n",
            "temp_creds = temp_client.get_authentication_tokens() \n",
            "url = temp_creds['auth_url'] \n",
            " \n",
            "# Now visit that URL to authorize the application and get a PIN \n",
            "print(f\"go visit {url} and get the PIN code and paste it below\") \n",
            "webbrowser.open(url) \n",
            "PIN_CODE = input(\"please enter the PIN code: \") \n",
            " \n",
            "# Now we use that PIN_CODE to get the actual tokens \n",
            "auth_client = Twython(CONSUMER_KEY, \n",
            "                      CONSUMER_SECRET, \n",
            "                      temp_creds['oauth_token'],temp_creds['oauth_token_secret']) \n",
            "final_step = auth_client.get_authorized_tokens(PIN_CODE) \n",
            "ACCESS_TOKEN = final_step['oauth_token'] \n",
            "ACCESS_TOKEN_SECRET = final_step['oauth_token_secret'] \n",
            " \n",
            "# And get a new Twython instance using them. \n",
            "twitter = Twython(CONSUMER_KEY, \n",
            "                  CONSUMER_SECRET, \n",
            "                  ACCESS_TOKEN, \n",
            "                  ACCESS_TOKEN_SECRET)\n",
            "T I P\n",
            "At this point you may want to consider saving the ACCESS_TOKEN  and\n",
            "ACCESS_TOKEN_SECRET  somewhere safe, so that next time you don’ t have to go through\n",
            "this rigmarole.\n",
            "Once we have an authenticated Twython  instance, we can start performing\n",
            "searches:\n",
            "# Search for tweets containing the phrase \"data science\" \n",
            "for status in twitter.search(q='\"data science\"')[\"statuses\"]: \n",
            "    user = status[\"user\"][\"screen_name\"] \n",
            "    text = status[\"text\"] \n",
            "    print(f\"{user}: {text}\\n\")\n",
            "If you run this, you should get some tweets back like:\n",
            "haithemnyc: Data scientists with the technical savvy &amp; analytical chops to \n",
            "derive meaning from big data are in demand. http://t.co/HsF9Q0dShP \n",
            " \n",
            "RPubsRecent: Data Science http://t.co/6hcHUz2PHM \n",
            " \n",
            "spleonard1: Using #dplyr in #R to work through a procrastinated assignment for \n",
            "@rdpeng in @coursera data science specialization. So easy and Awesome.\n",
            "This isn’ t that interesting, lar gely because the T witter Search API just\n",
            "shows you whatever handful of recent results it feels like. When you’re\n",
            "doing data science, more often you want a lot of tweets. This is where the\n",
            "Streaming API  is useful. It allows you to connect to (a sample of) the greatT witter firehose. T o use it, you’ll need to authenticate using your access\n",
            "tokens.\n",
            "In order to access the Streaming API with T wython, we need to define a\n",
            "class that inherits from TwythonStreamer  and that overrides its\n",
            "on_success  method, and possibly its on_error  method:\n",
            "from twython import TwythonStreamer \n",
            " \n",
            "# Appending data to a global variable is pretty poor form \n",
            "# but it makes the example much simpler \n",
            "tweets = [] \n",
            " \n",
            "class MyStreamer(TwythonStreamer): \n",
            "    def on_success(self, data): \n",
            "        \"\"\" \n",
            "        What do we do when Twitter sends us data? \n",
            "        Here data will be a Python dict representing a tweet. \n",
            "        \"\"\" \n",
            "        # We only want to collect English-language tweets \n",
            "        if data.get('lang') == 'en': \n",
            "            tweets.append(data) \n",
            "            print(f\"received tweet #{len(tweets)}\") \n",
            " \n",
            "        # Stop when we've collected enough \n",
            "        if len(tweets) >= 100: \n",
            "            self.disconnect() \n",
            " \n",
            "    def on_error(self, status_code, data): \n",
            "        print(status_code, data) \n",
            "        self.disconnect()\n",
            "MyStreamer  will connect to the T witter stream and wait for T witter to feed\n",
            "it data. Each time it receives some data (here, a tweet represented as a\n",
            "Python object), it passes it to the on_success  method, which appends it to\n",
            "our tweets  list if its language is English, and then disconnects the streamer\n",
            "after it’ s collected 1,000 tweets.\n",
            "All that’ s left is to initialize it and start it running:\n",
            "stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET, \n",
            "                    ACCESS_TOKEN, ACCESS_TOKEN_SECRET)# starts consuming public statuses that contain the keyword 'data' \n",
            "stream.statuses.filter(track='data') \n",
            " \n",
            "# if instead we wanted to start consuming a sample of *all* public statuses \n",
            "# stream.statuses.sample()\n",
            "This will run until it collects 100 tweets (or until it encounters an error) and\n",
            "stop, at which point you can start analyzing those tweets. For instance, you\n",
            "could find the most common hashtags with:\n",
            "top_hashtags = Counter(hashtag['text'].lower() \n",
            "                       for tweet in tweets \n",
            "                       for hashtag in tweet[\"entities\"][\"hashtags\"]) \n",
            " \n",
            "print(top_hashtags.most_common(5))\n",
            "Each tweet contains a lot of data. Y ou can either poke around yourself or\n",
            "dig through the T witter API documentation .\n",
            "N O T E\n",
            "In a non-toy project, you probably wouldn’ t want to rely on an in-memory list  for\n",
            "storing the tweets. Instead you’d want to save them to a file or a database, so that you’d\n",
            "have them permanently .\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "pandas  is the  primary library that data science types use for\n",
            "working with—and, in particular , importing—data.\n",
            "Scrapy  is  a full-featured library for building complicated web\n",
            "scrapers that do things like follow unknown links.\n",
            "Kaggle  hosts  a lar ge collection of datasets.Chapter 10. W orking with Data\n",
            "Experts often possess mor e data than judgment.\n",
            "— Colin Powell\n",
            "W orking with data is both an art and a science. W e’ve mostly been talking\n",
            "about the science part, but in this chapter we’ll look at some of the art.\n",
            "E x p l o r i n g  Y o u r  D a t a\n",
            "After  you’ve identified the questions you’re trying to answer and have\n",
            "gotten your hands on some data, you might be tempted to dive in and\n",
            "immediately start building models and getting answers. But you should\n",
            "resist this ur ge. Y our first step should be to explor e  your data.\n",
            "Exploring One-Dimensional Data\n",
            "The simplest case is when you have a  one-dimensional dataset, which is just\n",
            "a collection of numbers. For example, these could be the daily average\n",
            "number of minutes each user spends on your site, the number of times each\n",
            "of a collection of data science tutorial videos was watched, or the number of\n",
            "pages of each of the data science books in your data science library .\n",
            "An obvious first step is to compute a few summary statistics. Y ou’d like to\n",
            "know how many data points you have, the smallest, the lar gest, the mean,\n",
            "and the standard deviation.\n",
            "But even these don’ t necessarily give you a great understanding. A good\n",
            "next step is to create a histogram, in which you group your data into\n",
            "discrete buckets  and count how many points fall into each bucket:\n",
            "from typing import List, Dict \n",
            "from collections import Counter \n",
            "import mathimport matplotlib.pyplot as plt \n",
            " \n",
            "def bucketize(point: float, bucket_size: float) -> float: \n",
            "    \"\"\"Floor the point to the next lower multiple of bucket_size\"\"\" \n",
            "    return bucket_size * math.floor(point / bucket_size) \n",
            " \n",
            "def make_histogram(points: List[float], bucket_size: float) -> Dict[float, \n",
            "int]: \n",
            "    \"\"\"Buckets the points and counts how many in each bucket\"\"\" \n",
            "    return Counter(bucketize(point, bucket_size) for point in points) \n",
            " \n",
            "def plot_histogram(points: List[float], bucket_size: float, title: str = \"\"): \n",
            "    histogram = make_histogram(points, bucket_size) \n",
            "    plt.bar(histogram.keys(), histogram.values(), width=bucket_size) \n",
            "    plt.title(title)\n",
            "For example, consider the two following sets of data:\n",
            "import random \n",
            "from scratch.probability import inverse_normal_cdf \n",
            " \n",
            "random.seed(0) \n",
            " \n",
            "# uniform between -100 and 100 \n",
            "uniform = [200 * random.random() - 100 for _ in range(10000)] \n",
            " \n",
            "# normal distribution with mean 0, standard deviation 57 \n",
            "normal = [57 * inverse_normal_cdf(random.random()) \n",
            "          for _ in range(10000)]\n",
            "Both have means close to 0 and standard deviations close to 58. However ,\n",
            "they have very dif ferent distributions. Figure 10-1  shows the distribution of\n",
            "uniform :\n",
            "plot_histogram(uniform, 10, \"Uniform Histogram\")\n",
            "while Figure 10-2  shows the distribution of normal :\n",
            "plot_histogram(normal, 10, \"Normal Histogram\")Figur e 10-1. Histogram of uniform\n",
            "In this case the two distributions have a pretty dif ferent max  and min , but\n",
            "even knowing that wouldn’ t have been suf ficient to understand how  they\n",
            "dif fered.\n",
            "T wo Dimensions\n",
            "Now  imagine you have a dataset with two dimensions. Maybe in addition to\n",
            "daily minutes you have years of data science experience. Of course you’d\n",
            "want to understand each dimension individually . But you probably also\n",
            "want to scatter the data.\n",
            "For example, consider another fake dataset:\n",
            "def random_normal() -> float: \n",
            "    \"\"\"Returns a random draw from a standard normal distribution\"\"\" \n",
            "    return inverse_normal_cdf(random.random())xs = [random_normal() for _ in range(1000)] \n",
            "ys1 = [ x + random_normal() / 2 for x in xs] \n",
            "ys2 = [-x + random_normal() / 2 for x in xs]\n",
            "If you were to run plot_histogram  on ys1  and ys2 , you’d get similar -\n",
            "looking plots (indeed, both are normally distributed with the same mean\n",
            "and standard deviation).\n",
            "Figur e 10-2. Histogram of normal\n",
            "But each has a very dif ferent joint distribution with xs , as shown in\n",
            "Figure 10-3 :\n",
            "plt.scatter(xs, ys1, marker='.', color='black', label='ys1') \n",
            "plt.scatter(xs, ys2, marker='.', color='gray',  label='ys2') \n",
            "plt.xlabel('xs') \n",
            "plt.ylabel('ys') \n",
            "plt.legend(loc=9) \n",
            "plt.title(\"Very Different Joint Distributions\") \n",
            "plt.show()Figur e 10-3. Scattering two differ ent ys\n",
            "This dif ference would also be apparent if you looked at the correlations:\n",
            "from scratch.statistics import correlation \n",
            " \n",
            "print(correlation(xs, ys1))      # about 0.9 \n",
            "print(correlation(xs, ys2))      # about -0.9\n",
            "Many Dimensions\n",
            "W ith  many dimensions, you’d like to know how all the dimensions relate to\n",
            "one another . A simple approach is to look at the corr elation matrix , in\n",
            "which the entry in row i  and column j  is the correlation between the i th\n",
            "dimension and the j th dimension of the data:\n",
            "from scratch.linear_algebra import Matrix, Vector, make_matrix \n",
            " \n",
            "def correlation_matrix(data: List[Vector]) -> Matrix:\"\"\" \n",
            "    Returns the len(data) x len(data) matrix whose (i, j)-th entry \n",
            "    is the correlation between data[i] and data[j] \n",
            "    \"\"\" \n",
            "    def correlation_ij(i: int, j: int) -> float: \n",
            "        return correlation(data[i], data[j]) \n",
            " \n",
            "    return make_matrix(len(data), len(data), correlation_ij)\n",
            "A  more visual approach (if you don’ t have too many dimensions) is to make\n",
            "a scatterplot matrix  ( Figure 10-4 ) showing all the pairwise scatterplots. T o\n",
            "do that we’ll use plt.subplots , which allows us to create subplots of our\n",
            "chart. W e give it the number of rows and the number of columns, and it\n",
            "returns a figure  object (which we won’ t use) and a two-dimensional array\n",
            "of axes  objects (each of which we’ll plot to):\n",
            "# corr_data is a list of four 100-d vectors \n",
            "num_vectors = len(corr_data) \n",
            "fig, ax = plt.subplots(num_vectors, num_vectors) \n",
            " \n",
            "for i in range(num_vectors): \n",
            "    for j in range(num_vectors): \n",
            " \n",
            "        # Scatter column_j on the x-axis vs. column_i on the y-axis \n",
            "        if i != j: ax[i][j].scatter(corr_data[j], corr_data[i]) \n",
            " \n",
            "        # unless i == j, in which case show the series name \n",
            "        else: ax[i][j].annotate(\"series \" + str(i), (0.5, 0.5), \n",
            "                                xycoords='axes fraction', \n",
            "                                ha=\"center\", va=\"center\") \n",
            " \n",
            "        # Then hide axis labels except left and bottom charts \n",
            "        if i < num_vectors - 1: ax[i][j].xaxis.set_visible(False) \n",
            "        if j > 0: ax[i][j].yaxis.set_visible(False) \n",
            " \n",
            "# Fix the bottom-right and top-left axis labels, which are wrong because \n",
            "# their charts only have text in them \n",
            "ax[-1][-1].set_xlim(ax[0][-1].get_xlim()) \n",
            "ax[0][0].set_ylim(ax[0][1].get_ylim()) \n",
            " \n",
            "plt.show()Figur e 10-4. Scatterplot matrix\n",
            "Looking at the scatterplots, you can see that series 1 is very negatively\n",
            "correlated with series 0, series 2 is positively correlated with series 1, and\n",
            "series 3 only takes on the values 0 and 6, with 0 corresponding to small\n",
            "values of series 2 and 6 corresponding to lar ge values.\n",
            "This is a quick way to get a rough sense of which of your variables are\n",
            "correlated (unless you spend hours tweaking matplotlib to display things\n",
            "exactly the way you want them to, in which case it’ s not a quick way).\n",
            "U s i n g  N a m e d T u p l e s\n",
            "One  common way of representing data is using dict s:\n",
            "import datetime \n",
            " \n",
            "stock_price = {'closing_price': 102.06,'date': datetime.date(2014, 8, 29), \n",
            "               'symbol': 'AAPL'}\n",
            "There are several reasons why this is less than ideal, however . This is a\n",
            "slightly inef ficient representation (a dict  involves some overhead), so that\n",
            "if you have a lot of stock prices they’ll take up more memory than they\n",
            "have to. For the most part, this is a minor consideration.\n",
            "A lar ger issue is that accessing things by dict  key is error -prone. The\n",
            "following code will run without error and just do the wrong thing:\n",
            "# oops, typo \n",
            "stock_price['cosing_price'] = 103.06\n",
            "Finally , while we can type-annotate uniform dictionaries:\n",
            "prices: Dict[datetime.date, float] = {}\n",
            "there’ s no helpful way to annotate dictionaries-as-data that have lots of\n",
            "dif ferent value types. So we also lose the power of type hints.\n",
            "As an alternative, Python includes a namedtuple  class, which is like a\n",
            "tuple  but with named slots:\n",
            "from collections import namedtuple \n",
            " \n",
            "StockPrice = namedtuple('StockPrice', ['symbol', 'date', 'closing_price']) \n",
            "price = StockPrice('MSFT', datetime.date(2018, 12, 14), 106.03) \n",
            " \n",
            "assert price.symbol == 'MSFT' \n",
            "assert price.closing_price == 106.03\n",
            "Like regular tuple s, namedtuple s are immutable, which means that you\n",
            "can’ t modify their values once they’re created. Occasionally this will get in\n",
            "our way , but mostly that’ s a good thing.\n",
            "Y ou’ll notice that we still haven’ t solved the type annotation issue. W e do\n",
            "that by using the typed variant, NamedTuple :from typing import NamedTuple \n",
            " \n",
            "class StockPrice(NamedTuple): \n",
            "    symbol: str \n",
            "    date: datetime.date \n",
            "    closing_price: float \n",
            " \n",
            "    def is_high_tech(self) -> bool: \n",
            "        \"\"\"It's a class, so we can add methods too\"\"\" \n",
            "        return self.symbol in ['MSFT', 'GOOG', 'FB', 'AMZN', 'AAPL'] \n",
            " \n",
            "price = StockPrice('MSFT', datetime.date(2018, 12, 14), 106.03) \n",
            " \n",
            "assert price.symbol == 'MSFT' \n",
            "assert price.closing_price == 106.03 \n",
            "assert price.is_high_tech()\n",
            "And now your editor can help you out, as shown in Figure 10-5 .\n",
            "Figur e 10-5. Helpful editor\n",
            "N O T E\n",
            "V ery few people use NamedTuple  in this way . But they should!\n",
            "D a t a c l a s s e s\n",
            "Dataclasses  are (sort of) a mutable version of NamedTuple . (I say “sort of”\n",
            "because NamedTuple s represent their data compactly as tuples, whereas\n",
            "dataclasses are regular Python classes that simply generate some methods\n",
            "for you automatically .)N O T E\n",
            "Dataclasses are new in Python 3.7. If you’re using an older version, this section won’ t\n",
            "work for you.\n",
            "The syntax is very similar to NamedTuple . But instead of inheriting from a\n",
            "base class, we use a decorator:\n",
            "from dataclasses import dataclass \n",
            " \n",
            "@dataclass \n",
            "class StockPrice2: \n",
            "    symbol: str \n",
            "    date: datetime.date \n",
            "    closing_price: float \n",
            " \n",
            "    def is_high_tech(self) -> bool: \n",
            "        \"\"\"It's a class, so we can add methods too\"\"\" \n",
            "        return self.symbol in ['MSFT', 'GOOG', 'FB', 'AMZN', 'AAPL'] \n",
            " \n",
            "price2 = StockPrice2('MSFT', datetime.date(2018, 12, 14), 106.03) \n",
            " \n",
            "assert price2.symbol == 'MSFT' \n",
            "assert price2.closing_price == 106.03 \n",
            "assert price2.is_high_tech()\n",
            "As mentioned, the big dif ference is that we can modify a dataclass\n",
            "instance’ s values:\n",
            "# stock split \n",
            "price2.closing_price /= 2 \n",
            "assert price2.closing_price == 51.03\n",
            "If we tried to modify a field of the NamedTuple  version, we’d get an\n",
            "AttributeError .\n",
            "This also leaves us susceptible to the kind of errors we were hoping to\n",
            "avoid by not using dict s:# It's a regular class, so add new fields however you like! \n",
            "price2.cosing_price = 75  # oops\n",
            "W e won’ t be using dataclasses, but you may encounter them out in the wild.\n",
            "C l e a n i n g  a n d  M u n g i n g\n",
            "Real-world  data is dirty . Often you’ll have to do some work on it before\n",
            "you can use it. W e saw examples of this in Chapter 9 . W e have to convert\n",
            "strings to float s or int s before we can use them. W e have to check for\n",
            "missing values and outliers and bad data.\n",
            "Previously , we did that right before using the data:\n",
            "closing_price = float(row[2])\n",
            "But it’ s probably less error -prone to do the parsing in a function that we can\n",
            "test:\n",
            "from dateutil.parser import parse \n",
            " \n",
            "def parse_row(row: List[str]) -> StockPrice: \n",
            "    symbol, date, closing_price = row \n",
            "    return StockPrice(symbol=symbol, \n",
            "                      date=parse(date).date(), \n",
            "                      closing_price=float(closing_price)) \n",
            " \n",
            "# Now test our function \n",
            "stock = parse_row([\"MSFT\", \"2018-12-14\", \"106.03\"]) \n",
            " \n",
            "assert stock.symbol == \"MSFT\" \n",
            "assert stock.date == datetime.date(2018, 12, 14) \n",
            "assert stock.closing_price == 106.03\n",
            "What if there’ s bad data? A “float” value that doesn’ t actually represent a\n",
            "number? Maybe you’d rather get a None  than crash your program?\n",
            "from typing import Optional \n",
            "import redef try_parse_row(row: List[str]) -> Optional[StockPrice]: \n",
            "    symbol, date_, closing_price_ = row \n",
            " \n",
            "    # Stock symbol should be all capital letters \n",
            "    if not re.match(r\"^[A-Z]+$\", symbol): \n",
            "        return None \n",
            " \n",
            "    try: \n",
            "        date = parse(date_).date() \n",
            "    except ValueError: \n",
            "        return None \n",
            " \n",
            "    try: \n",
            "        closing_price = float(closing_price_) \n",
            "    except ValueError: \n",
            "        return None \n",
            " \n",
            "    return StockPrice(symbol, date, closing_price) \n",
            " \n",
            "# Should return None for errors \n",
            "assert try_parse_row([\"MSFT0\", \"2018-12-14\", \"106.03\"]) is None \n",
            "assert try_parse_row([\"MSFT\", \"2018-12--14\", \"106.03\"]) is None \n",
            "assert try_parse_row([\"MSFT\", \"2018-12-14\", \"x\"]) is None \n",
            " \n",
            "# But should return same as before if data is good \n",
            "assert try_parse_row([\"MSFT\", \"2018-12-14\", \"106.03\"]) == stock\n",
            "For example, if we have comma-delimited stock prices with bad data:\n",
            "AAPL,6/20/2014,90.91 \n",
            "MSFT,6/20/2014,41.68 \n",
            "FB,6/20/3014,64.5 \n",
            "AAPL,6/19/2014,91.86 \n",
            "MSFT,6/19/2014,n/a \n",
            "FB,6/19/2014,64.34\n",
            "we can now read and return only the valid rows:\n",
            "import csv \n",
            " \n",
            "data: List[StockPrice] = [] \n",
            " \n",
            "with open(\"comma_delimited_stock_prices.csv\") as f: \n",
            "    reader = csv.reader(f) \n",
            "    for row in reader:maybe_stock = try_parse_row(row) \n",
            "        if maybe_stock is None: \n",
            "            print(f\"skipping invalid row: {row}\") \n",
            "        else: \n",
            "            data.append(maybe_stock)\n",
            "and decide what we want to do about the invalid ones. Generally speaking,\n",
            "the three options are to get rid of them, to go back to the source and try to\n",
            "fix the bad/missing data, or to do nothing and cross our fingers. If there’ s\n",
            "one bad row out of millions, it’ s probably okay to ignore it. But if half your\n",
            "rows have bad data, that’ s something you need to fix.\n",
            "A good next step is to check for outliers, using techniques from “Exploring\n",
            "Y our Data”  or by ad hoc investigating. For example, did you notice that one\n",
            "of the dates in the stocks file had the year 3014? That won’ t (necessarily)\n",
            "give you an error , but it’ s quite plainly wrong, and you’ll get screwy results\n",
            "if you don’ t catch it. Real-world datasets have missing decimal points, extra\n",
            "zeros, typographical errors, and countless other problems that it’ s your job\n",
            "to catch. (Maybe it’ s not of ficially your job, but who else is going to do it?)\n",
            "M a n i p u l a t i n g  D a t a\n",
            "One  of the most important skills of a data scientist is manipulating data . It’ s\n",
            "more of a general approach than a specific technique, so we’ll just work\n",
            "through a handful of examples to give you the flavor of it.\n",
            "Imagine we have a bunch of stock price data that looks like this:\n",
            "data = [ \n",
            "    StockPrice(symbol='MSFT', \n",
            "               date=datetime.date(2018, 12, 24), \n",
            "               closing_price=106.03), \n",
            "    # ... \n",
            "]\n",
            "Let’ s start asking questions about this data. Along the way we’ll try to\n",
            "notice patterns in what we’re doing and abstract out some tools to make the\n",
            "manipulation easier .For instance, suppose we want to know the highest-ever closing price for\n",
            "AAPL. Let’ s break this down into concrete steps:\n",
            "1 . Restrict ourselves to AAPL rows.\n",
            "2 . Grab the closing_price  from each row .\n",
            "3 . T ake the max  of those prices.\n",
            "W e can do all three at once using a comprehension:\n",
            "max_aapl_price = max(stock_price.closing_price \n",
            "                     for stock_price in data \n",
            "                     if stock_price.symbol == \"AAPL\")\n",
            "More generally , we might want to know the highest-ever closing price for\n",
            "each stock in our dataset. One way to do this is:\n",
            "1 . Create a dict  to keep track of highest prices (we’ll use a\n",
            "defaultdict  that returns minus infinity for missing values, since\n",
            "any price will be greater than that).\n",
            "2 . Iterate over our data, updating it.\n",
            "Here’ s the code:\n",
            "from collections import defaultdict \n",
            " \n",
            "max_prices: Dict[str, float] = defaultdict(lambda: float('-inf')) \n",
            " \n",
            "for sp in data: \n",
            "    symbol, closing_price = sp.symbol, sp.closing_price \n",
            "    if closing_price > max_prices[symbol]: \n",
            "        max_prices[symbol] = closing_price\n",
            "W e can now start to ask more complicated things, like what are the lar gest\n",
            "and smallest one-day percent changes in our dataset. The percent change is\n",
            "price_today / price_yesterday - 1 , which means we need some way\n",
            "of associating today’ s price and yesterday’ s price. One approach is to group\n",
            "the prices by symbol, and then, within each group:1 . Order the prices by date.\n",
            "2 . Use zip  to get (previous, current) pairs.\n",
            "3 . T urn the pairs into new “percent change” rows.\n",
            "Let’ s start by grouping the prices by symbol:\n",
            "from typing import List \n",
            "from collections import defaultdict \n",
            " \n",
            "# Collect the prices by symbol \n",
            "prices: Dict[str, List[StockPrice]] = defaultdict(list) \n",
            " \n",
            "for sp in data: \n",
            "    prices[sp.symbol].append(sp)\n",
            "Since the prices are tuples, they’ll get sorted by their fields in order: first by\n",
            "symbol, then by date, then by price. This means that if we have some prices\n",
            "all with the same symbol, sort  will sort them by date (and then by price,\n",
            "which does nothing, since we only have one per date), which is what we\n",
            "want.\n",
            "# Order the prices by date \n",
            "prices = {symbol: sorted(symbol_prices) \n",
            "          for symbol, symbol_prices in prices.items()}\n",
            "which we can use to compute a sequence of day-over -day changes:\n",
            "def pct_change(yesterday: StockPrice, today: StockPrice) -> float: \n",
            "    return today.closing_price / yesterday.closing_price - 1 \n",
            " \n",
            "class DailyChange(NamedTuple): \n",
            "    symbol: str \n",
            "    date: datetime.date \n",
            "    pct_change: float \n",
            " \n",
            "def day_over_day_changes(prices: List[StockPrice]) -> List[DailyChange]: \n",
            "    \"\"\" \n",
            "    Assumes prices are for one stock and are in order \n",
            "    \"\"\" \n",
            "    return [DailyChange(symbol=today.symbol,date=today.date, \n",
            "                        pct_change=pct_change(yesterday, today)) \n",
            "            for yesterday, today in zip(prices, prices[1:])]\n",
            "and then collect them all:\n",
            "all_changes = [change \n",
            "               for symbol_prices in prices.values() \n",
            "               for change in day_over_day_changes(symbol_prices)]\n",
            "At which point it’ s easy to find the lar gest and smallest:\n",
            "max_change = max(all_changes, key=lambda change: change.pct_change) \n",
            "# see e.g. http://news.cnet.com/2100-1001-202143.html \n",
            "assert max_change.symbol == 'AAPL' \n",
            "assert max_change.date == datetime.date(1997, 8, 6) \n",
            "assert 0.33 < max_change.pct_change < 0.34 \n",
            " \n",
            "min_change = min(all_changes, key=lambda change: change.pct_change) \n",
            "# see e.g. http://money.cnn.com/2000/09/29/markets/techwrap/ \n",
            "assert min_change.symbol == 'AAPL' \n",
            "assert min_change.date == datetime.date(2000, 9, 29) \n",
            "assert -0.52 < min_change.pct_change < -0.51\n",
            "W e can now use this new all_changes  dataset to find which month is the\n",
            "best to invest in tech stocks. W e’ll just look at the average daily change by\n",
            "month:\n",
            "changes_by_month: List[DailyChange] = {month: [] for month in range(1, 13)} \n",
            " \n",
            "for change in all_changes: \n",
            "    changes_by_month[change.date.month].append(change) \n",
            " \n",
            "avg_daily_change = { \n",
            "    month: sum(change.pct_change for change in changes) / len(changes) \n",
            "    for month, changes in changes_by_month.items() \n",
            "} \n",
            " \n",
            "# October is the best month \n",
            "assert avg_daily_change[10] == max(avg_daily_change.values())W e’ll be doing these sorts of manipulations throughout the book, usually\n",
            "without calling too much explicit attention to them.\n",
            "R e s c a l i n g\n",
            "Many  techniques are sensitive to the scale  of your data. For example,\n",
            "imagine that you have a dataset consisting of the heights and weights of\n",
            "hundreds of data scientists, and that you are trying to identify clusters  of\n",
            "body sizes.\n",
            "Intuitively , we’d like clusters to represent points near each other , which\n",
            "means that we need some notion of distance between points. W e already\n",
            "have a Euclidean distance  function, so a natural approach might be to treat\n",
            "(height, weight) pairs as points in two-dimensional space. Consider the\n",
            "people listed in T able 10-1 .\n",
            "T able 10-1. Heights and weights\n",
            "Person Height (inches) Height (centimeters) W eight (pounds)\n",
            "A 63 160 150\n",
            "B 67 170.2 160\n",
            "C 70 177.8 171\n",
            "If we measure height in inches, then B’ s nearest neighbor is A:\n",
            "from scratch.linear_algebra import distance \n",
            " \n",
            "a_to_b = distance([63, 150], [67, 160])        # 10.77 \n",
            "a_to_c = distance([63, 150], [70, 171])        # 22.14 \n",
            "b_to_c = distance([67, 160], [70, 171])        # 11.40\n",
            "However , if we measure height in centimeters, then B’ s nearest neighbor is\n",
            "instead C:a_to_b = distance([160, 150], [170.2, 160])    # 14.28 \n",
            "a_to_c = distance([160, 150], [177.8, 171])    # 27.53 \n",
            "b_to_c = distance([170.2, 160], [177.8, 171])  # 13.37\n",
            "Obviously it’ s a problem if changing units can change results like this. For\n",
            "this reason, when dimensions aren’ t comparable with one another , we will\n",
            "sometimes r escale  our data so that each dimension has mean 0 and standard\n",
            "deviation 1. This ef fectively gets rid of the units, converting each dimension\n",
            "to “standard deviations from the mean.”\n",
            "T o start with, we’ll need to compute the mean  and the\n",
            "standard_deviation  for each position:\n",
            "from typing import Tuple \n",
            " \n",
            "from scratch.linear_algebra import vector_mean \n",
            "from scratch.statistics import standard_deviation \n",
            " \n",
            "def scale(data: List[Vector]) -> Tuple[Vector, Vector]: \n",
            "    \"\"\"returns the mean and standard deviation for each position\"\"\" \n",
            "    dim = len(data[0]) \n",
            " \n",
            "    means = vector_mean(data) \n",
            "    stdevs = [standard_deviation([vector[i] for vector in data]) \n",
            "              for i in range(dim)] \n",
            " \n",
            "    return means, stdevs \n",
            " \n",
            "vectors = [[-3, -1, 1], [-1, 0, 1], [1, 1, 1]] \n",
            "means, stdevs = scale(vectors) \n",
            "assert means == [-1, 0, 1] \n",
            "assert stdevs == [2, 1, 0]\n",
            "W e can then use them to create a new dataset:\n",
            "def rescale(data: List[Vector]) -> List[Vector]: \n",
            "    \"\"\" \n",
            "    Rescales the input data so that each position has \n",
            "    mean 0 and standard deviation 1. (Leaves a position \n",
            "    as is if its standard deviation is 0.) \n",
            "    \"\"\" \n",
            "    dim = len(data[0]) \n",
            "    means, stdevs = scale(data)# Make a copy of each vector \n",
            "    rescaled = [v[:] for v in data] \n",
            " \n",
            "    for v in rescaled: \n",
            "        for i in range(dim): \n",
            "            if stdevs[i] > 0: \n",
            "                v[i] = (v[i] - means[i]) / stdevs[i] \n",
            " \n",
            "    return rescaled\n",
            "Of course, let’ s write a test to conform that rescale  does what we think it\n",
            "should:\n",
            "means, stdevs = scale(rescale(vectors)) \n",
            "assert means == [0, 0, 1] \n",
            "assert stdevs == [1, 1, 0]\n",
            "As always, you need to use your judgment. If you were to take a huge\n",
            "dataset of heights and weights and filter it down to only the people with\n",
            "heights between 69.5 inches and 70.5 inches, it’ s quite likely (depending on\n",
            "the question you’re trying to answer) that the variation remaining is simply\n",
            "noise , and you might not want to put its standard deviation on equal footing\n",
            "with other dimensions’ deviations.\n",
            "A n  A s i d e :  t q d m\n",
            "Frequently  we’ll end up doing computations that take a long time. When\n",
            "you’re doing such work, you’d like to know that you’re making progress\n",
            "and how long you should expect to wait.\n",
            "One way of doing this is with the tqdm  library , which generates custom\n",
            "progress bars. W e’ll use it some throughout the rest of the book, so let’ s\n",
            "take this chance to learn how it works.\n",
            "T o start with, you’ll need to install it:\n",
            "python -m pip install tqdmThere are only a few features you need to know about. The first is that an\n",
            "iterable wrapped in tqdm.tqdm  will produce a progress bar:\n",
            "import tqdm \n",
            " \n",
            "for i in tqdm.tqdm(range(100)): \n",
            "    # do something slow \n",
            "    _ = [random.random() for _ in range(1000000)]\n",
            "which produces an output that looks like this:\n",
            " 56%|████████████████████              | 56/100 [00:08<00:06,  6.49it/s]\n",
            "In particular , it shows you what fraction of your loop is done (though it\n",
            "can’ t do this if you use a generator), how long it’ s been running, and how\n",
            "long it expects to run.\n",
            "In this case (where we are just wrapping a call to range ) you can just use\n",
            "tqdm.trange .\n",
            "Y ou can also set the description of the progress bar while it’ s running. T o do\n",
            "that, you need to capture the tqdm  iterator in a with  statement:\n",
            "from typing import List \n",
            " \n",
            "def primes_up_to(n: int) -> List[int]: \n",
            "    primes = [2] \n",
            " \n",
            "    with tqdm.trange(3, n) as t: \n",
            "        for i in t: \n",
            "            # i is prime if no smaller prime divides it \n",
            "            i_is_prime = not any(i % p == 0 for p in primes) \n",
            "            if i_is_prime: \n",
            "                primes.append(i) \n",
            " \n",
            "            t.set_description(f\"{len(primes)} primes\") \n",
            " \n",
            "    return primes \n",
            " \n",
            "my_primes = primes_up_to(100_000)This adds a description like the following, with a counter that updates as\n",
            "new primes are discovered:\n",
            "5116 primes:  50%|████████        | 49529/99997 [00:03<00:03, 15905.90it/s]\n",
            "Using tqdm  will occasionally make your code flaky—sometimes the screen\n",
            "redraws poorly , and sometimes the loop will simply hang. And if you\n",
            "accidentally wrap a tqdm  loop inside another tqdm  loop, strange things\n",
            "might happen. T ypically its benefits outweigh these downsides, though, so\n",
            "we’ll try to use it whenever we have slow-running computations.\n",
            "D i m e n s i o n a l i t y  R e d u c t i o n\n",
            "Sometimes  the “actual” (or useful) dimensions of the data might not\n",
            "correspond to the dimensions we have. For example, consider the dataset\n",
            "pictured in Figure 10-6 .Figur e 10-6. Data with the “wr ong” axes\n",
            "Most of the variation in the data seems to be along a single dimension that\n",
            "doesn’ t correspond to either the x-axis or the y-axis.\n",
            "When  this is the case, we can use a technique called principal component\n",
            "analysis  (PCA) to extract one or more dimensions that capture as much of\n",
            "the variation in the data as possible.\n",
            "N O T E\n",
            "In practice, you wouldn’ t use this technique on such a low-dimensional dataset.\n",
            "Dimensionality reduction is mostly useful when your dataset has a lar ge number of\n",
            "dimensions and you want to find a small subset that captures most of the variation.\n",
            "Unfortunately , that case is dif ficult to illustrate in a two-dimensional book format.As a first step, we’ll need to translate the data so that each dimension has\n",
            "mean 0:\n",
            "from scratch.linear_algebra import subtract \n",
            " \n",
            "def de_mean(data: List[Vector]) -> List[Vector]: \n",
            "    \"\"\"Recenters the data to have mean 0 in every dimension\"\"\" \n",
            "    mean = vector_mean(data) \n",
            "    return [subtract(vector, mean) for vector in data]\n",
            "(If we don’ t do this, our techniques are likely to identify the mean itself\n",
            "rather than the variation in the data.)\n",
            "Figure 10-7  shows the example  data after de-meaning.\n",
            "Figur e 10-7. Data after de-meaning\n",
            "Now , given a de-meaned matrix X , we can ask which is the direction that\n",
            "captures the greatest variance in the data.Specifically , given a direction d  (a vector of magnitude 1), each row x  in the\n",
            "matrix extends dot(x, d)  in the d  direction. And every nonzero vector w\n",
            "determines a direction if we rescale it to have magnitude 1:\n",
            "from scratch.linear_algebra import magnitude \n",
            " \n",
            "def direction(w: Vector) -> Vector: \n",
            "    mag = magnitude(w) \n",
            "    return [w_i / mag for w_i in w]\n",
            "Therefore, given a nonzero vector w , we can compute the variance of our\n",
            "dataset in the direction determined by w :\n",
            "from scratch.linear_algebra import dot \n",
            " \n",
            "def directional_variance(data: List[Vector], w: Vector) -> float: \n",
            "    \"\"\" \n",
            "    Returns the variance of x in the direction of w \n",
            "    \"\"\" \n",
            "    w_dir = direction(w) \n",
            "    return sum(dot(v, w_dir) ** 2 for v in data)\n",
            "W e’d like to find the direction that maximizes this variance. W e can do this\n",
            "using gradient descent, as soon as we have the gradient function:\n",
            "def directional_variance_gradient(data: List[Vector], w: Vector) -> Vector: \n",
            "    \"\"\" \n",
            "    The gradient of directional variance with respect to w \n",
            "    \"\"\" \n",
            "    w_dir = direction(w) \n",
            "    return [sum(2 * dot(v, w_dir) * v[i] for v in data) \n",
            "            for i in range(len(w))]\n",
            "And now the first principal component that we have is just the direction that\n",
            "maximizes the directional_variance  function:\n",
            "from scratch.gradient_descent import gradient_step \n",
            " \n",
            "def first_principal_component(data: List[Vector], \n",
            "                              n: int = 100, \n",
            "                              step_size: float = 0.1) -> Vector:# Start with a random guess \n",
            "    guess = [1.0 for _ in data[0]] \n",
            " \n",
            "    with tqdm.trange(n) as t: \n",
            "        for _ in t: \n",
            "            dv = directional_variance(data, guess) \n",
            "            gradient = directional_variance_gradient(data, guess) \n",
            "            guess = gradient_step(guess, gradient, step_size) \n",
            "            t.set_description(f\"dv: {dv:.3f}\") \n",
            " \n",
            "    return direction(guess)\n",
            "On the de-meaned dataset, this returns the direction [0.924, 0.383] ,\n",
            "which does appear to capture the primary axis along which our data varies\n",
            "( Figure 10-8 ).\n",
            "Figur e 10-8. First principal component\n",
            "Once we’ve found the direction that’ s the first principal component, we can\n",
            "project our data onto it to find the values of that component:from scratch.linear_algebra import scalar_multiply \n",
            " \n",
            "def project(v: Vector, w: Vector) -> Vector: \n",
            "    \"\"\"return the projection of v onto the direction w\"\"\" \n",
            "    projection_length = dot(v, w) \n",
            "    return scalar_multiply(projection_length, w)\n",
            "If we want to find further components, we first remove the projections from\n",
            "the data:\n",
            "from scratch.linear_algebra import subtract \n",
            " \n",
            "def remove_projection_from_vector(v: Vector, w: Vector) -> Vector: \n",
            "    \"\"\"projects v onto w and subtracts the result from v\"\"\" \n",
            "    return subtract(v, project(v, w)) \n",
            " \n",
            "def remove_projection(data: List[Vector], w: Vector) -> List[Vector]: \n",
            "    return [remove_projection_from_vector(v, w) for v in data]\n",
            "Because this example dataset is only two-dimensional, after we remove the\n",
            "first component, what’ s left will be ef fectively one-dimensional ( Figure 10-\n",
            "9 ).Figur e 10-9. Data after r emoving the first principal component\n",
            "At that point, we can find the next principal component by repeating the\n",
            "process on the result of remove_projection  ( Figure 10-10 ).\n",
            "On a higher -dimensional dataset, we can iteratively find as many\n",
            "components as we want:\n",
            "def pca(data: List[Vector], num_components: int) -> List[Vector]: \n",
            "    components: List[Vector] = [] \n",
            "    for _ in range(num_components): \n",
            "        component = first_principal_component(data) \n",
            "        components.append(component) \n",
            "        data = remove_projection(data, component) \n",
            " \n",
            "    return components\n",
            "W e can then transform  our data into the lower -dimensional space spanned\n",
            "by the components:def transform_vector(v: Vector, components: List[Vector]) -> Vector: \n",
            "    return [dot(v, w) for w in components] \n",
            " \n",
            "def transform(data: List[Vector], components: List[Vector]) -> List[Vector]: \n",
            "    return [transform_vector(v, components) for v in data]\n",
            "This technique is valuable for a couple of reasons. First, it can help us clean\n",
            "our data by eliminating noise dimensions and consolidating highly\n",
            "correlated dimensions.\n",
            "Figur e 10-10. First two principal components\n",
            "Second, after extracting a low-dimensional representation of our data, we\n",
            "can use a variety of techniques that don’ t work as well on high-dimensional\n",
            "data. W e’ll see examples of such techniques throughout the book.\n",
            "At the same time, while this technique can help you build better models, it\n",
            "can also make those models harder to interpret. It’ s easy to understand\n",
            "conclusions like “every extra year of experience adds an average of $10k insalary .” It’ s much harder to make sense of “every increase of 0.1 in the third\n",
            "principal component adds an average of $10k in salary .”\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "As  mentioned at the end of Chapter 9 , pandas  is probably the\n",
            "primary Python tool for cleaning, munging, manipulating, and\n",
            "working with data. All the examples we did by hand in this chapter\n",
            "could be done much more simply using pandas. Python for Data\n",
            "Analysis  (O’Reilly), by W es McKinney , is probably the best way to\n",
            "learn pandas.\n",
            "scikit-learn has  a wide variety of matrix decomposition  functions,\n",
            "including PCA.Chapter 11. Machine Learning\n",
            "I am always r eady to learn although I do not always like being taught.\n",
            "— W inston Churchill\n",
            "Many people imagine that data science is mostly machine learning and that\n",
            "data scientists mostly build and train and tweak machine learning models all\n",
            "day long. (Then again, many of those people don’ t actually know what\n",
            "machine learning is .) In fact, data science is mostly turning business\n",
            "problems into data problems and collecting data and understanding data and\n",
            "cleaning data and formatting data, after which machine learning is almost\n",
            "an afterthought. Even so, it’ s an interesting and essential afterthought that\n",
            "you pretty much have to know about in order to do data science.\n",
            "M o d e l i n g\n",
            "Before  we can talk about machine learning, we need to talk about models .\n",
            "What is a model? It’ s simply a specification of a mathematical (or\n",
            "probabilistic) relationship that exists between dif ferent variables.\n",
            "For instance, if you’re trying to raise money for your social networking site,\n",
            "you might  build a business model  (likely in a spreadsheet) that takes inputs\n",
            "like “number of users,” “ad revenue per user ,” and “number of employees”\n",
            "and outputs your annual profit for the next several years. A cookbook recipe\n",
            "entails a model that relates inputs like “number of eaters” and “hungriness”\n",
            "to quantities of ingredients needed. And if you’ve ever watched poker on\n",
            "television, you know that each player ’ s “win probability” is estimated in\n",
            "real time based on a model that takes into account the cards that have been\n",
            "revealed so far and the distribution of cards in the deck.\n",
            "The business model is probably based on simple mathematical\n",
            "relationships: profit is revenue minus expenses, revenue is units sold times\n",
            "average price, and so on. The recipe model is probably based on trial anderror—someone went in a kitchen and tried dif ferent combinations of\n",
            "ingredients until they found one they liked. And the poker model is based\n",
            "on probability theory , the rules of poker , and some reasonably innocuous\n",
            "assumptions about the random process by which cards are dealt.\n",
            "W h a t  I s  M a c h i n e  L e a r n i n g ?\n",
            "Everyone  has her own exact definition, but we’ll use machine learning  to\n",
            "refer to creating and using models that are learned fr om data . In  other\n",
            "contexts this might be called pr edictive modeling  or data mining , but we\n",
            "will stick with machine learning. T ypically , our goal will be to use existing\n",
            "data to develop models that we can use to pr edict  various outcomes for new\n",
            "data, such as:\n",
            "Whether an email message is spam or not\n",
            "Whether a credit card transaction is fraudulent\n",
            "Which advertisement a shopper is most likely to click on\n",
            "Which football team is going to win the Super Bowl\n",
            "W e’ll  look at both supervised  models (in which there is a set of data labeled\n",
            "with the correct answers to learn from) and unsupervised  models (in which\n",
            "there are no such labels). There are various other types, like semisupervised\n",
            "(in which only some of the data are labeled), online  (in which the model\n",
            "needs to continuously adjust to newly arriving data), and  r einfor cement  (in\n",
            "which, after making a series of predictions, the model gets a signal\n",
            "indicating how well it did) that we won’ t cover in this book.\n",
            "Now , in  even the simplest situation there are entire universes of models that\n",
            "might describe the relationship we’re interested in. In most cases we will\n",
            "ourselves choose a parameterized  family of models and then use data to\n",
            "learn parameters that are in some way optimal.\n",
            "For instance, we might assume that a person’ s height is (roughly) a linear\n",
            "function of his weight and then use data to learn what that linear function is.Or we might assume that a decision tree is a good way to diagnose what\n",
            "diseases our patients have and then use data to learn the “optimal” such\n",
            "tree. Throughout the rest of the book, we’ll be investigating dif ferent\n",
            "families of models that we can learn.\n",
            "But before we can do that, we need to better understand the fundamentals of\n",
            "machine learning. For the rest of the chapter , we’ll discuss some of those\n",
            "basic concepts, before we move on to the models themselves.\n",
            "O v e r f i t t i n g  a n d  U n d e r f i t t i n g\n",
            "A  common danger in machine learning is overfitting —producing a model\n",
            "that performs well on the data you train it on but generalizes poorly to any\n",
            "new data. This could involve learning noise  in the data. Or it could involve\n",
            "learning to identify specific inputs rather than whatever factors are actually\n",
            "predictive for the desired output.\n",
            "The other side of this is underfitting —producing a model that doesn’ t\n",
            "perform well even on the training data, although typically when this\n",
            "happens you decide your model isn’ t good enough and keep looking for a\n",
            "better one.\n",
            "In Figure 1 1-1 , I’ve fit three polynomials to a sample of data. (Don’ t worry\n",
            "about how; we’ll get to that in later chapters.)Figur e 1 1-1. Overfitting and underfitting\n",
            "The horizontal line shows the best fit degree 0 (i.e., constant) polynomial. It\n",
            "severely underfits  the training data. The best fit degree 9 (i.e., 10-\n",
            "parameter) polynomial goes through every training data point exactly , but it\n",
            "very severely overfits ; if we were to pick a few more data points, it would\n",
            "quite likely miss them by a lot. And the degree 1 line strikes a nice balance;\n",
            "it’ s pretty close to every point, and—if these data are representative—the\n",
            "line will likely be close to new data points as well.\n",
            "Clearly , models that are too complex lead to overfitting and don’ t generalize\n",
            "well beyond the data they were trained on. So how do we make sure our\n",
            "models aren’ t too complex? The most fundamental approach involves using\n",
            "dif ferent data to train the model and to test the model.\n",
            "The simplest way to do this is to split the dataset, so that (for example) two-\n",
            "thirds of it is used to train the model, after which we measure the model’ sperformance on the remaining third:\n",
            "import random \n",
            "from typing import TypeVar, List, Tuple \n",
            "X = TypeVar('X')  # generic type to represent a data point \n",
            " \n",
            "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]: \n",
            "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\" \n",
            "    data = data[:]                    # Make a shallow copy \n",
            "    random.shuffle(data)              # because shuffle modifies the list. \n",
            "    cut = int(len(data) * prob)       # Use prob to find a cutoff \n",
            "    return data[:cut], data[cut:]     # and split the shuffled list there. \n",
            " \n",
            "data = [n for n in range(1000)] \n",
            "train, test = split_data(data, 0.75) \n",
            " \n",
            "# The proportions should be correct \n",
            "assert len(train) == 750 \n",
            "assert len(test) == 250 \n",
            " \n",
            "# And the original data should be preserved (in some order) \n",
            "assert sorted(train + test) == data\n",
            "Often, we’ll have paired input variables and output variables. In that case,\n",
            "we need to make sure to put corresponding values together in either the\n",
            "training data or the test data:\n",
            "Y = TypeVar('Y')  # generic type to represent output variables \n",
            " \n",
            "def train_test_split(xs: List[X], \n",
            "                     ys: List[Y], \n",
            "                     test_pct: float) -> Tuple[List[X], List[X], List[Y], \n",
            "                                                                 List[Y]]: \n",
            "    # Generate the indices and split them \n",
            "    idxs = [i for i in range(len(xs))] \n",
            "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct) \n",
            " \n",
            "    return ([xs[i] for i in train_idxs],  # x_train \n",
            "            [xs[i] for i in test_idxs],   # x_test \n",
            "            [ys[i] for i in train_idxs],  # y_train \n",
            "            [ys[i] for i in test_idxs])   # y_test\n",
            "As always, we want to make sure our code works right:xs = [x for x in range(1000)]  # xs are 1 ... 1000 \n",
            "ys = [2 * x for x in xs]       # each y_i is twice x_i \n",
            "x_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.25) \n",
            " \n",
            "# Check that the proportions are correct \n",
            "assert len(x_train) == len(y_train) == 750 \n",
            "assert len(x_test) == len(y_test) == 250 \n",
            " \n",
            "# Check that the corresponding data points are paired correctly \n",
            "assert all(y == 2 * x for x, y in zip(x_train, y_train)) \n",
            "assert all(y == 2 * x for x, y in zip(x_test, y_test))\n",
            "After which you can do something like:\n",
            "model = SomeKindOfModel() \n",
            "x_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.33) \n",
            "model.train(x_train, y_train) \n",
            "performance = model.test(x_test, y_test)\n",
            "If the model was overfit to the training data, then it will hopefully perform\n",
            "really poorly on the (completely separate) test data. Said dif ferently , if it\n",
            "performs well on the test data, then you can be more confident that it’ s\n",
            "fitting  rather than overfitting .\n",
            "However , there are a couple of ways this can go wrong.\n",
            "The first is if there are common patterns in the test and training data that\n",
            "wouldn’ t generalize to a lar ger dataset.\n",
            "For example, imagine that your dataset consists of user activity , with one\n",
            "row per user per week. In such a case, most users will appear in both the\n",
            "training data and the test data, and certain models might learn to identify\n",
            "users rather than discover relationships involving attributes . This isn’ t a\n",
            "huge worry , although it did happen to me once.\n",
            "A bigger problem is if you use the test/train split not just to judge a model\n",
            "but also to choose  from among many models. In that case, although each\n",
            "individual model may not be overfit, “choosing a model that performs best\n",
            "on the test set” is a meta-training that makes the test set function as a\n",
            "second training set. (Of course the model that performed best on the test set\n",
            "is going to perform well on the test set.)In  such a situation, you should split the data into three parts: a training set\n",
            "for building models, a validation  set for choosing among trained models,\n",
            "and a test set for judging the final model.\n",
            "C o r r e c t n e s s\n",
            "When  I’m not doing data science, I dabble in medicine. And in my spare\n",
            "time I’ve come up with a cheap, noninvasive test that can be given to a\n",
            "newborn baby that predicts—with greater than 98% accuracy—whether the\n",
            "newborn will ever develop leukemia. My lawyer has convinced me the test\n",
            "is unpatentable, so I’ll share with you the details here: predict leukemia if\n",
            "and only if the baby is named Luke (which sounds sort of like “leukemia”).\n",
            "As  we’ll see, this test is indeed more than 98% accurate. Nonetheless, it’ s\n",
            "an incredibly stupid test, and a good illustration of why we don’ t typically\n",
            "use “accuracy” to measure how good a (binary classification) model is.\n",
            "Imagine  building a model to make a binary  judgment. Is this email spam?\n",
            "Should we hire this candidate? Is this air traveler secretly a terrorist?\n",
            "Given  a set of labeled data and such a predictive model, every data point\n",
            "lies in one of four categories:\n",
            "T rue positive\n",
            "“This message is spam, and we correctly predicted spam.”\n",
            "False positive (T ype 1 error)\n",
            "“This message is not spam, but we predicted spam.”\n",
            "False negative (T ype 2 error)\n",
            "“This message is spam, but we predicted not spam.”\n",
            "T rue negative\n",
            "“This message is not spam, and we correctly predicted not spam.”\n",
            "W e  often represent these as counts in a confusion matrix :Spam Not spam\n",
            "Predict “spam” T rue positive False positive\n",
            "Predict “not spam” False negative T rue negative\n",
            "Let’ s see how my leukemia test fits into this framework. These days\n",
            "approximately 5 babies out of 1,000 are named Luke . And the lifetime\n",
            "prevalence of leukemia is about 1.4%, or 14 out of every 1,000 people .\n",
            "If we believe these two factors are independent and apply my “Luke is for\n",
            "leukemia” test to 1 million people, we’d expect to see a confusion matrix\n",
            "like:\n",
            "Leukemia No leukemia T otal\n",
            "“Luke” 70 4,930 5,000\n",
            "Not “Luke” 13,930 981,070 995,000\n",
            "T otal 14,000 986,000 1,000,000\n",
            "W e can then use these to compute various statistics about model\n",
            "performance. For example, accuracy  is defined as the fraction of correct\n",
            "predictions:\n",
            "def accuracy(tp: int, fp: int, fn: int, tn: int) -> float: \n",
            "    correct = tp + tn \n",
            "    total = tp + fp + fn + tn \n",
            "    return correct / total \n",
            " \n",
            "assert accuracy(70, 4930, 13930, 981070) == 0.98114\n",
            "That seems like a pretty impressive number . But clearly this is not a good\n",
            "test, which means that we probably shouldn’ t put a lot of credence in raw\n",
            "accuracy .\n",
            "It’ s common  to look at the combination of pr ecision  and r ecall . Precision\n",
            "measures how accurate our positive  predictions were:def precision(tp: int, fp: int, fn: int, tn: int) -> float: \n",
            "    return tp / (tp + fp) \n",
            " \n",
            "assert precision(70, 4930, 13930, 981070) == 0.014\n",
            "And recall measures what fraction of the positives our model identified:\n",
            "def recall(tp: int, fp: int, fn: int, tn: int) -> float: \n",
            "    return tp / (tp + fn) \n",
            " \n",
            "assert recall(70, 4930, 13930, 981070) == 0.005\n",
            "These are both terrible numbers, reflecting that this is a terrible model.\n",
            "Sometimes  precision and recall are combined into the F1 scor e , which is\n",
            "defined as:\n",
            "def f1_score(tp: int, fp: int, fn: int, tn: int) -> float: \n",
            "    p = precision(tp, fp, fn, tn) \n",
            "    r = recall(tp, fp, fn, tn) \n",
            " \n",
            "    return 2 * p * r / (p + r)\n",
            "This  is the harmonic mean  of precision and recall and necessarily lies\n",
            "between them.\n",
            "Usually the choice of a model involves a tradeof f between precision and\n",
            "recall. A model that predicts “yes” when it’ s even a little bit confident will\n",
            "probably have a high recall but a low precision; a model that predicts “yes”\n",
            "only when it’ s extremely confident is likely to have a low recall and a high\n",
            "precision.\n",
            "Alternatively , you can think of this as a tradeof f between false positives and\n",
            "false negatives. Saying “yes” too often will give you lots of false positives;\n",
            "saying “no” too often will give you lots of false negatives.\n",
            "Imagine that there were 10 risk factors for leukemia, and that the more of\n",
            "them you had the more likely you were to develop leukemia. In that case\n",
            "you can imagine a continuum of tests: “predict leukemia if at least one risk\n",
            "factor ,” “predict leukemia if at least two risk factors,” and so on. As youincrease the threshold, you increase the test’ s precision (since people with\n",
            "more risk factors are more likely to develop the disease), and you decrease\n",
            "the test’ s recall (since fewer and fewer of the eventual disease-suf ferers will\n",
            "meet the threshold). In cases like this, choosing the right threshold is a\n",
            "matter of finding the right tradeof f.\n",
            "T h e  B i a s - V a r i a n c e  T r a d e o f f\n",
            "Another  way of thinking about the overfitting problem is as a tradeof f\n",
            "between bias and variance.\n",
            "Both are measures of what would happen if you were to retrain your model\n",
            "many times on dif ferent sets of training data (from the same lar ger\n",
            "population).\n",
            "For example, the degree 0 model in “Overfitting and Underfitting”  will\n",
            "make a lot of mistakes for pretty much any training set (drawn from the\n",
            "same population), which means that it has a high bias . However , any two\n",
            "randomly chosen training sets should give pretty similar models (since any\n",
            "two randomly chosen training sets should have pretty similar average\n",
            "values). So we say that it has a low variance . High bias and low variance\n",
            "typically correspond to underfitting.\n",
            "On the other hand, the degree 9 model fit the training set perfectly . It has\n",
            "very low bias but very high variance (since any two training sets would\n",
            "likely give rise to very dif ferent models). This corresponds to overfitting.\n",
            "Thinking about model problems this way can help you figure out what to do\n",
            "when your model doesn’ t work so well.\n",
            "If your model has high bias (which means it performs poorly even on your\n",
            "training data), one thing to try is adding more features. Going from the\n",
            "degree 0 model in “Overfitting and Underfitting”  to the degree 1 model was\n",
            "a big improvement.\n",
            "If your model has high variance, you can similarly r emove  features. But\n",
            "another solution is to obtain more data (if you can).In Figure 1 1-2 , we fit a degree 9 polynomial to dif ferent size samples. The\n",
            "model fit based on 10 data points is all over the place, as we saw before. If\n",
            "we instead train on 100 data points, there’ s much less overfitting. And the\n",
            "model trained from 1,000 data points looks very similar to the degree 1\n",
            "model. Holding model complexity constant, the more data you have, the\n",
            "harder it is to overfit. On the other hand, more data won’ t help with bias. If\n",
            "your model doesn’ t use enough features to capture regularities in the data,\n",
            "throwing more data at it won’ t help.\n",
            "Figur e 1 1-2. Reducing variance with mor e data\n",
            "F e a t u r e  E x t r a c t i o n  a n d  S e l e c t i o n\n",
            "As  has been mentioned, when your data doesn’ t have enough features, your\n",
            "model is likely to underfit. And when your data has too many features, it’ seasy to overfit. But what are features, and where do they come from?\n",
            "Featur es  are whatever inputs we provide to our model.\n",
            "In the simplest case, features are simply given to you. If you want to predict\n",
            "someone’ s salary based on her years of experience, then years of experience\n",
            "is the only feature you have. (Although, as we saw in “Overfitting and\n",
            "Underfitting” , you might also consider adding years of experience squared,\n",
            "cubed, and so on if that helps you build a better model.)\n",
            "Things become more interesting as your data becomes more complicated.\n",
            "Imagine trying to build a  spam filter to predict whether an email is junk or\n",
            "not. Most models won’ t know what to do with a raw email, which is just a\n",
            "collection of text. Y ou’ll have to extract features. For example:\n",
            "Does the email contain the word V iagra ?\n",
            "How many times does the letter d  appear?\n",
            "What was the domain of the sender?\n",
            "The answer to a question like the first question here is simply a yes or no,\n",
            "which we typically encode as a 1 or 0. The second is a number . And the\n",
            "third is a choice from a discrete set of options.\n",
            "Pretty much always, we’ll extract features from our data that fall into one of\n",
            "these three categories. What’ s more, the types of features we have constrain\n",
            "the types of models we can use.\n",
            "The Naive Bayes classifier we’ll build in Chapter 13  is suited to\n",
            "yes-or -no features, like the first one in the preceding list.\n",
            "Regression models, which we’ll study in Chapters 14  and 16 ,\n",
            "require numeric features (which could include dummy variables\n",
            "that are 0s and 1s).\n",
            "And decision trees, which we’ll look at in Chapter 17 , can deal\n",
            "with numeric or categorical data.Although in the spam filter example we looked for ways to create features,\n",
            "sometimes we’ll instead look for ways to remove features.\n",
            "For example, your inputs might be vectors of several hundred numbers.\n",
            "Depending on the situation, it might be appropriate to distill these down to a\n",
            "handful of important dimensions (as in “Dimensionality Reduction” ) and\n",
            "use only that small number of features. Or it might be appropriate to use a\n",
            "technique (like regularization, which we’ll look at in “Regularization” ) that\n",
            "penalizes models the more features they use.\n",
            "How do we choose features? That’ s where  a combination of experience  and\n",
            "domain expertise  comes into play . If you’ve received lots of emails, then\n",
            "you probably have a sense that the presence of certain words might be a\n",
            "good indicator of spamminess. And you might also get the sense that the\n",
            "number of d s is likely not a good indicator of spamminess. But in general\n",
            "you’ll have to try dif ferent things, which is part of the fun.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "Keep reading! The next several chapters are about dif ferent\n",
            "families of machine learning models.\n",
            "The  Coursera Machine Learning  course is the original MOOC and\n",
            "is a good place to get a deeper understanding of the basics of\n",
            "machine learning.\n",
            "The Elements of Statistical Learning , by Jerome H. Friedman,\n",
            "Robert T ibshirani, and T revor Hastie (Springer), is a somewhat\n",
            "canonical textbook that can be downloaded online for free . But be\n",
            "warned: it’ s very  mathy .Chapter 12. k-Nearest\n",
            "Neighbors\n",
            "If you want to annoy your neighbors, tell the truth about them.\n",
            "— Pietro Aretino\n",
            "Imagine  that you’re trying to predict how I’m going to vote in the next\n",
            "presidential election. If you know nothing else about me (and if you have\n",
            "the data), one sensible approach is to look at how my neighbors  are\n",
            "planning to vote. Living in Seattle, as I do, my neighbors are invariably\n",
            "planning to vote for the Democratic candidate, which suggests that\n",
            "“Democratic candidate” is a good guess for me as well.\n",
            "Now imagine you know more about me than just geography—perhaps you\n",
            "know my age, my income, how many kids I have, and so on. T o the extent\n",
            "my behavior is influenced (or characterized) by those things, looking just at\n",
            "my neighbors who are close to me among all those dimensions seems likely\n",
            "to be an even better predictor than looking at all my neighbors. This is the\n",
            "idea behind  near est neighbors classification .\n",
            "T h e  M o d e l\n",
            "Nearest neighbors  is one of the simplest predictive models there is. It makes\n",
            "no mathematical assumptions, and it doesn’ t require any sort of heavy\n",
            "machinery . The only things it requires are:\n",
            "Some notion of distance\n",
            "An assumption that points that are close to one another are similar\n",
            "Most of the techniques we’ll see in this book look at the dataset as a whole\n",
            "in order to learn patterns in the data. Nearest neighbors, on the other hand,quite consciously neglects a lot of information, since the prediction for each\n",
            "new point depends only on the handful of points closest to it.\n",
            "What’ s more, nearest neighbors is probably not going to help you\n",
            "understand the drivers of whatever phenomenon you’re looking at.\n",
            "Predicting my votes based on my neighbors’ votes doesn’ t tell you much\n",
            "about what causes me to vote the way I do, whereas some alternative model\n",
            "that predicted my vote based on (say) my income and marital status very\n",
            "well might.\n",
            "In the general situation, we have some data points and we have a\n",
            "corresponding set of labels. The labels could be True  and False , indicating\n",
            "whether each input satisfies some condition like “is spam?” or “is\n",
            "poisonous?” or “would be enjoyable to watch?” Or they could be\n",
            "categories, like movie ratings (G, PG, PG-13, R, NC-17). Or they could be\n",
            "the names of presidential candidates. Or they could be favorite\n",
            "programming languages.\n",
            "In our case, the data points will be vectors, which means that we can use the\n",
            "distance  function from Chapter 4 .\n",
            "Let’ s say we’ve picked a number k  like 3 or 5. Then, when we want to\n",
            "classify some new data point, we find the k  nearest labeled points and let\n",
            "them vote on the new output.\n",
            "T o do this, we’ll need a function that counts votes. One possibility is:\n",
            "from typing import List \n",
            "from collections import Counter \n",
            " \n",
            "def raw_majority_vote(labels: List[str]) -> str: \n",
            "    votes = Counter(labels) \n",
            "    winner, _ = votes.most_common(1)[0] \n",
            "    return winner \n",
            " \n",
            "assert raw_majority_vote(['a', 'b', 'c', 'b']) == 'b'\n",
            "But this doesn’ t do anything intelligent with ties. For example, imagine\n",
            "we’re rating movies and the five nearest movies are rated G, G, PG, PG,and R. Then G has two votes and PG also has two votes. In that case, we\n",
            "have several options:\n",
            "Pick one of the winners at random.\n",
            "W eight the votes by distance and pick the weighted winner .\n",
            "Reduce k  until we find a unique winner .\n",
            "W e’ll implement the third:\n",
            "def majority_vote(labels: List[str]) -> str: \n",
            "    \"\"\"Assumes that labels are ordered from nearest to farthest.\"\"\" \n",
            "    vote_counts = Counter(labels) \n",
            "    winner, winner_count = vote_counts.most_common(1)[0] \n",
            "    num_winners = len([count \n",
            "                       for count in vote_counts.values() \n",
            "                       if count == winner_count]) \n",
            " \n",
            "    if num_winners == 1: \n",
            "        return winner                     # unique winner, so return it \n",
            "    else: \n",
            "        return majority_vote(labels[:-1]) # try again without the farthest \n",
            " \n",
            "# Tie, so look at first 4, then 'b' \n",
            "assert majority_vote(['a', 'b', 'c', 'b', 'a']) == 'b'\n",
            "This approach is sure to work eventually , since in the worst case we go all\n",
            "the way down to just one label, at which point that one label wins.\n",
            "W ith this function it’ s easy to create a classifier:\n",
            "from typing import NamedTuple \n",
            "from scratch.linear_algebra import Vector, distance \n",
            " \n",
            "class LabeledPoint(NamedTuple): \n",
            "    point: Vector \n",
            "    label: str \n",
            " \n",
            "def knn_classify(k: int, \n",
            "                 labeled_points: List[LabeledPoint], \n",
            "                 new_point: Vector) -> str: \n",
            " \n",
            "    # Order the labeled points from nearest to farthest.by_distance = sorted(labeled_points, \n",
            "                         key=lambda lp: distance(lp.point, new_point)) \n",
            " \n",
            "    # Find the labels for the k closest \n",
            "    k_nearest_labels = [lp.label for lp in by_distance[:k]] \n",
            " \n",
            "    # and let them vote. \n",
            "    return majority_vote(k_nearest_labels)\n",
            "Let’ s take a look at how this works.\n",
            "E x a m p l e :  T h e  I r i s  D a t a s e t\n",
            "The Iris  dataset  is a staple of machine learning. It contains a bunch of\n",
            "measurements for 150 flowers representing three species of iris. For each\n",
            "flower we have its petal length, petal width, sepal length, and sepal width,\n",
            "as well as its species. Y ou can download it from\n",
            "https://ar chive.ics.uci.edu/ml/datasets/iris :\n",
            "import requests \n",
            " \n",
            "data = requests.get( \n",
            "  \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" \n",
            ") \n",
            " \n",
            "with open('iris.dat', 'w') as f: \n",
            "    f.write(data.text)\n",
            "The data is comma-separated, with fields:\n",
            "sepal_length, sepal_width, petal_length, petal_width, class\n",
            "For example, the first row looks like:\n",
            "5.1,3.5,1.4,0.2,Iris-setosa\n",
            "In this section we’ll try to build a model that can predict the class (that is,\n",
            "the species) from the first four measurements.T o start with, let’ s load and explore the data. Our nearest neighbors function\n",
            "expects a LabeledPoint , so let’ s represent our data that way:\n",
            "from typing import Dict \n",
            "import csv \n",
            "from collections import defaultdict \n",
            " \n",
            "def parse_iris_row(row: List[str]) -> LabeledPoint: \n",
            "    \"\"\" \n",
            "    sepal_length, sepal_width, petal_length, petal_width, class \n",
            "    \"\"\" \n",
            "    measurements = [float(value) for value in row[:-1]] \n",
            "    # class is e.g. \"Iris-virginica\"; we just want \"virginica\" \n",
            "    label = row[-1].split(\"-\")[-1] \n",
            " \n",
            "    return LabeledPoint(measurements, label) \n",
            " \n",
            "with open('iris.data') as f: \n",
            "    reader = csv.reader(f) \n",
            "    iris_data = [parse_iris_row(row) for row in reader] \n",
            " \n",
            "# We'll also group just the points by species/label so we can plot them \n",
            "points_by_species: Dict[str, List[Vector]] = defaultdict(list) \n",
            "for iris in iris_data: \n",
            "    points_by_species[iris.label].append(iris.point)\n",
            "W e’d like to plot the measurements so we can see how they vary by species.\n",
            "Unfortunately , they are four -dimensional, which makes them tricky to plot.\n",
            "One thing we can do is look at the scatterplots for each of the six pairs of\n",
            "measurements ( Figure 12-1 ). I won’ t explain all the details, but it’ s a nice\n",
            "illustration of more complicated things you can do with matplotlib, so it’ s\n",
            "worth studying:\n",
            "from matplotlib import pyplot as plt \n",
            "metrics = ['sepal length', 'sepal width', 'petal length', 'petal width'] \n",
            "pairs = [(i, j) for i in range(4) for j in range(4) if i < j] \n",
            "marks = ['+', '.', 'x']  # we have 3 classes, so 3 markers \n",
            " \n",
            "fig, ax = plt.subplots(2, 3) \n",
            " \n",
            "for row in range(2): \n",
            "    for col in range(3): \n",
            "        i, j = pairs[3 * row + col]ax[row][col].set_title(f\"{metrics[i]} vs {metrics[j]}\", fontsize=8) \n",
            "        ax[row][col].set_xticks([]) \n",
            "        ax[row][col].set_yticks([]) \n",
            " \n",
            "        for mark, (species, points) in zip(marks, points_by_species.items()): \n",
            "            xs = [point[i] for point in points] \n",
            "            ys = [point[j] for point in points] \n",
            "            ax[row][col].scatter(xs, ys, marker=mark, label=species) \n",
            " \n",
            "ax[-1][-1].legend(loc='lower right', prop={'size': 6}) \n",
            "plt.show()\n",
            "Figur e 12-1. Iris scatterplots\n",
            "If you look at those plots, it seems like the measurements really do cluster\n",
            "by species. For example, looking at sepal length and sepal width alone, you\n",
            "probably couldn’ t distinguish between versicolor  and vir ginica . But once\n",
            "you add petal length and width into the mix, it seems like you should be\n",
            "able to predict the species based on the nearest neighbors.T o start with, let’ s split the data into a test set and a training set:\n",
            "import random \n",
            "from scratch.machine_learning import split_data \n",
            " \n",
            "random.seed(12) \n",
            "iris_train, iris_test = split_data(iris_data, 0.70) \n",
            "assert len(iris_train) == 0.7 * 150 \n",
            "assert len(iris_test) == 0.3 * 150\n",
            "The training set will be the “neighbors” that we’ll use to classify the points\n",
            "in the test set. W e just need to choose a value for k , the number of neighbors\n",
            "who get to vote. T oo small (think k  = 1), and we let outliers have too much\n",
            "influence; too lar ge (think k  = 105), and we just predict the most common\n",
            "class in the dataset.\n",
            "In a real application (and with more data), we might create a separate\n",
            "validation set and use it to choose k . Here we’ll just use k  = 5:\n",
            "from typing import Tuple \n",
            " \n",
            "# track how many times we see (predicted, actual) \n",
            "confusion_matrix: Dict[Tuple[str, str], int] = defaultdict(int) \n",
            "num_correct = 0 \n",
            " \n",
            "for iris in iris_test: \n",
            "    predicted = knn_classify(5, iris_train, iris.point) \n",
            "    actual = iris.label \n",
            " \n",
            "    if predicted == actual: \n",
            "        num_correct += 1 \n",
            " \n",
            "    confusion_matrix[(predicted, actual)] += 1 \n",
            " \n",
            "pct_correct = num_correct / len(iris_test) \n",
            "print(pct_correct, confusion_matrix)\n",
            "On this simple dataset, the model predicts almost perfectly . There’ s one\n",
            "versicolor  for which it predicts vir ginica , but otherwise it gets things\n",
            "exactly right.T h e  C u r s e  o f  D i m e n s i o n a l i t y\n",
            "The k -nearest neighbors  algorithm runs into trouble in higher dimensions\n",
            "thanks to the “curse of dimensionality ,” which boils down to the fact that\n",
            "high-dimensional spaces are vast . Points in high-dimensional spaces tend\n",
            "not to be close to one another at all. One way to see this is by randomly\n",
            "generating pairs of points in the d -dimensional “unit cube” in a variety of\n",
            "dimensions, and calculating the distances between them.\n",
            "Generating random points should be second nature by now:\n",
            "def random_point(dim: int) -> Vector: \n",
            "    return [random.random() for _ in range(dim)]\n",
            "as is writing a function to generate the distances:\n",
            "def random_distances(dim: int, num_pairs: int) -> List[float]: \n",
            "    return [distance(random_point(dim), random_point(dim)) \n",
            "            for _ in range(num_pairs)]\n",
            "For every dimension from 1 to 100, we’ll compute 10,000 distances and use\n",
            "those to compute the average distance between points and the minimum\n",
            "distance between points in each dimension ( Figure 12-2 ):\n",
            "import tqdm \n",
            "dimensions = range(1, 101) \n",
            " \n",
            "avg_distances = [] \n",
            "min_distances = [] \n",
            " \n",
            "random.seed(0) \n",
            "for dim in tqdm.tqdm(dimensions, desc=\"Curse of Dimensionality\"): \n",
            "    distances = random_distances(dim, 10000)      # 10,000 random pairs \n",
            "    avg_distances.append(sum(distances) / 10000)  # track the average \n",
            "    min_distances.append(min(distances))          # track the minimumFigur e 12-2. The curse of dimensionality\n",
            "As the number of dimensions increases, the average distance between\n",
            "points increases. But what’ s more problematic is the ratio between the\n",
            "closest distance and the average distance ( Figure 12-3 ):\n",
            "min_avg_ratio = [min_dist / avg_dist \n",
            "                 for min_dist, avg_dist in zip(min_distances, avg_distances)]Figur e 12-3. The curse of dimensionality again\n",
            "In low-dimensional datasets, the closest points tend to be much closer than\n",
            "average. But two points are close only if they’re close in every dimension,\n",
            "and every extra dimension—even if just noise—is another opportunity for\n",
            "each point to be farther away from every other point. When you have a lot\n",
            "of dimensions, it’ s likely that the closest points aren’ t much closer than\n",
            "average, so two points being close doesn’ t mean very much (unless there’ s a\n",
            "lot of structure in your data that makes it behave as if it were much lower -\n",
            "dimensional).\n",
            "A dif ferent way of thinking about the problem involves the sparsity of\n",
            "higher -dimensional spaces.\n",
            "If you pick 50 random numbers between 0 and 1, you’ll probably get a\n",
            "pretty good sample of the unit interval ( Figure 12-4 ).Figur e 12-4. Fifty random points in one dimension\n",
            "If you pick 50 random points in the unit square, you’ll get less coverage\n",
            "( Figure 12-5 ).Figur e 12-5. Fifty random points in two dimensions\n",
            "And in three dimensions, less still ( Figure 12-6 ).\n",
            "matplotlib doesn’ t graph four dimensions well, so that’ s as far as we’ll go,\n",
            "but you can see already that there are starting to be lar ge empty spaces with\n",
            "no points near them. In more dimensions—unless you get exponentially\n",
            "more data—those lar ge empty spaces represent regions far from all the\n",
            "points you want to use in your predictions.\n",
            "So if you’re trying to use nearest neighbors in higher dimensions, it’ s\n",
            "probably a good idea to do some kind of dimensionality reduction first.Figur e 12-6. Fifty random points in thr ee dimensions\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "scikit-learn has many nearest neighbor  models.Chapter 13. Naive Bayes\n",
            "It is well for the heart to be naive and for the mind not to be.\n",
            "— Anatole France\n",
            "A  social network isn’ t much good if people can’ t network. Accordingly ,\n",
            "DataSciencester has a popular feature that allows members to send messages to\n",
            "other members. And while most members are responsible citizens who send only\n",
            "well-received “how’ s it going?” messages, a few miscreants persistently spam other\n",
            "members about get-rich schemes, no-prescription-required pharmaceuticals, and\n",
            "for -profit data science credentialing programs. Y our users have begun to complain,\n",
            "and so the VP of Messaging has asked you to use data science to figure out a way to\n",
            "filter out these spam messages.\n",
            "A  R e a l l y  D u m b  S p a m  F i l t e r\n",
            "Imagine a “universe” that consists of receiving a message chosen randomly from all\n",
            "possible messages. Let S  be the event “the message is spam” and B  be the event\n",
            "“the message contains the word bitcoin .” Bayes’ s theorem tells us that the\n",
            "probability that the message is spam conditional on containing the word bitcoin  is:\n",
            "P(S|B)=[P(B|S)P(S)]/[P(B|S)P(S)+P(B|¬S)P(¬S)]\n",
            "The numerator is the probability that a message is spam and  contains bitcoin , while\n",
            "the denominator is just the probability that a message contains bitcoin . Hence, you\n",
            "can think of this calculation as simply representing the proportion of bitcoin\n",
            "messages that are spam.\n",
            "If we have a lar ge collection of messages we know are spam, and a lar ge collection\n",
            "of messages we know are not spam, then we can easily estimate P ( B | S ) and P ( B |\n",
            "¬S ). If we further assume that any message is equally likely to be spam or not spam\n",
            "(so that P ( S ) = P ( ¬S ) = 0.5), then:\n",
            "P(S|B)=P(B|S)/[P(B|S)+P(B|¬S)]\n",
            "For example, if 50% of spam messages have the word bitcoin , but only 1% of\n",
            "nonspam messages do, then the probability that any given bitcoin -containing emailis spam is:\n",
            "0.5/(0.5+0.01)=98%\n",
            "A  M o r e  S o p h i s t i c a t e d  S p a m  F i l t e r\n",
            "Imagine now that we have a vocabulary of many words, w  ..., w . T o move this\n",
            "into the realm of probability theory , we’ll write X  for the event “a message contains\n",
            "the word w .” Also imagine that (through some unspecified-at-this-point process)\n",
            "we’ve come up with an estimate P ( X | S ) for the probability that a spam message\n",
            "contains the i th word, and a similar estimate P ( X |¬ S ) for the probability that a\n",
            "nonspam message contains the i th word.\n",
            "The key to Naive Bayes is making the (big) assumption that the presences (or\n",
            "absences) of each word are independent of one another , conditional on a message\n",
            "being spam or not. Intuitively , this assumption means that knowing whether a\n",
            "certain spam message contains the word bitcoin  gives you no information about\n",
            "whether that same message contains the word r olex . In math terms, this means that:\n",
            "P(X1=x1,...,Xn=xn|S)=P(X1=x1|S)×⋯×P(Xn=xn|S)\n",
            "This is an extreme assumption. (There’ s a reason the technique has naive  in its\n",
            "name.) Imagine that our vocabulary consists only  of the words bitcoin  and r olex ,\n",
            "and that half of all spam messages are for “earn bitcoin” and that the other half are\n",
            "for “authentic rolex.” In this case, the Naive Bayes estimate that a spam message\n",
            "contains both bitcoin  and r olex  is:\n",
            "P(X1=1,X2=1|S)=P(X1=1|S)P(X2=1|S)=.5×.5=.25\n",
            "since we’ve assumed away the knowledge that bitcoin  and r olex  actually never\n",
            "occur together . Despite the unrealisticness of this assumption, this model often\n",
            "performs well and has historically been used in actual spam filters.\n",
            "The same Bayes’ s theorem reasoning we used for our “bitcoin-only” spam filter\n",
            "tells us that we can calculate the probability a message is spam using the equation:\n",
            "P(S|X=x)=P(X=x|S)/[P(X=x|S)+P(X=x|¬S)]\n",
            "The Naive Bayes assumption allows us to compute each of the probabilities on the\n",
            "right simply by multiplying together the individual probability estimates for each1 n\n",
            "i\n",
            "i\n",
            "i\n",
            "ivocabulary word.\n",
            "In practice, you usually want to avoid multiplying lots of probabilities together , to\n",
            "prevent  a problem called underflow , in which computers don’ t deal well with\n",
            "floating-point numbers that are too close to 0. Recalling from algebra that \n",
            "log(ab)=loga+logb  and that exp(logx)=x , we usually compute p1*⋯*pn\n",
            "as the equivalent (but floating-point-friendlier):\n",
            "exp(log(p1)+⋯+log(pn))\n",
            "The only challenge left is coming up with estimates for P(Xi|S)  and P(Xi|¬S) ,\n",
            "the probabilities that a spam message (or nonspam message) contains the word wi .\n",
            "If we have a fair number of “training” messages labeled as spam and not spam, an\n",
            "obvious first try is to estimate P(Xi|S)  simply as the fraction of spam messages\n",
            "containing the word wi .\n",
            "This causes a big problem, though. Imagine that in our training set the vocabulary\n",
            "word data  only occurs in nonspam messages. Then we’d estimate P(data|S)=0 .\n",
            "The result is that our Naive Bayes classifier would always assign spam probability\n",
            "0 to any  message containing the word data , even a message like “data on free\n",
            "bitcoin and authentic rolex watches.” T o avoid this problem, we usually use some\n",
            "kind of smoothing.\n",
            "In  particular , we’ll choose a pseudocount — k —and estimate the probability of\n",
            "seeing the i th word in a spam message as:\n",
            "P(Xi|S)=(k+numberofspamscontainingwi)/(2k+numberofspams)\n",
            "W e do similarly for P(Xi|¬S) . That is, when computing the spam probabilities for\n",
            "the i th word, we assume we also saw k  additional nonspams containing the word\n",
            "and k  additional nonspams not containing the word.\n",
            "For example, if data  occurs in 0/98 spam messages, and if k  is 1, we estimate\n",
            "P (data| S ) as 1/100 = 0.01, which allows our classifier to still assign some nonzero\n",
            "spam probability to messages that contain the word data .\n",
            "I m p l e m e n t a t i o n\n",
            "Now  we have all the pieces we need to build our classifier . First, let’ s create a\n",
            "simple function to tokenize messages into distinct words. W e’ll first convert eachmessage to lowercase, then use re.findall  to extract “words” consisting of letters,\n",
            "numbers, and apostrophes. Finally , we’ll use set  to get just the distinct words:\n",
            "from typing import Set \n",
            "import re \n",
            " \n",
            "def tokenize(text: str) -> Set[str]: \n",
            "    text = text.lower()                         # Convert to lowercase, \n",
            "    all_words = re.findall(\"[a-z0-9']+\", text)  # extract the words, and \n",
            "    return set(all_words)                       # remove duplicates. \n",
            " \n",
            "assert tokenize(\"Data Science is science\") == {\"data\", \"science\", \"is\"}\n",
            "W e’ll also define a type for our training data:\n",
            "from typing import NamedTuple \n",
            " \n",
            "class Message(NamedTuple): \n",
            "    text: str \n",
            "    is_spam: bool\n",
            "As our classifier needs to keep track of tokens, counts, and labels from the training\n",
            "data, we’ll make it a class. Following convention, we refer to nonspam emails as\n",
            "ham  emails.\n",
            "The constructor will take just one parameter , the pseudocount to use when\n",
            "computing probabilities. It also initializes an empty set of tokens, counters to track\n",
            "how often each token is seen in spam messages and ham messages, and counts of\n",
            "how many spam and ham messages it was trained on:\n",
            "from typing import List, Tuple, Dict, Iterable \n",
            "import math \n",
            "from collections import defaultdict \n",
            " \n",
            "class NaiveBayesClassifier: \n",
            "    def __init__(self, k: float = 0.5) -> None: \n",
            "        self.k = k  # smoothing factor \n",
            " \n",
            "        self.tokens: Set[str] = set() \n",
            "        self.token_spam_counts: Dict[str, int] = defaultdict(int) \n",
            "        self.token_ham_counts: Dict[str, int] = defaultdict(int) \n",
            "        self.spam_messages = self.ham_messages = 0\n",
            "Next, we’ll give it a method to train it on a bunch of messages. First, we increment\n",
            "the spam_messages  and ham_messages  counts. Then we tokenize each messagetext, and for each token we increment the token_spam_counts  or\n",
            "token_ham_counts  based on the message type:\n",
            "    def train(self, messages: Iterable[Message]) -> None: \n",
            "        for message in messages: \n",
            "            # Increment message counts \n",
            "            if message.is_spam: \n",
            "                self.spam_messages += 1 \n",
            "            else: \n",
            "                self.ham_messages += 1 \n",
            " \n",
            "            # Increment word counts \n",
            "            for token in tokenize(message.text): \n",
            "                self.tokens.add(token) \n",
            "                if message.is_spam: \n",
            "                    self.token_spam_counts[token] += 1 \n",
            "                else: \n",
            "                    self.token_ham_counts[token] += 1\n",
            "Ultimately we’ll want to predict P (spam | token). As we saw earlier , to apply\n",
            "Bayes’ s theorem we need to know P (token | spam) and P (token | ham) for each\n",
            "token in the vocabulary . So we’ll create a “private” helper function to compute\n",
            "those:\n",
            "    def _probabilities(self, token: str) -> Tuple[float, float]: \n",
            "        \"\"\"returns P(token | spam) and P(token | ham)\"\"\" \n",
            "        spam = self.token_spam_counts[token] \n",
            "        ham = self.token_ham_counts[token] \n",
            " \n",
            "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k) \n",
            "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k) \n",
            " \n",
            "        return p_token_spam, p_token_ham\n",
            "Finally , we’re ready to write our predict  method. As mentioned earlier , rather than\n",
            "multiplying together lots of small probabilities, we’ll instead sum up the log\n",
            "probabilities:\n",
            "    def predict(self, text: str) -> float: \n",
            "        text_tokens = tokenize(text) \n",
            "        log_prob_if_spam = log_prob_if_ham = 0.0 \n",
            " \n",
            "        # Iterate through each word in our vocabulary \n",
            "        for token in self.tokens: \n",
            "            prob_if_spam, prob_if_ham = self._probabilities(token) \n",
            " \n",
            "            # If *token* appears in the message,# add the log probability of seeing it \n",
            "            if token in text_tokens: \n",
            "                log_prob_if_spam += math.log(prob_if_spam) \n",
            "                log_prob_if_ham += math.log(prob_if_ham) \n",
            " \n",
            "            # Otherwise add the log probability of _not_ seeing it, \n",
            "            # which is log(1 - probability of seeing it) \n",
            "            else: \n",
            "                log_prob_if_spam += math.log(1.0 - prob_if_spam) \n",
            "                log_prob_if_ham += math.log(1.0 - prob_if_ham) \n",
            " \n",
            "        prob_if_spam = math.exp(log_prob_if_spam) \n",
            "        prob_if_ham = math.exp(log_prob_if_ham) \n",
            "        return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
            "And now we have a classifier .\n",
            "T e s t i n g  O u r  M o d e l\n",
            "Let’ s make  sure our model works by writing some unit tests for it.\n",
            "messages = [Message(\"spam rules\", is_spam=True), \n",
            "            Message(\"ham rules\", is_spam=False), \n",
            "            Message(\"hello ham\", is_spam=False)] \n",
            " \n",
            "model = NaiveBayesClassifier(k=0.5) \n",
            "model.train(messages)\n",
            "First, let’ s check that it got the counts right:\n",
            "assert model.tokens == {\"spam\", \"ham\", \"rules\", \"hello\"} \n",
            "assert model.spam_messages == 1 \n",
            "assert model.ham_messages == 2 \n",
            "assert model.token_spam_counts == {\"spam\": 1, \"rules\": 1} \n",
            "assert model.token_ham_counts == {\"ham\": 2, \"rules\": 1, \"hello\": 1}\n",
            "Now let’ s make a prediction. W e’ll also (laboriously) go through our Naive Bayes\n",
            "logic by hand, and make sure that we get the same result:\n",
            "text = \"hello spam\" \n",
            " \n",
            "probs_if_spam = [ \n",
            "    (1 + 0.5) / (1 + 2 * 0.5),      # \"spam\"  (present) \n",
            "    1 - (0 + 0.5) / (1 + 2 * 0.5),  # \"ham\"   (not present) \n",
            "    1 - (1 + 0.5) / (1 + 2 * 0.5),  # \"rules\" (not present) \n",
            "    (0 + 0.5) / (1 + 2 * 0.5)       # \"hello\" (present) \n",
            "]]\n",
            " \n",
            "probs_if_ham = [ \n",
            "    (0 + 0.5) / (2 + 2 * 0.5),      # \"spam\"  (present) \n",
            "    1 - (2 + 0.5) / (2 + 2 * 0.5),  # \"ham\"   (not present) \n",
            "    1 - (1 + 0.5) / (2 + 2 * 0.5),  # \"rules\" (not present) \n",
            "    (1 + 0.5) / (2 + 2 * 0.5),      # \"hello\" (present) \n",
            "] \n",
            " \n",
            "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam)) \n",
            "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham)) \n",
            " \n",
            "# Should be about 0.83 \n",
            "assert model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)\n",
            "This test passes, so it seems like our model is doing what we think it is. If you look\n",
            "at the actual probabilities, the two big drivers are that our message contains spam\n",
            "(which our lone training spam message did) and that it doesn’ t contain ham  (which\n",
            "both our training ham messages did).\n",
            "Now let’ s try it on some real data.\n",
            "U s i n g  O u r  M o d e l\n",
            "A  popular (if somewhat old) dataset is the SpamAssassin public corpus . W e’ll look\n",
            "at the files prefixed with 20021010 .\n",
            "Here is a script that will download and unpack them to the directory of your choice\n",
            "(or you can do it manually):\n",
            "from io import BytesIO  # So we can treat bytes as a file. \n",
            "import requests         # To download the files, which \n",
            "import tarfile          # are in .tar.bz format. \n",
            " \n",
            "BASE_URL = \"https://spamassassin.apache.org/old/publiccorpus\" \n",
            "FILES = [\"20021010_easy_ham.tar.bz2\", \n",
            "         \"20021010_hard_ham.tar.bz2\", \n",
            "         \"20021010_spam.tar.bz2\"] \n",
            " \n",
            "# This is where the data will end up, \n",
            "# in /spam, /easy_ham, and /hard_ham subdirectories. \n",
            "# Change this to where you want the data. \n",
            "OUTPUT_DIR = 'spam_data' \n",
            " \n",
            "for filename in FILES: \n",
            "    # Use requests to get the file contents at each URL. \n",
            "    content = requests.get(f\"{BASE_URL}/{filename}\").content# Wrap the in-memory bytes so we can use them as a \"file.\" \n",
            "    fin = BytesIO(content) \n",
            " \n",
            "    # And extract all the files to the specified output dir. \n",
            "    with tarfile.open(fileobj=fin, mode='r:bz2') as tf: \n",
            "        tf.extractall(OUTPUT_DIR)\n",
            "It’ s possible the location of the files will change (this happened between the first\n",
            "and second editions of this book), in which case adjust the script accordingly .\n",
            "After downloading the data you should have three folders: spam , easy_ham , and\n",
            "har d_ham . Each folder contains many emails, each contained in a single file. T o\n",
            "keep things r eally  simple, we’ll just look at the subject lines of each email.\n",
            "How do we identify the subject line? When we look through the files, they all seem\n",
            "to start with “Subject:”. So we’ll look for that:\n",
            "import glob, re \n",
            " \n",
            "# modify the path to wherever you've put the files \n",
            "path = 'spam_data/*/*' \n",
            " \n",
            "data: List[Message] = [] \n",
            " \n",
            "# glob.glob returns every filename that matches the wildcarded path \n",
            "for filename in glob.glob(path): \n",
            "    is_spam = \"ham\" not in filename \n",
            " \n",
            "    # There are some garbage characters in the emails; the errors='ignore' \n",
            "    # skips them instead of raising an exception. \n",
            "    with open(filename, errors='ignore') as email_file: \n",
            "        for line in email_file: \n",
            "            if line.startswith(\"Subject:\"): \n",
            "                subject = line.lstrip(\"Subject: \") \n",
            "                data.append(Message(subject, is_spam)) \n",
            "                break  # done with this file\n",
            "Now we can split the data into training data and test data, and then we’re ready to\n",
            "build a classifier:\n",
            "import random \n",
            "from scratch.machine_learning import split_data \n",
            " \n",
            "random.seed(0)      # just so you get the same answers as me \n",
            "train_messages, test_messages = split_data(data, 0.75) \n",
            " \n",
            "model = NaiveBayesClassifier() \n",
            "model.train(train_messages)Let’ s generate some predictions and check how our model does:\n",
            "from collections import Counter \n",
            " \n",
            "predictions = [(message, model.predict(message.text)) \n",
            "               for message in test_messages] \n",
            " \n",
            "# Assume that spam_probability > 0.5 corresponds to spam prediction \n",
            "# and count the combinations of (actual is_spam, predicted is_spam) \n",
            "confusion_matrix = Counter((message.is_spam, spam_probability > 0.5) \n",
            "                           for message, spam_probability in predictions) \n",
            " \n",
            "print(confusion_matrix)\n",
            "This gives 84 true positives (spam classified as “spam”), 25 false positives (ham\n",
            "classified as “spam”), 703 true negatives (ham classified as “ham”), and 44 false\n",
            "negatives (spam classified as “ham”). This means our precision is 84 / (84 + 25) =\n",
            "77%, and our recall is 84 / (84 + 44) = 65%, which are not bad numbers for such a\n",
            "simple model. (Presumably we’d do better if we looked at more than the subject\n",
            "lines.)\n",
            "W e can also inspect the model’ s innards to see which words are least and most\n",
            "indicative of spam:\n",
            "def p_spam_given_token(token: str, model: NaiveBayesClassifier) -> float: \n",
            "    # We probably shouldn't call private methods, but it's for a good cause. \n",
            "    prob_if_spam, prob_if_ham = model._probabilities(token) \n",
            " \n",
            "    return prob_if_spam / (prob_if_spam + prob_if_ham) \n",
            " \n",
            "words = sorted(model.tokens, key=lambda t: p_spam_given_token(t, model)) \n",
            " \n",
            "print(\"spammiest_words\", words[-10:]) \n",
            "print(\"hammiest_words\", words[:10])\n",
            "The spammiest words include things like sale , mortgage , money , and rates , whereas\n",
            "the hammiest words include things like spambayes , users , apt , and perl . So that also\n",
            "gives us some intuitive confidence that our model is basically doing the right thing.\n",
            "How could we get better performance? One obvious way would be to get more data\n",
            "to train on. There are a number of ways to improve the model as well. Here are\n",
            "some possibilities that you might try:\n",
            "Look at the message content, not just the subject line. Y ou’ll have to be\n",
            "careful how you deal with the message headers.Our classifier takes into account every word that appears in the training set,\n",
            "even words that appear only once. Modify the classifier to accept an\n",
            "optional min_count  threshold and ignore tokens that don’ t appear at least\n",
            "that many times.\n",
            "The tokenizer has no notion of similar words (e.g., cheap  and cheapest ).\n",
            "Modify the classifier to take an optional stemmer  function that converts\n",
            "words  to equivalence classes  of words. For example, a really simple\n",
            "stemmer function might be:\n",
            "def drop_final_s(word): \n",
            "    return re.sub(\"s$\", \"\", word)\n",
            "Creating  a good stemmer function is hard. People frequently use the Porter\n",
            "stemmer .\n",
            "Although our features are all of the form “message contains word wi ,”\n",
            "there’ s no reason why this has to be the case. In our implementation, we\n",
            "could add extra features like “message contains a number” by creating\n",
            "phony tokens like contains:number  and modifying the tokenizer  to emit\n",
            "them when appropriate.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "Paul Graham’ s  articles “A Plan for Spam”  and “Better Bayesian Filtering”\n",
            "are interesting and give more insight into the ideas behind building spam\n",
            "filters.\n",
            "scikit-learn  contains a BernoulliNB  model  that implements the same\n",
            "Naive Bayes algorithm we implemented here, as well as other variations on\n",
            "the model.Chapter 14. Simple Linear\n",
            "Regression\n",
            "Art, like morality , consists in drawing the line somewher e.\n",
            "— G. K. Chesterton\n",
            "In Chapter 5 , we  used the correlation  function to measure the strength of\n",
            "the linear relationship between two variables. For most applications,\n",
            "knowing that such a linear relationship exists isn’ t enough. W e’ll want to\n",
            "understand the nature of the relationship. This is where we’ll use simple\n",
            "linear regression.\n",
            "T h e  M o d e l\n",
            "Recall  that we were investigating the relationship between a\n",
            "DataSciencester user ’ s number of friends and the amount of time the user\n",
            "spends on the site each day . Let’ s assume that you’ve convinced yourself\n",
            "that having more friends causes  people to spend more time on the site,\n",
            "rather than one of the alternative explanations we discussed.\n",
            "The VP of Engagement asks you to build a model describing this\n",
            "relationship. Since you found a pretty strong linear relationship, a natural\n",
            "place to start is a linear model.\n",
            "In particular , you hypothesize that there are constants α  (alpha) and β  (beta)\n",
            "such that:\n",
            "yi=βxi+α+εi\n",
            "where yi  is the number of minutes user i  spends on the site daily , xi  is the\n",
            "number of friends user i  has, and ε  is a (hopefully small) error term\n",
            "representing the fact that there are other factors not accounted for by this\n",
            "simple model.Assuming we’ve determined such an alpha  and beta , then we make\n",
            "predictions simply with:\n",
            "def predict(alpha: float, beta: float, x_i: float) -> float: \n",
            "    return beta * x_i + alpha\n",
            "How do we choose alpha  and beta ? W ell, any choice of alpha  and beta\n",
            "gives us a predicted output for each input x_i . Since we know the actual\n",
            "output y_i , we can compute the error for each pair:\n",
            "def error(alpha: float, beta: float, x_i: float, y_i: float) -> float: \n",
            "    \"\"\" \n",
            "    The error from predicting beta * x_i + alpha \n",
            "    when the actual value is y_i \n",
            "    \"\"\" \n",
            "    return predict(alpha, beta, x_i) - y_i\n",
            "What we’d really like to know is the total error over the entire dataset. But\n",
            "we don’ t want to just add the errors—if the prediction for x_1  is too high\n",
            "and the prediction for x_2  is too low , the errors may just cancel out.\n",
            "So instead we add up the squar ed  errors:\n",
            "from scratch.linear_algebra import Vector \n",
            " \n",
            "def sum_of_sqerrors(alpha: float, beta: float, x: Vector, y: Vector) -> float: \n",
            "    return sum(error(alpha, beta, x_i, y_i) ** 2 \n",
            "               for x_i, y_i in zip(x, y))\n",
            "The least squar es solution  is  to choose the alpha  and beta  that make\n",
            "sum_of_sqerrors  as small as possible.\n",
            "Using calculus (or tedious algebra), the error -minimizing alpha  and beta\n",
            "are given by:\n",
            "from typing import Tuple \n",
            "from scratch.linear_algebra import Vector \n",
            "from scratch.statistics import correlation, standard_deviation, mean \n",
            " \n",
            "def least_squares_fit(x: Vector, y: Vector) -> Tuple[float, float]:\"\"\" \n",
            "    Given two vectors x and y, \n",
            "    find the least-squares values of alpha and beta \n",
            "    \"\"\" \n",
            "    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x) \n",
            "    alpha = mean(y) - beta * mean(x) \n",
            "    return alpha, beta\n",
            "W ithout going through the exact mathematics, let’ s think about why this\n",
            "might be a reasonable solution. The choice of alpha  simply says that when\n",
            "we see the average value of the independent variable x , we predict the\n",
            "average value of the dependent variable y .\n",
            "The choice of beta  means that when the input value increases by\n",
            "standard_deviation(x) , the prediction then increases by\n",
            "correlation(x, y) * standard_deviation(y) . In the case where x  and\n",
            "y  are perfectly correlated, a one-standard-deviation increase in x  results in a\n",
            "one-standard-deviation-of- y  increase in the prediction. When they’re\n",
            "perfectly anticorrelated, the increase in x  results in a decr ease  in the\n",
            "prediction. And when the correlation is 0, beta  is 0, which means that\n",
            "changes in x  don’ t af fect the prediction at all.\n",
            "As usual, let’ s write a quick test for this:\n",
            "x = [i for i in range(-100, 110, 10)] \n",
            "y = [3 * i - 5 for i in x] \n",
            " \n",
            "# Should find that y = 3x - 5 \n",
            "assert least_squares_fit(x, y) == (-5, 3)\n",
            "Now it’ s easy to apply this to the outlierless data from Chapter 5 :\n",
            "from scratch.statistics import num_friends_good, daily_minutes_good \n",
            " \n",
            "alpha, beta = least_squares_fit(num_friends_good, daily_minutes_good) \n",
            "assert 22.9 < alpha < 23.0 \n",
            "assert 0.9 < beta < 0.905This gives values of alpha  = 22.95 and beta  = 0.903. So our model says\n",
            "that we expect a user with n  friends to spend 22.95 + n  * 0.903 minutes on\n",
            "the site each day . That is, we predict that a user with no friends on\n",
            "DataSciencester would still spend about 23 minutes a day on the site. And\n",
            "for each additional friend, we expect a user to spend almost a minute more\n",
            "on the site each day .\n",
            "In Figure 14-1 , we plot the prediction line to get a sense of how well the\n",
            "model fits the observed data.\n",
            "Figur e 14-1. Our simple linear model\n",
            "Of course, we need a better way to figure out how well we’ve fit the data\n",
            "than staring at the graph. A  common measure is the coefficient of\n",
            "determination  (or R-squar ed ), which measures the fraction of the total\n",
            "variation in the dependent variable that is captured by the model:from scratch.statistics import de_mean \n",
            " \n",
            "def total_sum_of_squares(y: Vector) -> float: \n",
            "    \"\"\"the total squared variation of y_i's from their mean\"\"\" \n",
            "    return sum(v ** 2 for v in de_mean(y)) \n",
            " \n",
            "def r_squared(alpha: float, beta: float, x: Vector, y: Vector) -> float: \n",
            "    \"\"\" \n",
            "    the fraction of variation in y captured by the model, which equals \n",
            "    1 - the fraction of variation in y not captured by the model \n",
            "    \"\"\" \n",
            "    return 1.0 - (sum_of_sqerrors(alpha, beta, x, y) / \n",
            "                  total_sum_of_squares(y)) \n",
            " \n",
            "rsq = r_squared(alpha, beta, num_friends_good, daily_minutes_good) \n",
            "assert 0.328 < rsq < 0.330\n",
            "Recall that we chose the alpha  and beta  that minimized the sum of the\n",
            "squared prediction errors. A linear model we could have chosen is “always\n",
            "predict mean(y) ” (corresponding to alpha  = mean(y) and beta  = 0), whose\n",
            "sum of squared errors exactly equals its total sum of squares. This means an\n",
            "R-squared of 0, which indicates a model that (obviously , in this case)\n",
            "performs no better than just predicting the mean.\n",
            "Clearly , the least squares model must be at least as good as that one, which\n",
            "means that the sum of the squared errors is at most  the total sum of squares,\n",
            "which means that the R-squared must be at least 0. And the sum of squared\n",
            "errors must be at least 0, which means that the R-squared can be at most 1.\n",
            "The higher the number , the better our model fits the data. Here we calculate\n",
            "an R-squared of 0.329, which tells us that our model is only sort of okay at\n",
            "fitting the data, and that clearly there are other factors at play .\n",
            "U s i n g  G r a d i e n t  D e s c e n t\n",
            "If  we write theta = [alpha, beta] , we can also solve this using gradient\n",
            "descent:\n",
            "import random \n",
            "import tqdmfrom scratch.gradient_descent import gradient_step \n",
            " \n",
            "num_epochs = 10000 \n",
            "random.seed(0) \n",
            " \n",
            "guess = [random.random(), random.random()]  # choose random value to start \n",
            " \n",
            "learning_rate = 0.00001 \n",
            " \n",
            "with tqdm.trange(num_epochs) as t: \n",
            "    for _ in t: \n",
            "        alpha, beta = guess \n",
            " \n",
            "        # Partial derivative of loss with respect to alpha \n",
            "        grad_a = sum(2 * error(alpha, beta, x_i, y_i) \n",
            "                     for x_i, y_i in zip(num_friends_good, \n",
            "                                         daily_minutes_good)) \n",
            " \n",
            "        # Partial derivative of loss with respect to beta \n",
            "        grad_b = sum(2 * error(alpha, beta, x_i, y_i) * x_i \n",
            "                     for x_i, y_i in zip(num_friends_good, \n",
            "                                         daily_minutes_good)) \n",
            " \n",
            "        # Compute loss to stick in the tqdm description \n",
            "        loss = sum_of_sqerrors(alpha, beta, \n",
            "                               num_friends_good, daily_minutes_good) \n",
            "        t.set_description(f\"loss: {loss:.3f}\") \n",
            " \n",
            "        # Finally, update the guess \n",
            "        guess = gradient_step(guess, [grad_a, grad_b], -learning_rate) \n",
            " \n",
            "# We should get pretty much the same results: \n",
            "alpha, beta = guess \n",
            "assert 22.9 < alpha < 23.0 \n",
            "assert 0.9 < beta < 0.905\n",
            "If you run this you’ll get the same values for alpha  and beta  as we did\n",
            "using the exact formula.\n",
            "M a x i m u m  L i k e l i h o o d  E s t i m a t i o n\n",
            "Why  choose least squares? One justification involves maximum likelihood\n",
            "estimation . Imagine that we have a sample of data v1,...,vn  that comesfrom a distribution that depends on some unknown parameter θ  (theta):\n",
            "p(v1,...,vn|θ)\n",
            "If we didn’ t know θ , we could turn around and think of this quantity as the\n",
            "likelihood  of θ  given the sample:\n",
            "L(θ|v1,...,vn)\n",
            "Under this approach, the most likely θ  is the value that maximizes this\n",
            "likelihood function—that is, the value that makes the observed data the\n",
            "most probable. In the case of a continuous distribution, in which we have a\n",
            "probability distribution function rather than a probability mass function, we\n",
            "can do the same thing.\n",
            "Back to regression. One assumption that’ s often made about the simple\n",
            "regression model is that the regression errors are normally distributed with\n",
            "mean 0 and some (known) standard deviation σ . If that’ s the case, then the\n",
            "likelihood based on seeing a pair (x_i, y_i)  is:\n",
            "L(α,β|xi,yi,σ)= exp(−(yi−α−βxi)2/2σ2)\n",
            "The likelihood based on the entire dataset is the product of the individual\n",
            "likelihoods, which is lar gest precisely when alpha  and beta  are chosen to\n",
            "minimize the sum of squared errors. That is, in this case (with these\n",
            "assumptions), minimizing the sum of squared errors is equivalent to\n",
            "maximizing the likelihood of the observed data.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "Continue reading about multiple regression in Chapter 15 !1\n",
            "√2πσChapter 15. Multiple Regression\n",
            "I don’ t look at a pr oblem and put variables in ther e that don’ t affect it.\n",
            "— Bill Parcells\n",
            "Although  the VP is pretty impressed with your predictive model, she thinks\n",
            "you can do better . T o that end, you’ve collected additional data: you know\n",
            "how many hours each of your users works each day , and whether they have\n",
            "a PhD. Y ou’d like to use this additional data to improve your model.\n",
            "Accordingly , you hypothesize a linear model with more independent\n",
            "variables:\n",
            "minutes=α+β1friends+β2workhours+β3phd+ε\n",
            "Obviously , whether a user has a PhD is not a number—but, as we\n",
            "mentioned in Chapter 1 1 , we can introduce a dummy variable  that equals 1\n",
            "for users with PhDs and 0 for users without, after which it’ s just as numeric\n",
            "as the other variables.\n",
            "T h e  M o d e l\n",
            "Recall  that in Chapter 14  we fit a model of the form:\n",
            "yi=α+βxi+εi\n",
            "Now imagine that each input xi  is not a single number but rather a vector of\n",
            "k  numbers, xi1,...,xik . The multiple regression model assumes that:\n",
            "yi=α+β1xi1+...+βkxik+εi\n",
            "In multiple regression the vector of parameters is usually called β . W e’ll\n",
            "want this to include the constant term as well, which we can achieve by\n",
            "adding a column of 1s to our data:beta = [alpha, beta_1, ..., beta_k]\n",
            "and:\n",
            "x_i = [1, x_i1, ..., x_ik]\n",
            "Then our model is just:\n",
            "from scratch.linear_algebra import dot, Vector \n",
            " \n",
            "def predict(x: Vector, beta: Vector) -> float: \n",
            "    \"\"\"assumes that the first element of x is 1\"\"\" \n",
            "    return dot(x, beta)\n",
            "In this particular case, our independent variable x  will be a list of vectors,\n",
            "each of which looks like this:\n",
            "[1,    # constant term \n",
            " 49,   # number of friends \n",
            " 4,    # work hours per day \n",
            " 0]    # doesn't have PhD\n",
            "F u r t h e r  A s s u m p t i o n s  o f  t h e  L e a s t  S q u a r e s\n",
            "M o d e l\n",
            "There  are a couple of further assumptions that are required for this model\n",
            "(and our solution) to make sense.\n",
            "The  first is that the columns of x  are linearly independent —that there’ s no\n",
            "way to write any one as a weighted sum of some of the others. If this\n",
            "assumption fails, it’ s impossible to estimate beta . T o see this in an extreme\n",
            "case, imagine we had an extra field num_acquaintances  in our data that for\n",
            "every user was exactly equal to num_friends .\n",
            "Then, starting with any beta , if we add any  amount to the num_friends\n",
            "coef ficient and subtract that same amount from the num_acquaintances\n",
            "coef ficient, the model’ s predictions will remain unchanged. This means thatthere’ s no way to find the  coef ficient for num_friends . (Usually violations\n",
            "of this assumption won’ t be so obvious.)\n",
            "The second important assumption is that the columns of x  are all\n",
            "uncorrelated with the errors ε . If this fails to be the case, our estimates of\n",
            "beta  will be systematically wrong.\n",
            "For instance, in Chapter 14 , we built a model that predicted that each\n",
            "additional friend was associated with an extra 0.90 daily minutes on the\n",
            "site.\n",
            "Imagine it’ s also the case that:\n",
            "People who work more hours spend less time on the site.\n",
            "People with more friends tend to work more hours.\n",
            "That is, imagine that the “actual” model is:\n",
            "minutes=α+β1friends+β2workhours+ε\n",
            "where β2  is negative, and that work hours and friends are positively\n",
            "correlated. In that case, when we minimize the errors of the single-variable\n",
            "model:\n",
            "minutes=α+β1friends+ε\n",
            "we will underestimate β1 .\n",
            "Think about what would happen if we made predictions using the single-\n",
            "variable model with the “actual” value of β1 . (That is, the value that arises\n",
            "from minimizing the errors of what we called the “actual” model.) The\n",
            "predictions would tend to be way too lar ge for users who work many hours\n",
            "and a little too lar ge for users who work few hours, because β2<0  and we\n",
            "“for got” to include it. Because work hours is positively correlated with\n",
            "number of friends, this means the predictions tend to be way too lar ge for\n",
            "users with many friends, and only slightly too lar ge for users with few\n",
            "friends.The result of this is that we can reduce the errors (in the single-variable\n",
            "model) by decreasing our estimate of β1 , which means that the error -\n",
            "minimizing β1  is smaller than the “actual” value. That is, in this case the\n",
            "single-variable least squares solution is biased to underestimate β1 . And, in\n",
            "general, whenever the independent variables are correlated with the errors\n",
            "like this, our least squares solution will give us a biased estimate of β1 .\n",
            "F i t t i n g  t h e  M o d e l\n",
            "As  we did in the simple linear model, we’ll choose beta  to minimize the\n",
            "sum of squared errors. Finding an exact solution is not simple to do by\n",
            "hand, which means we’ll need to use gradient descent. Again we’ll want to\n",
            "minimize the sum of the squared errors. The error function is almost\n",
            "identical to the one we used in Chapter 14 , except that instead of expecting\n",
            "parameters [alpha, beta]  it will take a vector of arbitrary length:\n",
            "from typing import List \n",
            " \n",
            "def error(x: Vector, y: float, beta: Vector) -> float: \n",
            "    return predict(x, beta) - y \n",
            " \n",
            "def squared_error(x: Vector, y: float, beta: Vector) -> float: \n",
            "    return error(x, y, beta) ** 2 \n",
            " \n",
            "x = [1, 2, 3] \n",
            "y = 30 \n",
            "beta = [4, 4, 4]  # so prediction = 4 + 8 + 12 = 24 \n",
            " \n",
            "assert error(x, y, beta) == -6 \n",
            "assert squared_error(x, y, beta) == 36\n",
            "If you know calculus, it’ s easy to compute the gradient:\n",
            "def sqerror_gradient(x: Vector, y: float, beta: Vector) -> Vector: \n",
            "    err = error(x, y, beta) \n",
            "    return [2 * err * x_i for x_i in x] \n",
            " \n",
            "assert sqerror_gradient(x, y, beta) == [-12, -24, -36]Otherwise, you’ll need to take my word for it.\n",
            "At this point, we’re ready to find the optimal beta  using gradient descent.\n",
            "Let’ s first write out a least_squares_fit  function that can work with any\n",
            "dataset:\n",
            "import random \n",
            "import tqdm \n",
            "from scratch.linear_algebra import vector_mean \n",
            "from scratch.gradient_descent import gradient_step \n",
            " \n",
            " \n",
            "def least_squares_fit(xs: List[Vector], \n",
            "                      ys: List[float], \n",
            "                      learning_rate: float = 0.001, \n",
            "                      num_steps: int = 1000, \n",
            "                      batch_size: int = 1) -> Vector: \n",
            "    \"\"\" \n",
            "    Find the beta that minimizes the sum of squared errors \n",
            "    assuming the model y = dot(x, beta). \n",
            "    \"\"\" \n",
            "    # Start with a random guess \n",
            "    guess = [random.random() for _ in xs[0]] \n",
            " \n",
            "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"): \n",
            "        for start in range(0, len(xs), batch_size): \n",
            "            batch_xs = xs[start:start+batch_size] \n",
            "            batch_ys = ys[start:start+batch_size] \n",
            " \n",
            "            gradient = vector_mean([sqerror_gradient(x, y, guess) \n",
            "                                    for x, y in zip(batch_xs, batch_ys)]) \n",
            "            guess = gradient_step(guess, gradient, -learning_rate) \n",
            " \n",
            "    return guess\n",
            "W e can then apply that to our data:\n",
            "from scratch.statistics import daily_minutes_good \n",
            "from scratch.gradient_descent import gradient_step \n",
            " \n",
            "random.seed(0) \n",
            "# I used trial and error to choose num_iters and step_size. \n",
            "# This will run for a while. \n",
            "learning_rate = 0.001beta = least_squares_fit(inputs, daily_minutes_good, learning_rate, 5000, 25) \n",
            "assert 30.50 < beta[0] < 30.70  # constant \n",
            "assert  0.96 < beta[1] <  1.00  # num friends \n",
            "assert -1.89 < beta[2] < -1.85  # work hours per day \n",
            "assert  0.91 < beta[3] <  0.94  # has PhD\n",
            "In practice, you wouldn’ t estimate a linear regression using gradient\n",
            "descent; you’d get the exact coef ficients using linear algebra techniques that\n",
            "are beyond the scope of this book. If you did so, you’d find the equation:\n",
            "minutes=30.58+0.972friends−1.87workhours+0.923phd\n",
            "which is pretty close to what we found.\n",
            "I n t e r p r e t i n g  t h e  M o d e l\n",
            "Y ou  should think of the coef ficients of the model as representing all-else-\n",
            "being-equal estimates of the impacts of each factor . All else being equal,\n",
            "each additional friend corresponds to an extra minute spent on the site each\n",
            "day . All else being equal, each additional hour in a user ’ s workday\n",
            "corresponds to about two fewer minutes spent on the site each day . All else\n",
            "being equal, having a PhD is associated with spending an extra minute on\n",
            "the site each day .\n",
            "What this doesn’ t (directly) tell us is anything about the interactions among\n",
            "the variables. It’ s possible that the ef fect of work hours is dif ferent for\n",
            "people with many friends than it is for people with few friends. This model\n",
            "doesn’ t capture that. One way to handle this case is to introduce a new\n",
            "variable that is the pr oduct  of “friends” and “work hours.” This ef fectively\n",
            "allows the “work hours” coef ficient to increase (or decrease) as the number\n",
            "of friends increases.\n",
            "Or it’ s possible that the more friends you have, the more time you spend on\n",
            "the site up to a point , after which further friends cause you to spend less\n",
            "time on the site. (Perhaps with too many friends the experience is just too\n",
            "overwhelming?) W e could try to capture this in our model by adding\n",
            "another variable that’ s the squar e  of the number of friends.Once we start adding variables, we need to worry about whether their\n",
            "coef ficients “matter .” There are no limits to the numbers of products, logs,\n",
            "squares, and higher powers we could add.\n",
            "G o o d n e s s  o f  F i t\n",
            "Again  we can look at the R-squared:\n",
            "from scratch.simple_linear_regression import total_sum_of_squares \n",
            " \n",
            "def multiple_r_squared(xs: List[Vector], ys: Vector, beta: Vector) -> float: \n",
            "    sum_of_squared_errors = sum(error(x, y, beta) ** 2 \n",
            "                                for x, y in zip(xs, ys)) \n",
            "    return 1.0 - sum_of_squared_errors / total_sum_of_squares(ys)\n",
            "which has now increased to 0.68:\n",
            "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta) < 0.68\n",
            "Keep in mind, however , that adding new variables to a regression will\n",
            "necessarily  increase the R-squared. After all, the simple regression model is\n",
            "just the special case of the multiple regression model where the coef ficients\n",
            "on “work hours” and “PhD” both equal 0. The optimal multiple regression\n",
            "model will necessarily have an error at least as small as that one.\n",
            "Because of this, in a multiple regression, we also need to look at the\n",
            "standar d err ors  of the coef ficients, which measure how certain we are about\n",
            "our estimates of each βi . The regression as a whole may fit our data very\n",
            "well, but if some of the independent variables are correlated (or irrelevant),\n",
            "their coef ficients might not mean  much.\n",
            "The typical approach to measuring these errors starts with another\n",
            "assumption—that the errors εi  are independent normal random variables\n",
            "with mean 0 and some shared (unknown) standard deviation σ . In that case,\n",
            "we (or , more likely , our statistical software) can use some linear algebra to\n",
            "find the standard error of each coef ficient. The lar ger it is, the less sure ourmodel is about that coef ficient. Unfortunately , we’re not set up to do that\n",
            "kind of linear algebra from scratch.\n",
            "D i g r e s s i o n :  T h e  B o o t s t r a p\n",
            "Imagine  that we have a sample of n  data points, generated by some\n",
            "(unknown to us) distribution:\n",
            "data = get_sample(num_points=n)\n",
            "In Chapter 5 , we wrote a function that could compute the median  of the\n",
            "sample, which we can use as an estimate of the median of the distribution\n",
            "itself.\n",
            "But how confident can we be about our estimate? If all the data points in the\n",
            "sample are very close to 100, then it seems likely that the actual median is\n",
            "close to 100. If approximately half the data points in the sample are close to\n",
            "0 and the other half are close to 200, then we can’ t be nearly as certain\n",
            "about the median.\n",
            "If we could repeatedly get new samples, we could compute the medians of\n",
            "many samples and look at the distribution of those medians. Often we can’ t.\n",
            "In that case we can bootstrap  new datasets by choosing n  data points with\n",
            "r eplacement  from our data. And then we can compute the medians of those\n",
            "synthetic datasets:\n",
            "from typing import TypeVar, Callable \n",
            " \n",
            "X = TypeVar('X')        # Generic type for data \n",
            "Stat = TypeVar('Stat')  # Generic type for \"statistic\" \n",
            " \n",
            "def bootstrap_sample(data: List[X]) -> List[X]: \n",
            "    \"\"\"randomly samples len(data) elements with replacement\"\"\" \n",
            "    return [random.choice(data) for _ in data] \n",
            " \n",
            "def bootstrap_statistic(data: List[X], \n",
            "                        stats_fn: Callable[[List[X]], Stat], \n",
            "                        num_samples: int) -> List[Stat]:\"\"\"evaluates stats_fn on num_samples bootstrap samples from data\"\"\" \n",
            "    return [stats_fn(bootstrap_sample(data)) for _ in range(num_samples)]\n",
            "For example, consider the two following datasets:\n",
            "# 101 points all very close to 100 \n",
            "close_to_100 = [99.5 + random.random() for _ in range(101)] \n",
            " \n",
            "# 101 points, 50 of them near 0, 50 of them near 200 \n",
            "far_from_100 = ([99.5 + random.random()] + \n",
            "                [random.random() for _ in range(50)] + \n",
            "                [200 + random.random() for _ in range(50)])\n",
            "If you compute the median s of the two datasets, both will be very close to\n",
            "100. However , if you look at:\n",
            "from scratch.statistics import median, standard_deviation \n",
            " \n",
            "medians_close = bootstrap_statistic(close_to_100, median, 100)\n",
            "you will mostly see numbers really close to 100. But if you look at:\n",
            "medians_far = bootstrap_statistic(far_from_100, median, 100)\n",
            "you will see a lot of numbers close to 0 and a lot of numbers close to 200.\n",
            "The standard_deviation  of the first set of medians is close to 0, while\n",
            "that of the second set of medians is close to 100:\n",
            "assert standard_deviation(medians_close) < 1 \n",
            "assert standard_deviation(medians_far) > 90\n",
            "(This extreme a case would be pretty easy to figure out by manually\n",
            "inspecting the data, but in general that won’ t be true.)\n",
            "S t a n d a r d  E r r o r s  o f  R e g r e s s i o n  C o e f f i c i e n t sW e  can take the same approach to estimating the standard errors of our\n",
            "regression coef ficients. W e repeatedly take a bootstrap_sample  of our\n",
            "data and estimate beta  based on that sample. If the coef ficient\n",
            "corresponding to one of the independent variables (say , num_friends )\n",
            "doesn’ t vary much across samples, then we can be confident that our\n",
            "estimate is relatively tight. If the coef ficient varies greatly across samples,\n",
            "then we can’ t be at all confident in our estimate.\n",
            "The only subtlety is that, before sampling, we’ll need to zip  our x  data and\n",
            "y  data to make sure that corresponding values of the independent and\n",
            "dependent variables are sampled together . This means that\n",
            "bootstrap_sample  will return a list of pairs (x_i, y_i) , which we’ll need\n",
            "to reassemble into an x_sample  and a y_sample :\n",
            "from typing import Tuple \n",
            " \n",
            "import datetime \n",
            " \n",
            "def estimate_sample_beta(pairs: List[Tuple[Vector, float]]): \n",
            "    x_sample = [x for x, _ in pairs] \n",
            "    y_sample = [y for _, y in pairs] \n",
            "    beta = least_squares_fit(x_sample, y_sample, learning_rate, 5000, 25) \n",
            "    print(\"bootstrap sample\", beta) \n",
            "    return beta \n",
            " \n",
            "random.seed(0) # so that you get the same results as me \n",
            " \n",
            "# This will take a couple of minutes! \n",
            "bootstrap_betas = bootstrap_statistic(list(zip(inputs, daily_minutes_good)), \n",
            "                                      estimate_sample_beta, \n",
            "                                      100)\n",
            "After which we can estimate the standard deviation of each coef ficient:\n",
            "bootstrap_standard_errors = [ \n",
            "    standard_deviation([beta[i] for beta in bootstrap_betas]) \n",
            "    for i in range(4)] \n",
            " \n",
            "print(bootstrap_standard_errors) \n",
            " \n",
            "# [1.272,    # constant term, actual error = 1.19#  0.103,    # num_friends,   actual error = 0.080 \n",
            "#  0.155,    # work_hours,    actual error = 0.127 \n",
            "#  1.249]    # phd,           actual error = 0.998\n",
            "(W e would likely get better estimates if we collected more than 100 samples\n",
            "and used more than 5,000 iterations to estimate each beta , but we don’ t\n",
            "have all day .)\n",
            "W e can use these to test hypotheses such as “does βi  equal 0?” Under the\n",
            "null hypothesis βi=0  (and with our other assumptions about the\n",
            "distribution of εi ), the statistic:\n",
            "tj=ˆβj/ˆσj\n",
            "which is our estimate of βj  divided by our estimate of its standard error ,\n",
            "follows  a Student’ s t-distribution  with “ n−k  degrees of freedom.”\n",
            "If we had a students_t_cdf  function, we could compute p -values for each\n",
            "least-squares coef ficient to indicate how likely we would be to observe such\n",
            "a value if the actual coef ficient were 0. Unfortunately , we don’ t have such a\n",
            "function. (Although we would if we weren’ t working from scratch.)\n",
            "However , as the degrees of freedom get lar ge, the t -distribution gets closer\n",
            "and closer to a standard normal. In a situation like this, where n  is much\n",
            "lar ger than k , we can use normal_cdf  and still feel good about ourselves:\n",
            "from scratch.probability import normal_cdf \n",
            " \n",
            "def p_value(beta_hat_j: float, sigma_hat_j: float) -> float: \n",
            "    if beta_hat_j > 0: \n",
            "        # if the coefficient is positive, we need to compute twice the \n",
            "        # probability of seeing an even *larger* value \n",
            "        return 2 * (1 - normal_cdf(beta_hat_j / sigma_hat_j)) \n",
            "    else: \n",
            "        # otherwise twice the probability of seeing a *smaller* value \n",
            "        return 2 * normal_cdf(beta_hat_j / sigma_hat_j) \n",
            " \n",
            "assert p_value(30.58, 1.27)   < 0.001  # constant term \n",
            "assert p_value(0.972, 0.103)  < 0.001  # num_friends \n",
            "assert p_value(-1.865, 0.155) < 0.001  # work_hours \n",
            "assert p_value(0.923, 1.249)  > 0.4    # phd(In a situation not  like this, we would probably be using statistical software\n",
            "that knows how to compute the t -distribution, as well as how to compute\n",
            "the exact standard errors.)\n",
            "While most of the coef ficients have very small p -values (suggesting that\n",
            "they are indeed nonzero), the coef ficient for “PhD” is not “significantly”\n",
            "dif ferent from 0, which makes it likely that the coef ficient for “PhD” is\n",
            "random rather than meaningful.\n",
            "In more elaborate regression scenarios, you sometimes want to test more\n",
            "elaborate hypotheses about the data, such as “at least one of the βj  is\n",
            "nonzero” or “ β1  equals β2  and  β3  equals β4 .” Y ou can do this with an F-\n",
            "test , but alas, that falls outside the scope of this book.\n",
            "R e g u l a r i z a t i o n\n",
            "In  practice, you’d often like to apply linear regression to datasets with lar ge\n",
            "numbers of variables. This creates a couple of extra wrinkles. First, the\n",
            "more variables you use, the more likely you are to overfit your model to the\n",
            "training set. And second, the more nonzero coef ficients you have, the harder\n",
            "it is to make sense of them. If the goal is to explain  some phenomenon, a\n",
            "sparse model with three factors might be more useful than a slightly better\n",
            "model with hundreds.\n",
            "Regularization  is an approach in which we add to the error term a penalty\n",
            "that gets lar ger as beta  gets lar ger . W e then minimize the combined error\n",
            "and penalty . The more importance we place on the penalty term, the more\n",
            "we discourage lar ge coef ficients.\n",
            "For example, in ridge r egr ession , we add a penalty proportional to the sum\n",
            "of the squares of the beta_i  (except that typically we don’ t penalize\n",
            "beta_0 , the constant term):\n",
            "# alpha is a *hyperparameter* controlling how harsh the penalty is. \n",
            "# Sometimes it's called \"lambda\" but that already means something in Python. \n",
            "def ridge_penalty(beta: Vector, alpha: float) -> float: \n",
            "    return alpha * dot(beta[1:], beta[1:])def squared_error_ridge(x: Vector, \n",
            "                        y: float, \n",
            "                        beta: Vector, \n",
            "                        alpha: float) -> float: \n",
            "    \"\"\"estimate error plus ridge penalty on beta\"\"\" \n",
            "    return error(x, y, beta) ** 2 + ridge_penalty(beta, alpha)\n",
            "W e can then plug this into gradient descent in the usual way:\n",
            "from scratch.linear_algebra import add \n",
            " \n",
            "def ridge_penalty_gradient(beta: Vector, alpha: float) -> Vector: \n",
            "    \"\"\"gradient of just the ridge penalty\"\"\" \n",
            "    return [0.] + [2 * alpha * beta_j for beta_j in beta[1:]] \n",
            " \n",
            "def sqerror_ridge_gradient(x: Vector, \n",
            "                           y: float, \n",
            "                           beta: Vector, \n",
            "                           alpha: float) -> Vector: \n",
            "    \"\"\" \n",
            "    the gradient corresponding to the ith squared error term \n",
            "    including the ridge penalty \n",
            "    \"\"\" \n",
            "    return add(sqerror_gradient(x, y, beta), \n",
            "               ridge_penalty_gradient(beta, alpha))\n",
            "And then we just need to modify the least_squares_fit  function to use\n",
            "the sqerror_ridge_gradient  instead of sqerror_gradient . (I’m not\n",
            "going to repeat the code here.)\n",
            "W ith alpha  set to 0, there’ s no penalty at all and we get the same results as\n",
            "before:\n",
            "random.seed(0) \n",
            "beta_0 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.0,  # alpha \n",
            "                                 learning_rate, 5000, 25) \n",
            "# [30.51, 0.97, -1.85, 0.91] \n",
            "assert 5 < dot(beta_0[1:], beta_0[1:]) < 6 \n",
            "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_0) < 0.69As we increase alpha , the goodness of fit gets worse, but the size of beta\n",
            "gets smaller:\n",
            "beta_0_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.1,  # alpha \n",
            "                                   learning_rate, 5000, 25) \n",
            "# [30.8, 0.95, -1.83, 0.54] \n",
            "assert 4 < dot(beta_0_1[1:], beta_0_1[1:]) < 5 \n",
            "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_0_1) < 0.69 \n",
            " \n",
            "beta_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 1,  # alpha \n",
            "                                 learning_rate, 5000, 25) \n",
            "# [30.6, 0.90, -1.68, 0.10] \n",
            "assert 3 < dot(beta_1[1:], beta_1[1:]) < 4 \n",
            "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_1) < 0.69 \n",
            " \n",
            "beta_10 = least_squares_fit_ridge(inputs, daily_minutes_good,10,  # alpha \n",
            "                                  learning_rate, 5000, 25) \n",
            "# [28.3, 0.67, -0.90, -0.01] \n",
            "assert 1 < dot(beta_10[1:], beta_10[1:]) < 2 \n",
            "assert 0.5 < multiple_r_squared(inputs, daily_minutes_good, beta_10) < 0.6\n",
            "In particular , the coef ficient on “PhD” vanishes as we increase the penalty ,\n",
            "which accords with our previous result that it wasn’ t significantly dif ferent\n",
            "from 0.\n",
            "N O T E\n",
            "Usually you’d want to rescale  your data before using this approach. After all, if you\n",
            "changed years of experience to centuries of experience, its least squares coef ficient\n",
            "would increase by a factor of 100 and suddenly get penalized much more, even though\n",
            "it’ s the same model.\n",
            "Another approach is lasso r egr ession , which uses the penalty:\n",
            "def lasso_penalty(beta, alpha): \n",
            "    return alpha * sum(abs(beta_i) for beta_i in beta[1:])\n",
            "Whereas the ridge penalty shrank the coef ficients overall, the lasso penalty\n",
            "tends to force coef ficients to be 0, which makes it good for learning sparsemodels. Unfortunately , it’ s not amenable to gradient descent, which means\n",
            "that we won’ t be able to solve it from scratch.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "Regression  has a rich and expansive theory behind it. This is\n",
            "another place where you should consider reading a textbook, or at\n",
            "least a lot of W ikipedia articles.\n",
            "scikit-learn  has a linear_model  module  that provides a\n",
            "LinearRegression  model similar to ours, as well as ridge\n",
            "regression, lasso regression, and other types of regularization.\n",
            "Statsmodels  is another  Python module that contains (among other\n",
            "things) linear regression models.Chapter 16. Logistic\n",
            "Regression\n",
            "A lot of people say ther e’ s a fine line between genius and insanity . I don’ t\n",
            "think ther e’ s a fine line, I actually think ther e’ s a yawning gulf.\n",
            "— Bill Bailey\n",
            "In Chapter 1 , we briefly  looked at the problem of trying to predict which\n",
            "DataSciencester users paid for premium accounts. Here we’ll revisit that\n",
            "problem.\n",
            "T h e  P r o b l e m\n",
            "W e  have an anonymized dataset of about 200 users, containing each user ’ s\n",
            "salary , her years of experience as a data scientist, and whether she paid for a\n",
            "premium account ( Figure 16-1 ). As is typical with categorical variables, we\n",
            "represent the dependent variable as either 0 (no premium account) or 1\n",
            "(premium account).\n",
            "As usual, our data is a list of rows [experience, salary,\n",
            "paid_account] . Let’ s turn it into the format we need:\n",
            "xs = [[1.0] + row[:2] for row in data]  # [1, experience, salary] \n",
            "ys = [row[2] for row in data]           # paid_account\n",
            "An obvious first attempt is to use linear regression and find the best model:\n",
            "paidaccount=β0+β1experience+β2salary+εFigur e 16-1. Paid and unpaid users\n",
            "And certainly , there’ s nothing preventing us from modeling the problem this\n",
            "way . The results are shown in Figure 16-2 :\n",
            "from matplotlib import pyplot as plt \n",
            "from scratch.working_with_data import rescale \n",
            "from scratch.multiple_regression import least_squares_fit, predict \n",
            "from scratch.gradient_descent import gradient_step \n",
            " \n",
            "learning_rate = 0.001 \n",
            "rescaled_xs = rescale(xs) \n",
            "beta = least_squares_fit(rescaled_xs, ys, learning_rate, 1000, 1) \n",
            "# [0.26, 0.43, -0.43] \n",
            "predictions = [predict(x_i, beta) for x_i in rescaled_xs] \n",
            " \n",
            "plt.scatter(predictions, ys) \n",
            "plt.xlabel(\"predicted\") \n",
            "plt.ylabel(\"actual\") \n",
            "plt.show()Figur e 16-2. Using linear r egr ession to pr edict pr emium accounts\n",
            "But this approach leads to a couple of immediate problems:\n",
            "W e’d like for our predicted outputs to be 0 or 1, to indicate class\n",
            "membership. It’ s fine if they’re between 0 and 1, since we can\n",
            "interpret these as probabilities—an output of 0.25 could mean 25%\n",
            "chance of being a paid member . But the outputs of the linear model\n",
            "can be huge positive numbers or even negative numbers, which it’ s\n",
            "not clear how to interpret. Indeed, here a lot of our predictions\n",
            "were negative.\n",
            "The linear regression model assumed that the errors were\n",
            "uncorrelated with the columns of x . But here, the regression\n",
            "coef ficient for experience  is 0.43, indicating that more experience\n",
            "leads to a greater likelihood of a premium account. This means that\n",
            "our model outputs very lar ge values for people with lots ofexperience. But we know that the actual values must be at most 1,\n",
            "which means that necessarily very lar ge outputs (and therefore\n",
            "very lar ge values of experience ) correspond to very lar ge\n",
            "negative values of the error term. Because this is the case, our\n",
            "estimate of beta  is biased.\n",
            "What we’d like instead is for lar ge positive values of dot(x_i, beta)  to\n",
            "correspond to probabilities close to 1, and for lar ge negative values to\n",
            "correspond to probabilities close to 0. W e can accomplish this by applying\n",
            "another function to the result.\n",
            "T h e  L o g i s t i c  F u n c t i o n\n",
            "In  the case of logistic regression, we use the logistic function , pictured in\n",
            "Figure 16-3 :\n",
            "def logistic(x: float) -> float: \n",
            "    return 1.0 / (1 + math.exp(-x))Figur e 16-3. The logistic function\n",
            "As its input gets lar ge and positive, it gets closer and closer to 1. As its\n",
            "input gets lar ge and negative, it gets closer and closer to 0. Additionally , it\n",
            "has the convenient property that its derivative is given by:\n",
            "def logistic_prime(x: float) -> float: \n",
            "    y = logistic(x) \n",
            "    return y * (1 - y)\n",
            "which we’ll make use of in a bit. W e’ll use this to fit a model:\n",
            "yi=f(xiβ)+εi\n",
            "where f  is the logistic  function.\n",
            "Recall that for linear regression we fit the model by minimizing the sum of\n",
            "squared errors, which ended up choosing the β  that maximized thelikelihood of the data.\n",
            "Here the two aren’ t equivalent, so we’ll use gradient descent to maximize\n",
            "the likelihood directly . This means we need to calculate the likelihood\n",
            "function and its gradient.\n",
            "Given some β , our model says that each yi  should equal 1 with probability \n",
            "f(xiβ)  and 0 with probability 1−f(xiβ) .\n",
            "In particular , the PDF for yi  can be written as:\n",
            "p(yi|xi,β)=f(xiβ)yi(1−f(xiβ))1−yi\n",
            "since if yi  is 0, this equals:\n",
            "1−f(xiβ)\n",
            "and if yi  is 1, it equals:\n",
            "f(xiβ)\n",
            "It turns out that it’ s actually simpler to maximize the log likelihood :\n",
            "logL(β|xi,yi)=yilogf(xiβ)+(1−yi)log(1−f(xiβ))\n",
            "Because log is a strictly increasing function, any beta  that maximizes the\n",
            "log likelihood also maximizes the likelihood, and vice versa. Because\n",
            "gradient descent minimizes things, we’ll actually work with the negative\n",
            "log likelihood, since maximizing the likelihood is the same as minimizing\n",
            "its negative:\n",
            "import math \n",
            "from scratch.linear_algebra import Vector, dot \n",
            " \n",
            "def _negative_log_likelihood(x: Vector, y: float, beta: Vector) -> float: \n",
            "    \"\"\"The negative log likelihood for one data point\"\"\" \n",
            "    if y == 1: \n",
            "        return -math.log(logistic(dot(x, beta))) \n",
            "    else: \n",
            "        return -math.log(1 - logistic(dot(x, beta)))If we assume dif ferent data points are independent from one another , the\n",
            "overall likelihood is just the product of the individual likelihoods. That\n",
            "means the overall log likelihood is the sum of the individual log\n",
            "likelihoods:\n",
            "from typing import List \n",
            " \n",
            "def negative_log_likelihood(xs: List[Vector], \n",
            "                            ys: List[float], \n",
            "                            beta: Vector) -> float: \n",
            "    return sum(_negative_log_likelihood(x, y, beta) \n",
            "               for x, y in zip(xs, ys))\n",
            "A little bit of calculus gives us the gradient:\n",
            "from scratch.linear_algebra import vector_sum \n",
            " \n",
            "def _negative_log_partial_j(x: Vector, y: float, beta: Vector, j: int) -> \n",
            "float: \n",
            "    \"\"\" \n",
            "    The jth partial derivative for one data point. \n",
            "    Here i is the index of the data point. \n",
            "    \"\"\" \n",
            "    return -(y - logistic(dot(x, beta))) * x[j] \n",
            " \n",
            "def _negative_log_gradient(x: Vector, y: float, beta: Vector) -> Vector: \n",
            "    \"\"\" \n",
            "    The gradient for one data point. \n",
            "    \"\"\" \n",
            "    return [_negative_log_partial_j(x, y, beta, j) \n",
            "            for j in range(len(beta))] \n",
            " \n",
            "def negative_log_gradient(xs: List[Vector], \n",
            "                          ys: List[float], \n",
            "                          beta: Vector) -> Vector: \n",
            "    return vector_sum([_negative_log_gradient(x, y, beta) \n",
            "                       for x, y in zip(xs, ys)])\n",
            "at which point we have all the pieces we need.\n",
            "A p p l y i n g  t h e  M o d e lW e’ll  want to split our data into a training set and a test set:\n",
            "from scratch.machine_learning import train_test_split \n",
            "import random \n",
            "import tqdm \n",
            " \n",
            "random.seed(0) \n",
            "x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33) \n",
            " \n",
            "learning_rate = 0.01 \n",
            " \n",
            "# pick a random starting point \n",
            "beta = [random.random() for _ in range(3)] \n",
            " \n",
            "with tqdm.trange(5000) as t: \n",
            "    for epoch in t: \n",
            "        gradient = negative_log_gradient(x_train, y_train, beta) \n",
            "        beta = gradient_step(beta, gradient, -learning_rate) \n",
            "        loss = negative_log_likelihood(x_train, y_train, beta) \n",
            "        t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n",
            "after which we find that beta  is approximately:\n",
            "[-2.0, 4.7, -4.5]\n",
            "These are coef ficients for the rescale d data, but we can transform them\n",
            "back to the original data as well:\n",
            "from scratch.working_with_data import scale \n",
            " \n",
            "means, stdevs = scale(xs) \n",
            "beta_unscaled = [(beta[0] \n",
            "                  - beta[1] * means[1] / stdevs[1] \n",
            "                  - beta[2] * means[2] / stdevs[2]), \n",
            "                 beta[1] / stdevs[1], \n",
            "                 beta[2] / stdevs[2]] \n",
            "# [8.9, 1.6, -0.000288]\n",
            "Unfortunately , these are not as easy to interpret as linear regression\n",
            "coef ficients. All else being equal, an extra year of experience adds 1.6 to the\n",
            "input of logistic . All else being equal, an extra $10,000 of salary\n",
            "subtracts 2.88 from the input of logistic .The impact on the output, however , depends on the other inputs as well. If\n",
            "dot(beta, x_i)  is already lar ge (corresponding to a probability close to\n",
            "1), increasing it even by a lot cannot af fect the probability very much. If it’ s\n",
            "close to 0, increasing it just a little might increase the probability quite a bit.\n",
            "What we can say is that—all else being equal—people with more\n",
            "experience are more likely to pay for accounts. And that—all else being\n",
            "equal—people with higher salaries are less likely to pay for accounts. (This\n",
            "was also somewhat apparent when we plotted the data.)\n",
            "G o o d n e s s  o f  F i t\n",
            "W e  haven’ t yet used the test data that we held out. Let’ s see what happens if\n",
            "we predict paid account  whenever the probability exceeds 0.5:\n",
            "true_positives = false_positives = true_negatives = false_negatives = 0 \n",
            " \n",
            "for x_i, y_i in zip(x_test, y_test): \n",
            "    prediction = logistic(dot(beta, x_i)) \n",
            " \n",
            "    if y_i == 1 and prediction >= 0.5:  # TP: paid and we predict paid \n",
            "        true_positives += 1 \n",
            "    elif y_i == 1:                      # FN: paid and we predict unpaid \n",
            "        false_negatives += 1 \n",
            "    elif prediction >= 0.5:             # FP: unpaid and we predict paid \n",
            "        false_positives += 1 \n",
            "    else:                               # TN: unpaid and we predict unpaid \n",
            "        true_negatives += 1 \n",
            " \n",
            "precision = true_positives / (true_positives + false_positives) \n",
            "recall = true_positives / (true_positives + false_negatives)\n",
            "This gives a precision of 75% (“when we predict paid account  we’re right\n",
            "75% of the time”) and a recall of 80% (“when a user has a paid account we\n",
            "predict paid account  80% of the time”), which is not terrible considering\n",
            "how little data we have.\n",
            "W e can also plot the predictions versus the actuals ( Figure 16-4 ), which also\n",
            "shows that the model performs well:predictions = [logistic(dot(beta, x_i)) for x_i in x_test] \n",
            "plt.scatter(predictions, y_test, marker='+') \n",
            "plt.xlabel(\"predicted probability\") \n",
            "plt.ylabel(\"actual outcome\") \n",
            "plt.title(\"Logistic Regression Predicted vs. Actual\") \n",
            "plt.show()\n",
            "Figur e 16-4. Logistic r egr ession pr edicted versus actual\n",
            "S u p p o r t  V e c t o r  M a c h i n e s\n",
            "The  set of points where dot(beta, x_i)  equals 0 is the boundary between\n",
            "our classes. W e can plot this to see exactly what our model is doing\n",
            "( Figure 16-5 ).Figur e 16-5. Paid and unpaid users with decision boundary\n",
            "This  boundary is a hyperplane  that splits the parameter space into two half-\n",
            "spaces corresponding to pr edict paid  and pr edict unpaid . W e found it as a\n",
            "side ef fect of finding the most likely logistic model.\n",
            "An alternative approach to classification is to just look for the hyperplane\n",
            "that “best” separates the classes in the training data. This is the idea behind\n",
            "the support vector machine , which finds the hyperplane that maximizes the\n",
            "distance to the nearest point in each class ( Figure 16-6 ).Figur e 16-6. A separating hyperplane\n",
            "Finding such a hyperplane is an optimization problem that involves\n",
            "techniques that are too advanced for us. A dif ferent problem is that a\n",
            "separating hyperplane might not exist at all. In our “who pays?” dataset\n",
            "there simply is no line that perfectly separates the paid users from the\n",
            "unpaid users.\n",
            "W e can sometimes get around this by transforming the data into a higher -\n",
            "dimensional space. For example, consider the simple one-dimensional\n",
            "dataset shown in Figure 16-7 .Figur e 16-7. A nonseparable one-dimensional dataset\n",
            "It’ s clear that there’ s no hyperplane that separates the positive examples\n",
            "from the negative ones. However , look at what happens when we map this\n",
            "dataset to two dimensions by sending the point x  to (x, x**2) . Suddenly\n",
            "it’ s possible to find a hyperplane that splits the data ( Figure 16-8 ).Figur e 16-8. Dataset becomes separable in higher dimensions\n",
            "This  is usually called the kernel trick  because rather than actually mapping\n",
            "the points into the higher -dimensional space (which could be expensive if\n",
            "there are a lot of points and the mapping is complicated), we can use a\n",
            "“kernel” function to compute dot products in the higher -dimensional space\n",
            "and use those to find a hyperplane.\n",
            "It’ s hard (and probably not a good idea) to use  support vector machines\n",
            "without relying on specialized optimization software written by people with\n",
            "the appropriate expertise, so we’ll have to leave our treatment here.\n",
            "F o r  F u r t h e r  I n v e s t i g a t i o n\n",
            "scikit-learn has modules  for both logistic regression  and support\n",
            "vector machines .LIBSVM  is the  support vector machine implementation that scikit-\n",
            "learn is using behind the scenes. Its website has a variety of useful\n",
            "documentation about support vector machines.Chapter 17. Decision T rees\n",
            "A tr ee is an incompr ehensible mystery .\n",
            "— Jim W oodring\n",
            "DataSciencester ’ s  VP of T alent has interviewed a number of job candidates\n",
            "from the site, with varying degrees of success. He’ s collected a dataset\n",
            "consisting of several (qualitative) attributes of each candidate, as well as\n",
            "whether that candidate interviewed well or poorly . Could you, he asks, use\n",
            "this data to build a model identifying which candidates will interview well,\n",
            "so that he doesn’ t have to waste time conducting interviews?\n",
            "This seems like a good fit for a decision tr ee , another predictive modeling\n",
            "tool in the data scientist’ s kit.\n",
            "W h a t  I s  a  D e c i s i o n  T r e e ?\n",
            "A  decision tree uses a tree structure to represent a number of possible\n",
            "decision paths  and an outcome for each path.\n",
            "If you have ever played the game T wenty Questions , then you are familiar\n",
            "with decision trees. For example:\n",
            "“I am thinking of an animal.”\n",
            "“Does it have more than five legs?”\n",
            "“No.”\n",
            "“Is it delicious?”\n",
            "“No.”\n",
            "“Does it appear on the back of the Australian five-cent coin?”\n",
            "“Y es.”“Is it an echidna?”\n",
            "“Y es, it is!”\n",
            "This corresponds to the path:\n",
            "“Not more than 5 legs” → “Not delicious” → “On the 5-cent coin” →\n",
            "“Echidna!”\n",
            "in an idiosyncratic (and not very comprehensive) “guess the animal”\n",
            "decision tree ( Figure 17-1 ).\n",
            "Figur e 17-1. A “guess the animal” decision tr ee\n",
            "Decision  trees have a lot to recommend them. They’re very easy to\n",
            "understand and interpret, and the process by which they reach a prediction\n",
            "is completely transparent. Unlike the other models we’ve looked at so far ,\n",
            "decision trees can easily handle a mix of numeric (e.g., number of legs) and\n",
            "categorical (e.g., delicious/not delicious) attributes and can even classify\n",
            "data for which attributes are missing.\n",
            "At the same time, finding an “optimal” decision tree for a set of training\n",
            "data is computationally a very hard problem. (W e will get around this bytrying to build a good-enough tree rather than an optimal one, although for\n",
            "lar ge datasets this can still be a lot of work.) More important, it is very easy\n",
            "(and very bad) to build decision trees that are overfitted  to the training data,\n",
            "and that don’ t generalize well to unseen data. W e’ll look at ways to address\n",
            "this.\n",
            "Most  people divide decision trees into classification tr ees  (which produce\n",
            "categorical outputs) and r egr ession tr ees  (which produce numeric outputs).\n",
            "In this chapter , we’ll focus on classification trees, and we’ll work through\n",
            "the ID3 algorithm for learning a decision tree from a set of labeled data,\n",
            "which should help us understand how decision trees actually work. T o make\n",
            "things simple, we’ll restrict ourselves to problems with binary outputs like\n",
            "“Should I hire this candidate?” or “Should I show this website visitor\n",
            "advertisement A or advertisement B?” or “W ill eating this food I found in\n",
            "the of fice fridge make me sick?”\n",
            "E n t r o p y\n",
            "In  order to build a decision tree, we will need to decide what questions to\n",
            "ask and in what order . At each stage of the tree there are some possibilities\n",
            "we’ve eliminated and some that we haven’ t. After learning that an animal\n",
            "doesn’ t have more than five legs, we’ve eliminated the possibility that it’ s a\n",
            "grasshopper . W e haven’ t eliminated the possibility that it’ s a duck. Each\n",
            "possible question partitions the remaining possibilities according to its\n",
            "answer .\n",
            "Ideally , we’d like to choose questions whose answers give a lot of\n",
            "information about what our tree should predict. If there’ s a single yes/no\n",
            "question for which “yes” answers always correspond to True  outputs and\n",
            "“no” answers to False  outputs (or vice versa), this would be an awesome\n",
            "question to pick. Conversely , a yes/no question for which neither answer\n",
            "gives you much new information about what the prediction should be is\n",
            "probably not a good choice.W e capture this notion of “how much information” with entr opy . Y ou have\n",
            "probably heard this term used to mean disorder . W e use it to represent the\n",
            "uncertainty associated with data.\n",
            "Imagine that we have a set S  of data, each member of which is labeled as\n",
            "belonging to one of a finite number of classes C1,...,Cn . If all the data\n",
            "points belong to a single class, then there is no real uncertainty , which\n",
            "means we’d like there to be low entropy . If the data points are evenly spread\n",
            "across the classes, there is a lot of uncertainty and we’d like there to be high\n",
            "entropy .\n",
            "In math terms, if pi  is the proportion of data labeled as class ci , we define\n",
            "the entropy as:\n",
            "H(S)=−p1log2p1−...−pnlog2pn\n",
            "with the (standard) convention that 0log0=0 .\n",
            "W ithout worrying too much about the grisly details, each term −pilog2pi\n",
            "is non-negative and is close to 0 precisely when pi  is either close to 0 or\n",
            "close to 1 ( Figure 17-2 ).Figur e 17-2. A graph of -p log p\n",
            "This means the entropy will be small when every pi  is close to 0 or 1 (i.e.,\n",
            "when most of the data is in a single class), and it will be lar ger when many\n",
            "of the pi ’ s are not close to 0 (i.e., when the data is spread across multiple\n",
            "classes). This is exactly the behavior we desire.\n",
            "It is easy enough to roll all of this into a function:\n",
            "from typing import List \n",
            "import math \n",
            " \n",
            "def entropy(class_probabilities: List[float]) -> float: \n",
            "    \"\"\"Given a list of class probabilities, compute the entropy\"\"\" \n",
            "    return sum(-p * math.log(p, 2) \n",
            "               for p in class_probabilities \n",
            "               if p > 0)                     # ignore zero probabilities \n",
            " \n",
            "assert entropy([1.0]) == 0assert entropy([0.5, 0.5]) == 1 \n",
            "assert 0.81 < entropy([0.25, 0.75]) < 0.82\n",
            "Our data will consist of pairs (input, label) , which means that we’ll\n",
            "need to compute the class probabilities ourselves. Notice that we don’ t\n",
            "actually care which label is associated with each probability , only what the\n",
            "probabilities are:\n",
            "from typing import Any \n",
            "from collections import Counter \n",
            " \n",
            "def class_probabilities(labels: List[Any]) -> List[float]: \n",
            "    total_count = len(labels) \n",
            "    return [count / total_count \n",
            "            for count in Counter(labels).values()] \n",
            " \n",
            "def data_entropy(labels: List[Any]) -> float: \n",
            "    return entropy(class_probabilities(labels)) \n",
            " \n",
            "assert data_entropy(['a']) == 0 \n",
            "assert data_entropy([True, False]) == 1 \n",
            "assert data_entropy([3, 4, 4, 4]) == entropy([0.25, 0.75])\n",
            "T h e  E n t r o p y  o f  a  P a r t i t i o n\n",
            "What  we’ve done so far is compute the entropy (think “uncertainty”) of a\n",
            "single set of labeled data. Now , each stage of a decision tree involves asking\n",
            "a question whose answer partitions data into one or (hopefully) more\n",
            "subsets. For instance, our “does it have more than five legs?” question\n",
            "partitions animals into those that have more than five legs (e.g., spiders) and\n",
            "those that don’ t (e.g., echidnas).\n",
            "Correspondingly , we’d like some notion of the entropy that results from\n",
            "partitioning a set of data in a certain way . W e want a partition to have low\n",
            "entropy if it splits the data into subsets that themselves have low entropy\n",
            "(i.e., are highly certain), and high entropy if it contains subsets that (are\n",
            "lar ge and) have high entropy (i.e., are highly uncertain).For example, my “Australian five-cent coin” question was pretty dumb\n",
            "(albeit pretty lucky!), as it partitioned the remaining animals at that point\n",
            "into S1  = {echidna} and S2  = {everything else}, where S2  is both lar ge and\n",
            "high-entropy . (S1  has no entropy , but it represents a small fraction of the\n",
            "remaining “classes.”)\n",
            "Mathematically , if we partition our data S  into subsets S1,...,Sm  containing\n",
            "proportions q1,...,qm  of the data, then we compute the entropy of the\n",
            "partition as a weighted sum:\n",
            "H=q1H(S1)+...+qmH(Sm)\n",
            "which we can implement as:\n",
            "def partition_entropy(subsets: List[List[Any]]) -> float: \n",
            "    \"\"\"Returns the entropy from this partition of data into subsets\"\"\" \n",
            "    total_count = sum(len(subset) for subset in subsets) \n",
            " \n",
            "    return sum(data_entropy(subset) * len(subset) / total_count \n",
            "               for subset in subsets)\n",
            "N O T E\n",
            "One problem with this approach is that partitioning by an attribute with many dif ferent\n",
            "values will result in a very low entropy due to overfitting. For example, imagine you\n",
            "work for a bank and are trying to build a decision tree to predict which of your\n",
            "customers are likely to default on their mortgages, using some historical data as your\n",
            "training set. Imagine further that the dataset contains each customer ’ s Social Security\n",
            "number . Partitioning on SSN will produce one-person subsets, each of which necessarily\n",
            "has zero entropy . But a model that relies on SSN is certain  not to generalize beyond the\n",
            "training set. For this reason, you should probably try to avoid (or bucket, if appropriate)\n",
            "attributes with lar ge numbers of possible values when creating decision trees.\n",
            "C r e a t i n g  a  D e c i s i o n  T r e e\n",
            "The  VP provides you with the interviewee data, consisting of (per your\n",
            "specification) a NamedTuple  of the relevant attributes for each candidate—her level, her preferred language, whether she is active on T witter , whether\n",
            "she has a PhD, and whether she interviewed well:\n",
            "from typing import NamedTuple, Optional \n",
            " \n",
            "class Candidate(NamedTuple): \n",
            "    level: str \n",
            "    lang: str \n",
            "    tweets: bool \n",
            "    phd: bool \n",
            "    did_well: Optional[bool] = None  # allow unlabeled data \n",
            " \n",
            "                  #  level     lang     tweets  phd  did_well \n",
            "inputs = [Candidate('Senior', 'Java',   False, False, False), \n",
            "          Candidate('Senior', 'Java',   False, True,  False), \n",
            "          Candidate('Mid',    'Python', False, False, True), \n",
            "          Candidate('Junior', 'Python', False, False, True), \n",
            "          Candidate('Junior', 'R',      True,  False, True), \n",
            "          Candidate('Junior', 'R',      True,  True,  False), \n",
            "          Candidate('Mid',    'R',      True,  True,  True), \n",
            "          Candidate('Senior', 'Python', False, False, False), \n",
            "          Candidate('Senior', 'R',      True,  False, True), \n",
            "          Candidate('Junior', 'Python', True,  False, True), \n",
            "          Candidate('Senior', 'Python', True,  True,  True), \n",
            "          Candidate('Mid',    'Python', False, True,  True), \n",
            "          Candidate('Mid',    'Java',   True,  False, True), \n",
            "          Candidate('Junior', 'Python', False, True,  False) \n",
            "         ]\n",
            "Our  tree will consist of decision nodes  (which ask a question and direct us\n",
            "dif ferently depending on the answer) and leaf nodes  (which give us a\n",
            "prediction). W e will build it using the  relatively simple ID3  algorithm,\n",
            "which operates in the following manner . Let’ s say we’re given some labeled\n",
            "data, and a list of attributes to consider branching on:\n",
            "If the data all have the same label, create a leaf node that predicts\n",
            "that label and then stop.\n",
            "If the list of attributes is empty (i.e., there are no more possible\n",
            "questions to ask), create a leaf node that predicts the most common\n",
            "label and then stop.Otherwise, try partitioning the data by each of the attributes.\n",
            "Choose the partition with the lowest partition entropy .\n",
            "Add a decision node based on the chosen attribute.\n",
            "Recur on each partitioned subset using the remaining attributes.\n",
            "This is what’ s known as a “greedy” algorithm because, at each step, it\n",
            "chooses the most immediately best option. Given a dataset, there may be a\n",
            "better tree with a worse-looking first move. If so, this algorithm won’ t find\n",
            "it. Nonetheless, it is relatively easy to understand and implement, which\n",
            "makes it a good place to begin exploring decision trees.\n",
            "Let’ s manually go through these steps on the interviewee dataset. The\n",
            "dataset has both True  and False  labels, and we have four attributes we can\n",
            "split on. So our first step will be to find the partition with the least entropy .\n",
            "W e’ll start by writing a function that does the partitioning:\n",
            "from typing import Dict, TypeVar \n",
            "from collections import defaultdict \n",
            " \n",
            "T = TypeVar('T')  # generic type for inputs \n",
            " \n",
            "def partition_by(inputs: List[T], attribute: str) -> Dict[Any, List[T]]: \n",
            "    \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\" \n",
            "    partitions: Dict[Any, List[T]] = defaultdict(list) \n",
            "    for input in inputs: \n",
            "        key = getattr(input, attribute)  # value of the specified attribute \n",
            "        partitions[key].append(input)    # add input to the correct partition \n",
            "    return partitions\n",
            "and one that uses it to compute entropy:\n",
            "def partition_entropy_by(inputs: List[Any], \n",
            "                         attribute: str, \n",
            "                         label_attribute: str) -> float: \n",
            "    \"\"\"Compute the entropy corresponding to the given partition\"\"\" \n",
            "    # partitions consist of our inputs \n",
            "    partitions = partition_by(inputs, attribute) \n",
            " \n",
            "    # but partition_entropy needs just the class labelslabels = [[getattr(input, label_attribute) for input in partition] \n",
            "              for partition in partitions.values()] \n",
            " \n",
            "    return partition_entropy(labels)\n",
            "Then we just need to find the minimum-entropy partition for the whole\n",
            "dataset:\n",
            "for key in ['level','lang','tweets','phd']: \n",
            "    print(key, partition_entropy_by(inputs, key, 'did_well')) \n",
            " \n",
            "assert 0.69 < partition_entropy_by(inputs, 'level', 'did_well')  < 0.70 \n",
            "assert 0.86 < partition_entropy_by(inputs, 'lang', 'did_well')   < 0.87 \n",
            "assert 0.78 < partition_entropy_by(inputs, 'tweets', 'did_well') < 0.79 \n",
            "assert 0.89 < partition_entropy_by(inputs, 'phd', 'did_well')    < 0.90\n",
            "The lowest entropy comes from splitting on level , so we’ll need to make a\n",
            "subtree for each possible level  value. Every Mid  candidate is labeled True ,\n",
            "which means that the Mid  subtree is simply a leaf node predicting True . For\n",
            "Senior  candidates, we have a mix of True s and False s, so we need to split\n",
            "again:\n",
            "senior_inputs = [input for input in inputs if input.level == 'Senior'] \n",
            " \n",
            "assert 0.4 == partition_entropy_by(senior_inputs, 'lang', 'did_well') \n",
            "assert 0.0 == partition_entropy_by(senior_inputs, 'tweets', 'did_well') \n",
            "assert 0.95 < partition_entropy_by(senior_inputs, 'phd', 'did_well') < 0.96\n",
            "This shows us that our next split should be on tweets , which results in a\n",
            "zero-entropy partition. For these Senior -level candidates, “yes” tweets\n",
            "always result in True  while “no” tweets always result in False .\n",
            "Finally , if we do the same thing for the Junior  candidates, we end up\n",
            "splitting on phd , after which we find that no PhD always results in True  and\n",
            "PhD always results in False .\n",
            "Figure 17-3  shows the complete decision tree.Figur e 17-3. The decision tr ee for hiring\n",
            "P u t t i n g  I t  A l l  T o g e t h e r\n",
            "Now  that we’ve seen how the algorithm works, we would like to implement\n",
            "it more generally . This means we need to decide how we want to represent\n",
            "trees. W e’ll use pretty much the most lightweight representation possible.\n",
            "W e define a tr ee  to be either:\n",
            "a Leaf  (that predicts a single value), or\n",
            "a Split  (containing an attribute to split on, subtrees for specific\n",
            "values of that attribute, and possibly a default value to use if we\n",
            "see an unknown value).\n",
            "from typing import NamedTuple, Union, Any \n",
            " \n",
            "class Leaf(NamedTuple): \n",
            "    value: Anyclass Split(NamedTuple): \n",
            "    attribute: str \n",
            "    subtrees: dict \n",
            "    default_value: Any = None \n",
            " \n",
            "DecisionTree = Union[Leaf, Split]\n",
            "W ith this representation, our hiring tree would look like:\n",
            "hiring_tree = Split('level', {   # first, consider \"level\" \n",
            "    'Junior': Split('phd', {     # if level is \"Junior\", next look at \"phd\" \n",
            "        False: Leaf(True),       #   if \"phd\" is False, predict True \n",
            "        True: Leaf(False)        #   if \"phd\" is True, predict False \n",
            "    }), \n",
            "    'Mid': Leaf(True),           # if level is \"Mid\", just predict True \n",
            "    'Senior': Split('tweets', {  # if level is \"Senior\", look at \"tweets\" \n",
            "        False: Leaf(False),      #   if \"tweets\" is False, predict False \n",
            "        True: Leaf(True)         #   if \"tweets\" is True, predict True \n",
            "    }) \n",
            "})\n",
            "There’ s still the question of what to do if we encounter an unexpected (or\n",
            "missing) attribute value. What should our hiring tree do if it encounters a\n",
            "candidate whose level  is Intern ? W e’ll handle this case by populating the\n",
            "default_value  attribute with the most common label.\n",
            "Given such a representation, we can classify an input with:\n",
            "def classify(tree: DecisionTree, input: Any) -> Any: \n",
            "    \"\"\"classify the input using the given decision tree\"\"\" \n",
            " \n",
            "    # If this is a leaf node, return its value \n",
            "    if isinstance(tree, Leaf): \n",
            "        return tree.value \n",
            " \n",
            "    # Otherwise this tree consists of an attribute to split on \n",
            "    # and a dictionary whose keys are values of that attribute \n",
            "    # and whose values are subtrees to consider next \n",
            "    subtree_key = getattr(input, tree.attribute)if subtree_key not in tree.subtrees:   # If no subtree for key, \n",
            "        return tree.default_value          # return the default value. \n",
            " \n",
            "    subtree = tree.subtrees[subtree_key]   # Choose the appropriate subtree \n",
            "    return classify(subtree, input)        # and use it to classify the input.\n",
            "All that’ s left is to build the tree representation from our training data:\n",
            "def build_tree_id3(inputs: List[Any], \n",
            "                   split_attributes: List[str], \n",
            "                   target_attribute: str) -> DecisionTree: \n",
            "    # Count target labels \n",
            "    label_counts = Counter(getattr(input, target_attribute) \n",
            "                           for input in inputs) \n",
            "    most_common_label = label_counts.most_common(1)[0][0] \n",
            " \n",
            "    # If there's a unique label, predict it \n",
            "    if len(label_counts) == 1: \n",
            "        return Leaf(most_common_label) \n",
            " \n",
            "    # If no split attributes left, return the majority label \n",
            "    if not split_attributes: \n",
            "        return Leaf(most_common_label) \n",
            " \n",
            "    # Otherwise split by the best attribute \n",
            " \n",
            "    def split_entropy(attribute: str) -> float: \n",
            "        \"\"\"Helper function for finding the best attribute\"\"\" \n",
            "        return partition_entropy_by(inputs, attribute, target_attribute) \n",
            " \n",
            "    best_attribute = min(split_attributes, key=split_entropy) \n",
            " \n",
            "    partitions = partition_by(inputs, best_attribute) \n",
            "    new_attributes = [a for a in split_attributes if a != best_attribute] \n",
            " \n",
            "    # Recursively build the subtrees \n",
            "    subtrees = {attribute_value : build_tree_id3(subset, \n",
            "                                                 new_attributes, \n",
            "                                                 target_attribute) \n",
            "                for attribute_value, subset in partitions.items()} \n",
            " \n",
            "    return Split(best_attribute, subtrees, default_value=most_common_label)\n",
            "In the tree we built, every leaf consisted entirely of True  inputs or entirely\n",
            "of False  inputs. This means that the tree predicts perfectly on the trainingdataset. But we can also apply it to new data that wasn’ t in the training set:\n",
            "tree = build_tree_id3(inputs, \n",
            "                      ['level', 'lang', 'tweets', 'phd'], \n",
            "                      'did_well') \n",
            " \n",
            "# Should predict True \n",
            "assert classify(tree, Candidate(\"Junior\", \"Java\", True, False)) \n",
            " \n",
            "# Should predict False \n",
            "assert not classify(tree, Candidate(\"Junior\", \"Java\", True, True))\n",
            "And also to data with unexpected values:\n",
            "# Should predict True \n",
            "assert classify(tree, Candidate(\"Intern\", \"Java\", True, True))\n",
            "N O T E\n",
            "Since our goal was mainly to demonstrate how  to build a tree, we built the tree using the\n",
            "entire dataset. As always, if we were really trying to create a good model for something,\n",
            "we would have collected more data and split it into train/validation/test subsets.\n",
            "R a n d o m  F o r e s t s\n",
            "Given  how closely decision trees can fit themselves to their training data,\n",
            "it’ s not surprising that they have a tendency to overfit. One way of avoiding\n",
            "this is a technique called random for ests , in which we build multiple\n",
            "decision trees and combine their outputs. If they’re classification trees, we\n",
            "might let them vote; if they’re regression trees, we might average their\n",
            "predictions.\n",
            "Our tree-building process was deterministic, so how do we get random\n",
            "trees?\n",
            "One piece involves bootstrapping data (recall “Digression: The Bootstrap” ).\n",
            "Rather than training each tree on all the inputs  in the training set, we train\n",
            "each tree on the result of bootstrap_sample(inputs) . Since each tree isbuilt using dif ferent data, each tree will be dif ferent from every other tree.\n",
            "(A side benefit is that it’ s totally fair to use the nonsampled data to test each\n",
            "tree, which means you can get away with using all of your data as the\n",
            "training set if you are clever in how you measure performance.) This\n",
            "technique is known as bootstrap aggr egating  or bagging .\n",
            "A second source of randomness involves changing the way we choose the\n",
            "best_attribute  to split on. Rather than looking at all the remaining\n",
            "attributes, we first choose a random subset of them and then split on\n",
            "whichever of those is best:\n",
            "    # if there are already few enough split candidates, look at all of them \n",
            "    if len(split_candidates) <= self.num_split_candidates: \n",
            "        sampled_split_candidates = split_candidates \n",
            "    # otherwise pick a random sample \n",
            "    else: \n",
            "        sampled_split_candidates = random.sample(split_candidates, \n",
            "                                                 self.num_split_candidates) \n",
            " \n",
            "    # now choose the best attribute only from those candidates \n",
            "    best_attribute = min(sampled_split_candidates, key=split_entropy) \n",
            " \n",
            "    partitions = partition_by(inputs, best_attribute)\n",
            "This  is an example of a broader technique called ensemble learning  in\n",
            "which we combine several weak learners  (typically high-bias, low-variance\n",
            "models) in order to produce an overall strong model.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "scikit-learn  has many decision tree  models. It also has an ensemble\n",
            "module that includes a RandomForestClassifier  as well as other\n",
            "ensemble methods.\n",
            "XGBoost  is a  library for training gradient boosted  decision  trees\n",
            "that tends to win a lot of Kaggle-style machine learning\n",
            "competitions.W e’ve barely  scratched the surface of decision trees and their\n",
            "algorithms. W ikipedia  is a good starting point for broader\n",
            "exploration.Chapter 18. Neural Networks\n",
            "I like nonsense; it wakes up the brain cells.\n",
            "— Dr . Seuss\n",
            "An  artificial neural network  (or neural network for short)  is a predictive\n",
            "model motivated by the way the brain operates. Think of the brain as a\n",
            "collection of neurons wired together . Each neuron looks at the outputs of\n",
            "the other neurons that feed into it, does a calculation, and then either fires\n",
            "(if the calculation exceeds some threshold) or doesn’ t (if it doesn’ t).\n",
            "Accordingly , artificial neural networks  consist of artificial neurons, which\n",
            "perform similar calculations over their inputs. Neural networks can solve a\n",
            "wide variety of problems like handwriting recognition and face detection,\n",
            "and they are used heavily in deep learning, one of the trendiest subfields of\n",
            "data science. However , most neural networks are “black boxes”—\n",
            "inspecting their details doesn’ t give you much understanding of how  they’re\n",
            "solving a problem. And lar ge neural networks can be dif ficult to train. For\n",
            "most problems you’ll encounter as a budding data scientist, they’re\n",
            "probably not the right choice. Someday , when you’re trying to build an\n",
            "artificial intelligence to bring about the Singularity , they very well might be.\n",
            "P e r c e p t r o n s\n",
            "Pretty  much the simplest neural network is the per ceptr on , which\n",
            "approximates a single neuron with n  binary inputs. It computes a weighted\n",
            "sum of its inputs and “fires” if that weighted sum is 0 or greater:\n",
            "from scratch.linear_algebra import Vector, dot \n",
            " \n",
            "def step_function(x: float) -> float: \n",
            "    return 1.0 if x >= 0 else 0.0 \n",
            " \n",
            "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\" \n",
            "    calculation = dot(weights, x) + bias \n",
            "    return step_function(calculation)\n",
            "The perceptron is simply distinguishing between the half-spaces separated\n",
            "by the hyperplane of points x  for which:\n",
            "dot(weights, x) + bias == 0\n",
            "W ith properly chosen weights, perceptrons can solve a number of simple\n",
            "problems ( Figure 18-1 ). For example, we can create an AND gate  (which\n",
            "returns 1 if both its inputs are 1 but returns 0 if one of its inputs is 0) with:\n",
            "and_weights = [2., 2] \n",
            "and_bias = -3. \n",
            " \n",
            "assert perceptron_output(and_weights, and_bias, [1, 1]) == 1 \n",
            "assert perceptron_output(and_weights, and_bias, [0, 1]) == 0 \n",
            "assert perceptron_output(and_weights, and_bias, [1, 0]) == 0 \n",
            "assert perceptron_output(and_weights, and_bias, [0, 0]) == 0\n",
            "If both inputs are 1, the calculation  equals 2 + 2 – 3 = 1, and the output is\n",
            "1. If only one of the inputs is 1, the calculation  equals 2 + 0 – 3 = –1, and\n",
            "the output is 0. And if both of the inputs are 0, the calculation  equals –3,\n",
            "and the output is 0.\n",
            "Using similar reasoning, we could build an OR gate  with:\n",
            "or_weights = [2., 2] \n",
            "or_bias = -1. \n",
            " \n",
            "assert perceptron_output(or_weights, or_bias, [1, 1]) == 1 \n",
            "assert perceptron_output(or_weights, or_bias, [0, 1]) == 1 \n",
            "assert perceptron_output(or_weights, or_bias, [1, 0]) == 1 \n",
            "assert perceptron_output(or_weights, or_bias, [0, 0]) == 0Figur e 18-1. Decision space for a two-input per ceptr on\n",
            "W e could also build a NOT gate  (which has one input and converts 1 to 0\n",
            "and 0 to 1) with:\n",
            "not_weights = [-2.] \n",
            "not_bias = 1. \n",
            " \n",
            "assert perceptron_output(not_weights, not_bias, [0]) == 1 \n",
            "assert perceptron_output(not_weights, not_bias, [1]) == 0\n",
            "However , there are some problems that simply can’ t be solved by a single\n",
            "perceptron. For example, no matter how hard you try , you cannot use a\n",
            "perceptron to build an XOR gate  that outputs 1 if exactly one of its inputs is\n",
            "1 and 0 otherwise. This is where we start needing more complicated neural\n",
            "networks.Of course, you don’ t need to approximate a neuron in order to build a logic\n",
            "gate:\n",
            "and_gate = min \n",
            "or_gate = max \n",
            "xor_gate = lambda x, y: 0 if x == y else 1\n",
            "Like real neurons, artificial neurons start getting more interesting when you\n",
            "start connecting them together .\n",
            "F e e d - F o r w a r d  N e u r a l  N e t w o r k s\n",
            "The  topology of the brain is enormously complicated, so it’ s common to\n",
            "approximate it with an idealized feed-forwar d  neural network that consists\n",
            "of discrete layers  of neurons, each connected to the next. This typically\n",
            "entails an input layer (which receives inputs and feeds them forward\n",
            "unchanged), one or more “hidden layers” (each of which consists of\n",
            "neurons that take the outputs of the previous layer , performs some\n",
            "calculation, and passes the result to the next layer), and an output layer\n",
            "(which produces the final outputs).\n",
            "Just like in the perceptron, each (noninput) neuron has a weight\n",
            "corresponding to each of its inputs and a bias. T o make our representation\n",
            "simpler , we’ll add the bias to the end of our weights vector and give  each\n",
            "neuron a bias input  that always equals 1.\n",
            "As with the perceptron, for each neuron we’ll sum up the products of its\n",
            "inputs and its weights. But here, rather than outputting the step_function\n",
            "applied to that product, we’ll output a smooth approximation of it. Here\n",
            "we’ll use the sigmoid  function ( Figure 18-2 ):\n",
            "import math \n",
            " \n",
            "def sigmoid(t: float) -> float: \n",
            "    return 1 / (1 + math.exp(-t))Figur e 18-2. The sigmoid function\n",
            "Why use sigmoid  instead of the simpler step_function ? In order to train\n",
            "a neural network, we need to use calculus, and in order to use calculus, we\n",
            "need smooth  functions. step_function  isn’ t even continuous, and sigmoid\n",
            "is a good smooth approximation of it.\n",
            "N O T E\n",
            "Y ou may remember sigmoid  from Chapter 16 , where it was called logistic .\n",
            "T echnically “sigmoid” refers to the shape  of the function and “logistic” to this particular\n",
            "function, although people often use the terms interchangeably .\n",
            "W e then calculate the output as:def neuron_output(weights: Vector, inputs: Vector) -> float: \n",
            "    # weights includes the bias term, inputs includes a 1 \n",
            "    return sigmoid(dot(weights, inputs))\n",
            "Given this function, we can represent a neuron simply as a vector of\n",
            "weights whose length is one more than the number of inputs to that neuron\n",
            "(because of the bias weight). Then we can represent a neural network as a\n",
            "list of (noninput) layers , where each layer is just a list of the neurons in that\n",
            "layer .\n",
            "That is, we’ll represent a neural network as a list (layers) of lists (neurons)\n",
            "of vectors (weights).\n",
            "Given such a representation, using the neural network is quite simple:\n",
            "from typing import List \n",
            " \n",
            "def feed_forward(neural_network: List[List[Vector]], \n",
            "                 input_vector: Vector) -> List[Vector]: \n",
            "    \"\"\" \n",
            "    Feeds the input vector through the neural network. \n",
            "    Returns the outputs of all layers (not just the last one). \n",
            "    \"\"\" \n",
            "    outputs: List[Vector] = [] \n",
            " \n",
            "    for layer in neural_network: \n",
            "        input_with_bias = input_vector + [1]              # Add a constant. \n",
            "        output = [neuron_output(neuron, input_with_bias)  # Compute the output \n",
            "                  for neuron in layer]                    # for each neuron. \n",
            "        outputs.append(output)                            # Add to results. \n",
            " \n",
            "        # Then the input to the next layer is the output of this one \n",
            "        input_vector = output \n",
            " \n",
            "    return outputs\n",
            "Now it’ s easy to build the XOR gate that we couldn’ t build with a single\n",
            "perceptron. W e just need to scale the weights up so that the\n",
            "neuron_output s are either really close to 0 or really close to 1:\n",
            "xor_network = [# hidden layer \n",
            "               [[20., 20, -30],      # 'and' neuron[20., 20, -10]],     # 'or'  neuron \n",
            "               # output layer \n",
            "               [[-60., 60, -30]]]    # '2nd input but not 1st input' neuron \n",
            " \n",
            "# feed_forward returns the outputs of all layers, so the [-1] gets the \n",
            "# final output, and the [0] gets the value out of the resulting vector \n",
            "assert 0.000 < feed_forward(xor_network, [0, 0])[-1][0] < 0.001 \n",
            "assert 0.999 < feed_forward(xor_network, [1, 0])[-1][0] < 1.000 \n",
            "assert 0.999 < feed_forward(xor_network, [0, 1])[-1][0] < 1.000 \n",
            "assert 0.000 < feed_forward(xor_network, [1, 1])[-1][0] < 0.001\n",
            "For a given input (which is a two-dimensional vector), the hidden layer\n",
            "produces a two-dimensional vector consisting of the “and” of the two input\n",
            "values and the “or” of the two input values.\n",
            "And the output layer takes a two-dimensional vector and computes “second\n",
            "element but not first element.” The result is a network that performs “or , but\n",
            "not and,” which is precisely XOR ( Figure 18-3 ).\n",
            "Figur e 18-3. A neural network for XOR\n",
            "One  suggestive way of thinking about this is that the hidden layer is\n",
            "computing featur es  of the input data (in this case “and” and “or”) and the\n",
            "output layer is combining those features in a way that generates the desired\n",
            "output.B a c k p r o p a g a t i o n\n",
            "Usually  we don’ t build neural networks by hand. This is in part because we\n",
            "use them to solve much bigger problems—an image recognition problem\n",
            "might involve hundreds or thousands of neurons. And it’ s in part because\n",
            "we usually won’ t be able to “reason out” what the neurons should be.\n",
            "Instead (as usual) we use data to train  neural networks. The typical\n",
            "approach is an algorithm called backpr opagation , which uses gradient\n",
            "descent or one of its variants.\n",
            "Imagine we have a training set that consists of input vectors and\n",
            "corresponding tar get output vectors. For example, in our previous\n",
            "xor_network  example, the input vector [1, 0]  corresponded to the tar get\n",
            "output [1] . Imagine that our network has some set of weights. W e then\n",
            "adjust the weights using the following algorithm:\n",
            "1 . Run feed_forward  on an input vector to produce the outputs of all\n",
            "the neurons in the network.\n",
            "2 . W e know the tar get output, so we can compute a loss  that’ s the sum\n",
            "of the squared errors.\n",
            "3 . Compute the gradient of this loss as a function of the output\n",
            "neuron’ s weights.\n",
            "4 . “Propagate” the gradients and errors backward to compute the\n",
            "gradients with respect to the hidden neurons’ weights.\n",
            "5 . T ake a gradient descent step.\n",
            "T ypically we run this algorithm many times for our entire training set until\n",
            "the network conver ges.\n",
            "T o start with, let’ s write the function to compute the gradients:\n",
            "def sqerror_gradients(network: List[List[Vector]], \n",
            "                      input_vector: Vector, \n",
            "                      target_vector: Vector) -> List[List[Vector]]: \n",
            "    \"\"\"Given a neural network, an input vector, and a target vector, \n",
            "    make a prediction and compute the gradient of the squared error \n",
            "    loss with respect to the neuron weights. \n",
            "    \"\"\" \n",
            "    # forward pass \n",
            "    hidden_outputs, outputs = feed_forward(network, input_vector) \n",
            " \n",
            "    # gradients with respect to output neuron pre-activation outputs \n",
            "    output_deltas = [output * (1 - output) * (output - target) \n",
            "                     for output, target in zip(outputs, target_vector)] \n",
            " \n",
            "    # gradients with respect to output neuron weights \n",
            "    output_grads = [[output_deltas[i] * hidden_output \n",
            "                     for hidden_output in hidden_outputs + [1]] \n",
            "                    for i, output_neuron in enumerate(network[-1])] \n",
            " \n",
            "    # gradients with respect to hidden neuron pre-activation outputs \n",
            "    hidden_deltas = [hidden_output * (1 - hidden_output) * \n",
            "                         dot(output_deltas, [n[i] for n in network[-1]]) \n",
            "                     for i, hidden_output in enumerate(hidden_outputs)] \n",
            " \n",
            "    # gradients with respect to hidden neuron weights \n",
            "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]] \n",
            "                    for i, hidden_neuron in enumerate(network[0])] \n",
            " \n",
            "    return [hidden_grads, output_grads]\n",
            "The math behind the preceding calculations is not terribly dif ficult, but it\n",
            "involves some tedious calculus and careful attention to detail, so I’ll leave it\n",
            "as an exercise for you.\n",
            "Armed with the ability to compute gradients, we can now train neural\n",
            "networks. Let’ s try to learn the XOR network we previously designed by\n",
            "hand.\n",
            "W e’ll start by generating the training data and initializing our neural\n",
            "network with random weights:\n",
            "import random \n",
            "random.seed(0) \n",
            " \n",
            "# training data \n",
            "xs = [[0., 0], [0., 1], [1., 0], [1., 1]] \n",
            "ys = [[0.], [1.], [1.], [0.]]# start with random weights \n",
            "network = [ # hidden layer: 2 inputs -> 2 outputs \n",
            "            [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron \n",
            "             [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron \n",
            "            # output layer: 2 inputs -> 1 output \n",
            "            [[random.random() for _ in range(2 + 1)]]   # 1st output neuron \n",
            "          ]\n",
            "As usual, we can train it using gradient descent. One dif ference from our\n",
            "previous examples is that here we have several parameter vectors, each with\n",
            "its own gradient, which means we’ll have to call gradient_step  for each\n",
            "of them.\n",
            "from scratch.gradient_descent import gradient_step \n",
            "import tqdm \n",
            " \n",
            "learning_rate = 1.0 \n",
            " \n",
            "for epoch in tqdm.trange(20000, desc=\"neural net for xor\"): \n",
            "    for x, y in zip(xs, ys): \n",
            "        gradients = sqerror_gradients(network, x, y) \n",
            " \n",
            "        # Take a gradient step for each neuron in each layer \n",
            "        network = [[gradient_step(neuron, grad, -learning_rate) \n",
            "                    for neuron, grad in zip(layer, layer_grad)] \n",
            "                   for layer, layer_grad in zip(network, gradients)] \n",
            " \n",
            "# check that it learned XOR \n",
            "assert feed_forward(network, [0, 0])[-1][0] < 0.01 \n",
            "assert feed_forward(network, [0, 1])[-1][0] > 0.99 \n",
            "assert feed_forward(network, [1, 0])[-1][0] > 0.99 \n",
            "assert feed_forward(network, [1, 1])[-1][0] < 0.01\n",
            "For me the resulting network has weights that look like:\n",
            "[   # hidden layer \n",
            "    [[7, 7, -3],     # computes OR \n",
            "     [5, 5, -8]],    # computes AND \n",
            "    # output layer \n",
            "    [[11, -12, -5]]  # computes \"first but not second\" \n",
            "]\n",
            "which is conceptually pretty similar to our previous bespoke network.E x a m p l e :  F i z z  B u z z\n",
            "The  VP of Engineering wants to interview technical candidates by making\n",
            "them solve “Fizz Buzz,” the following well-trod programming challenge:\n",
            "Print the numbers 1 to 100, except that if the number is divisible \n",
            "by 3, print \"fizz\"; if the number is divisible by 5, print \"buzz\"; \n",
            "and if the number is divisible by 15, print \"fizzbuzz\".\n",
            "He thinks the ability to solve this demonstrates extreme programming skill.\n",
            "Y ou think that this problem is so simple that a neural network could solve it.\n",
            "Neural networks take vectors as inputs and produce vectors as outputs. As\n",
            "stated, the programming problem is to turn an integer into a string. So the\n",
            "first challenge is to come up with a way to recast it as a vector problem.\n",
            "For the outputs it’ s not tough: there are basically four classes of outputs, so\n",
            "we can encode the output as a vector of four 0s and 1s:\n",
            "def fizz_buzz_encode(x: int) -> Vector: \n",
            "    if x % 15 == 0: \n",
            "        return [0, 0, 0, 1] \n",
            "    elif x % 5 == 0: \n",
            "        return [0, 0, 1, 0] \n",
            "    elif x % 3 == 0: \n",
            "        return [0, 1, 0, 0] \n",
            "    else: \n",
            "        return [1, 0, 0, 0] \n",
            " \n",
            "assert fizz_buzz_encode(2) == [1, 0, 0, 0] \n",
            "assert fizz_buzz_encode(6) == [0, 1, 0, 0] \n",
            "assert fizz_buzz_encode(10) == [0, 0, 1, 0] \n",
            "assert fizz_buzz_encode(30) == [0, 0, 0, 1]\n",
            "W e’ll use this to generate our tar get vectors. The input vectors are less\n",
            "obvious. Y ou don’ t want to just use a one-dimensional vector containing the\n",
            "input number , for a couple of reasons. A single input captures an\n",
            "“intensity ,” but the fact that 2 is twice as much as 1, and that 4 is twice as\n",
            "much again, doesn’ t feel relevant to this problem. Additionally , with justone input the hidden layer wouldn’ t be able to compute very interesting\n",
            "features, which means it probably wouldn’ t be able to solve the problem.\n",
            "It turns out that one thing that works reasonably well is to convert each\n",
            "number to its binary  representation of 1s and 0s. (Don’ t worry , this isn’ t\n",
            "obvious—at least it wasn’ t to me.)\n",
            "def binary_encode(x: int) -> Vector: \n",
            "    binary: List[float] = [] \n",
            " \n",
            "    for i in range(10): \n",
            "        binary.append(x % 2) \n",
            "        x = x // 2 \n",
            " \n",
            "    return binary \n",
            " \n",
            "#                             1  2  4  8 16 32 64 128 256 512 \n",
            "assert binary_encode(0)   == [0, 0, 0, 0, 0, 0, 0, 0,  0,  0] \n",
            "assert binary_encode(1)   == [1, 0, 0, 0, 0, 0, 0, 0,  0,  0] \n",
            "assert binary_encode(10)  == [0, 1, 0, 1, 0, 0, 0, 0,  0,  0] \n",
            "assert binary_encode(101) == [1, 0, 1, 0, 0, 1, 1, 0,  0,  0] \n",
            "assert binary_encode(999) == [1, 1, 1, 0, 0, 1, 1, 1,  1,  1]\n",
            "As the goal is to construct the outputs for the numbers 1 to 100, it would be\n",
            "cheating to train on those numbers. Therefore, we’ll train on the numbers\n",
            "101 to 1,023 (which is the lar gest number we can represent with 10 binary\n",
            "digits):\n",
            "xs = [binary_encode(n) for n in range(101, 1024)] \n",
            "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n",
            "Next, let’ s create a neural network with random initial weights. It will have\n",
            "10 input neurons (since we’re representing our inputs as 10-dimensional\n",
            "vectors) and 4 output neurons (since we’re representing our tar gets as 4-\n",
            "dimensional vectors). W e’ll give it 25 hidden units, but we’ll use a variable\n",
            "for that so it’ s easy to change:\n",
            "NUM_HIDDEN = 25 \n",
            " \n",
            "network = [ \n",
            "    # hidden layer: 10 inputs -> NUM_HIDDEN outputs[[random.random() for _ in range(10 + 1)] for _ in range(NUM_HIDDEN)], \n",
            " \n",
            "    # output_layer: NUM_HIDDEN inputs -> 4 outputs \n",
            "    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(4)] \n",
            "]\n",
            "That’ s it. Now we’re ready to train. Because this is a more involved\n",
            "problem (and there are a lot more things to mess up), we’d like to closely\n",
            "monitor the training process. In particular , for each epoch we’ll track the\n",
            "sum of squared errors and print them out. W e want to make sure they\n",
            "decrease:\n",
            "from scratch.linear_algebra import squared_distance \n",
            " \n",
            "learning_rate = 1.0 \n",
            " \n",
            "with tqdm.trange(500) as t: \n",
            "    for epoch in t: \n",
            "        epoch_loss = 0.0 \n",
            " \n",
            "        for x, y in zip(xs, ys): \n",
            "            predicted = feed_forward(network, x)[-1] \n",
            "            epoch_loss += squared_distance(predicted, y) \n",
            "            gradients = sqerror_gradients(network, x, y) \n",
            " \n",
            "            # Take a gradient step for each neuron in each layer \n",
            "            network = [[gradient_step(neuron, grad, -learning_rate) \n",
            "                        for neuron, grad in zip(layer, layer_grad)] \n",
            "                    for layer, layer_grad in zip(network, gradients)] \n",
            " \n",
            "        t.set_description(f\"fizz buzz (loss: {epoch_loss:.2f})\")\n",
            "This will take a while to train, but eventually the loss should start to bottom\n",
            "out.\n",
            "At last we’re ready to solve our original problem. W e have one remaining\n",
            "issue. Our network will produce a four -dimensional vector of numbers, but\n",
            "we want a single prediction. W e’ll do that by taking the argmax , which is\n",
            "the index of the lar gest value:\n",
            "def argmax(xs: list) -> int: \n",
            "    \"\"\"Returns the index of the largest value\"\"\"return max(range(len(xs)), key=lambda i: xs[i]) \n",
            " \n",
            "assert argmax([0, -1]) == 0               # items[0] is largest \n",
            "assert argmax([-1, 0]) == 1               # items[1] is largest \n",
            "assert argmax([-1, 10, 5, 20, -3]) == 3   # items[3] is largest\n",
            "Now we can finally solve “FizzBuzz”:\n",
            "num_correct = 0 \n",
            " \n",
            "for n in range(1, 101): \n",
            "    x = binary_encode(n) \n",
            "    predicted = argmax(feed_forward(network, x)[-1]) \n",
            "    actual = argmax(fizz_buzz_encode(n)) \n",
            "    labels = [str(n), \"fizz\", \"buzz\", \"fizzbuzz\"] \n",
            "    print(n, labels[predicted], labels[actual]) \n",
            " \n",
            "    if predicted == actual: \n",
            "        num_correct += 1 \n",
            " \n",
            "print(num_correct, \"/\", 100)\n",
            "For me the trained network gets 96/100 correct, which is well above the VP\n",
            "of Engineering’ s hiring threshold. Faced with the evidence, he relents and\n",
            "changes the interview challenge to “Invert a Binary T ree.”\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "Keep reading: Chapter 19  will explore these topics in much more\n",
            "detail.\n",
            "My blog post on “Fizz Buzz in T ensorflow”  is pretty good.Chapter 19. Deep Learning\n",
            "A little learning is a danger ous thing; Drink deep, or taste not the\n",
            "Pierian spring.\n",
            "— Alexander Pope\n",
            "Deep learning  originally  referred to the application of “deep” neural\n",
            "networks (that is, networks with more than one hidden layer), although in\n",
            "practice the term now encompasses a wide variety of neural architectures\n",
            "(including the “simple” neural networks we developed in Chapter 18 ).\n",
            "In this chapter we’ll build on our previous work and look at a wider variety\n",
            "of neural networks. T o do so, we’ll introduce a number of abstractions that\n",
            "allow us to think about neural networks in a more general way .\n",
            "T h e  T e n s o r\n",
            "Previously , we  made a distinction between vectors (one-dimensional arrays)\n",
            "and matrices (two-dimensional arrays). When we start working with more\n",
            "complicated neural networks, we’ll need to use higher -dimensional arrays\n",
            "as well.\n",
            "In many neural network libraries, n -dimensional arrays are referred to as\n",
            "tensors , which is what we’ll call them too. (There are pedantic\n",
            "mathematical reasons not to refer to n -dimensional arrays as tensors; if you\n",
            "are such a pedant, your objection is noted.)\n",
            "If I were writing an entire book about deep learning, I’d implement a full-\n",
            "featured Tensor  class that overloaded Python’ s arithmetic operators and\n",
            "could handle a variety of other operations. Such an implementation would\n",
            "take an entire chapter on its own. Here we’ll cheat and say that a Tensor  is\n",
            "just a list . This is true in one direction—all of our vectors and matrices\n",
            "and higher -dimensional analogues ar e  lists. It is certainly not true in theother direction—most Python list s are not n -dimensional arrays in our\n",
            "sense.\n",
            "N O T E\n",
            "Ideally you’d like to do something like:\n",
            "# A Tensor is either a float, or a List of Tensors \n",
            "Tensor = Union[float, List[Tensor]]\n",
            "However , Python won’ t let you define recursive types like that. And even if it did that\n",
            "definition is still not right, as it allows for bad “tensors” like:\n",
            "[[1.0, 2.0], \n",
            " [3.0]]\n",
            "whose rows have dif ferent sizes, which makes it not an n -dimensional array .\n",
            "So, like I said, we’ll just cheat:\n",
            "Tensor = list\n",
            "And we’ll write a helper function to find a tensor ’ s shape :\n",
            "from typing import List \n",
            " \n",
            "def shape(tensor: Tensor) -> List[int]: \n",
            "    sizes: List[int] = [] \n",
            "    while isinstance(tensor, list): \n",
            "        sizes.append(len(tensor)) \n",
            "        tensor = tensor[0] \n",
            "    return sizes \n",
            " \n",
            "assert shape([1, 2, 3]) == [3] \n",
            "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]\n",
            "Because tensors can have any number of dimensions, we’ll typically need\n",
            "to work with them recursively . W e’ll do one thing in the one-dimensionalcase and recurse in the higher -dimensional case:\n",
            "def is_1d(tensor: Tensor) -> bool: \n",
            "    \"\"\" \n",
            "    If tensor[0] is a list, it's a higher-order tensor. \n",
            "    Otherwise, tensor is 1-dimensional (that is, a vector). \n",
            "    \"\"\" \n",
            "    return not isinstance(tensor[0], list) \n",
            " \n",
            "assert is_1d([1, 2, 3]) \n",
            "assert not is_1d([[1, 2], [3, 4]])\n",
            "which we can use to write a recursive tensor_sum  function:\n",
            "def tensor_sum(tensor: Tensor) -> float: \n",
            "    \"\"\"Sums up all the values in the tensor\"\"\" \n",
            "    if is_1d(tensor): \n",
            "        return sum(tensor)  # just a list of floats, use Python sum \n",
            "    else: \n",
            "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row \n",
            "                   for tensor_i in tensor)   # and sum up those results. \n",
            " \n",
            "assert tensor_sum([1, 2, 3]) == 6 \n",
            "assert tensor_sum([[1, 2], [3, 4]]) == 10\n",
            "If you’re not used to thinking recursively , you should ponder this until it\n",
            "makes sense, because we’ll use the same logic throughout this chapter .\n",
            "However , we’ll create a couple of helper functions so that we don’ t have to\n",
            "rewrite this logic everywhere. The first applies a function elementwise to a\n",
            "single tensor:\n",
            "from typing import Callable \n",
            " \n",
            "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor: \n",
            "    \"\"\"Applies f elementwise\"\"\" \n",
            "    if is_1d(tensor): \n",
            "        return [f(x) for x in tensor] \n",
            "    else: \n",
            "        return [tensor_apply(f, tensor_i) for tensor_i in tensor] \n",
            " \n",
            "assert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4] \n",
            "assert tensor_apply(lambda x: 2 * x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]W e can use this to write a function that creates a zero tensor with the same\n",
            "shape as a given tensor:\n",
            "def zeros_like(tensor: Tensor) -> Tensor: \n",
            "    return tensor_apply(lambda _: 0.0, tensor) \n",
            " \n",
            "assert zeros_like([1, 2, 3]) == [0, 0, 0] \n",
            "assert zeros_like([[1, 2], [3, 4]]) == [[0, 0], [0, 0]]\n",
            "W e’ll also need to apply a function to corresponding elements from two\n",
            "tensors (which had better be the exact same shape, although we won’ t check\n",
            "that):\n",
            "def tensor_combine(f: Callable[[float, float], float], \n",
            "                   t1: Tensor, \n",
            "                   t2: Tensor) -> Tensor: \n",
            "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\" \n",
            "    if is_1d(t1): \n",
            "        return [f(x, y) for x, y in zip(t1, t2)] \n",
            "    else: \n",
            "        return [tensor_combine(f, t1_i, t2_i) \n",
            "                for t1_i, t2_i in zip(t1, t2)] \n",
            " \n",
            "import operator \n",
            "assert tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]) == [5, 7, 9] \n",
            "assert tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]) == [4, 10, 18]\n",
            "T h e  L a y e r  A b s t r a c t i o n\n",
            "In  the previous chapter we built a simple neural net that allowed us to stack\n",
            "two layers of neurons, each of which computed sigmoid(dot(weights,\n",
            "inputs)) .\n",
            "Although that’ s perhaps an idealized representation of what an actual\n",
            "neuron does, in practice we’d like to allow a wider variety of things.\n",
            "Perhaps we’d like the neurons to remember something about their previous\n",
            "inputs. Perhaps we’d like to use a dif ferent activation function than\n",
            "sigmoid . And frequently we’d like to use more than two layers. (Ourfeed_forward  function actually handled any number of layers, but our\n",
            "gradient computations did not.)\n",
            "In this chapter we’ll build machinery for implementing such a variety of\n",
            "neural networks. Our fundamental abstraction will be the Layer , something\n",
            "that knows how to apply some function to its inputs and that knows how to\n",
            "backpropagate gradients.\n",
            "One way of thinking about the neural networks we built in Chapter 18  is as\n",
            "a “linear” layer , followed by a “sigmoid” layer , then another linear layer\n",
            "and another sigmoid layer . W e didn’ t distinguish them in these terms, but\n",
            "doing so will allow us to experiment with much more general structures:\n",
            "from typing import Iterable, Tuple \n",
            " \n",
            "class Layer: \n",
            "    \"\"\" \n",
            "    Our neural networks will be composed of Layers, each of which \n",
            "    knows how to do some computation on its inputs in the \"forward\" \n",
            "    direction and propagate gradients in the \"backward\" direction. \n",
            "    \"\"\" \n",
            "    def forward(self, input): \n",
            "        \"\"\" \n",
            "        Note the lack of types. We're not going to be prescriptive \n",
            "        about what kinds of inputs layers can take and what kinds \n",
            "        of outputs they can return. \n",
            "        \"\"\" \n",
            "        raise NotImplementedError \n",
            " \n",
            "    def backward(self, gradient): \n",
            "        \"\"\" \n",
            "        Similarly, we're not going to be prescriptive about what the \n",
            "        gradient looks like. It's up to you the user to make sure \n",
            "        that you're doing things sensibly. \n",
            "        \"\"\" \n",
            "        raise NotImplementedError \n",
            " \n",
            "    def params(self) -> Iterable[Tensor]: \n",
            "        \"\"\" \n",
            "        Returns the parameters of this layer. The default implementation \n",
            "        returns nothing, so that if you have a layer with no parameters \n",
            "        you don't have to implement this. \n",
            "        \"\"\" \n",
            "        return ()def grads(self) -> Iterable[Tensor]: \n",
            "        \"\"\" \n",
            "        Returns the gradients, in the same order as params(). \n",
            "        \"\"\" \n",
            "        return ()\n",
            "The forward  and backward  methods will have to be implemented in our\n",
            "concrete subclasses. Once we build a neural net, we’ll want to train it using\n",
            "gradient descent, which means we’ll want to update each parameter in the\n",
            "network using its gradient. Accordingly , we insist that each layer be able to\n",
            "tell us its parameters and gradients.\n",
            "Some layers (for example, a layer that applies sigmoid  to each of its inputs)\n",
            "have no parameters to update, so we provide a default implementation that\n",
            "handles that case.\n",
            "Let’ s look at that layer:\n",
            "from scratch.neural_networks import sigmoid \n",
            " \n",
            "class Sigmoid(Layer): \n",
            "    def forward(self, input: Tensor) -> Tensor: \n",
            "        \"\"\" \n",
            "        Apply sigmoid to each element of the input tensor, \n",
            "        and save the results to use in backpropagation. \n",
            "        \"\"\" \n",
            "        self.sigmoids = tensor_apply(sigmoid, input) \n",
            "        return self.sigmoids \n",
            " \n",
            "    def backward(self, gradient: Tensor) -> Tensor: \n",
            "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad, \n",
            "                              self.sigmoids, \n",
            "                              gradient)\n",
            "There are a couple of things to notice here. One is that during the forward\n",
            "pass we saved the computed sigmoids so that we could use them later in the\n",
            "backward pass. Our layers will typically need to do this sort of thing.\n",
            "Second, you may be wondering where the sig * (1 - sig) * grad\n",
            "comes from. This is just the chain rule from calculus and corresponds to theoutput * (1 - output) * (output - target)  term in our previous\n",
            "neural networks.\n",
            "Finally , you can see how we were able to make use of the tensor_apply\n",
            "and the tensor_combine  functions. Most of our layers will use these\n",
            "functions similarly .\n",
            "T h e  L i n e a r  L a y e r\n",
            "The  other piece we’ll need to duplicate the neural networks from\n",
            "Chapter 18  is a “linear” layer that represents the dot(weights, inputs)\n",
            "part of the neurons.\n",
            "This layer will have parameters, which we’d like to initialize with random\n",
            "values.\n",
            "It turns out that the initial parameter values can make a huge dif ference in\n",
            "how quickly (and sometimes whether ) the network trains. If weights are too\n",
            "big, they may produce lar ge outputs in a range where the activation function\n",
            "has near -zero gradients. And parts of the network that have zero gradients\n",
            "necessarily can’ t learn anything via gradient descent.\n",
            "Accordingly , we’ll implement  three dif ferent schemes for randomly\n",
            "generating our weight tensors. The first is to choose each value from the\n",
            "random uniform distribution on [0, 1]—that is, as a random.random() . The\n",
            "second (and default) is to choose each value randomly from a standard\n",
            "normal distribution. And  the third is to use Xavier initialization , where each\n",
            "weight is initialized with a random draw from a normal distribution with\n",
            "mean 0 and variance 2 / ( num_inputs  + num_outputs ). It turns out this\n",
            "often works nicely for neural network weights. W e’ll implement these with\n",
            "a random_uniform  function and a random_normal  function:\n",
            "import random \n",
            " \n",
            "from scratch.probability import inverse_normal_cdf \n",
            " \n",
            "def random_uniform(*dims: int) -> Tensor:if len(dims) == 1: \n",
            "        return [random.random() for _ in range(dims[0])] \n",
            "    else: \n",
            "        return [random_uniform(*dims[1:]) for _ in range(dims[0])] \n",
            " \n",
            "def random_normal(*dims: int, \n",
            "                  mean: float = 0.0, \n",
            "                  variance: float = 1.0) -> Tensor: \n",
            "    if len(dims) == 1: \n",
            "        return [mean + variance * inverse_normal_cdf(random.random()) \n",
            "                for _ in range(dims[0])] \n",
            "    else: \n",
            "        return [random_normal(*dims[1:], mean=mean, variance=variance) \n",
            "                for _ in range(dims[0])] \n",
            " \n",
            "assert shape(random_uniform(2, 3, 4)) == [2, 3, 4] \n",
            "assert shape(random_normal(5, 6, mean=10)) == [5, 6]\n",
            "And then wrap them all in a random_tensor  function:\n",
            "def random_tensor(*dims: int, init: str = 'normal') -> Tensor: \n",
            "    if init == 'normal': \n",
            "        return random_normal(*dims) \n",
            "    elif init == 'uniform': \n",
            "        return random_uniform(*dims) \n",
            "    elif init == 'xavier': \n",
            "        variance = len(dims) / sum(dims) \n",
            "        return random_normal(*dims, variance=variance) \n",
            "    else: \n",
            "        raise ValueError(f\"unknown init: {init}\")\n",
            "Now we can define our linear layer . W e need to initialize it with the\n",
            "dimension of the inputs (which tells us how many weights each neuron\n",
            "needs), the dimension of the outputs (which tells us how many neurons we\n",
            "should have), and the initialization scheme we want:\n",
            "from scratch.linear_algebra import dot \n",
            " \n",
            "class Linear(Layer): \n",
            "    def __init__(self, \n",
            "                 input_dim: int, \n",
            "                 output_dim: int, \n",
            "                 init: str = 'xavier') -> None: \n",
            "        \"\"\"A layer of output_dim neurons, each with input_dim weights \n",
            "        (and a bias). \n",
            "        \"\"\" \n",
            "        self.input_dim = input_dim \n",
            "        self.output_dim = output_dim \n",
            " \n",
            "        # self.w[o] is the weights for the oth neuron \n",
            "        self.w = random_tensor(output_dim, input_dim, init=init) \n",
            " \n",
            "        # self.b[o] is the bias term for the oth neuron \n",
            "        self.b = random_tensor(output_dim, init=init)\n",
            "N O T E\n",
            "In case you’re wondering how important the initialization schemes are, some of the\n",
            "networks in this chapter I couldn’ t get to train at all with dif ferent initializations than the\n",
            "ones I used.\n",
            "The forward  method is easy to implement. W e’ll get one output per neuron,\n",
            "which we stick in a vector . And each neuron’ s output is just the dot  of its\n",
            "weights with the input, plus its bias:\n",
            "    def forward(self, input: Tensor) -> Tensor: \n",
            "        # Save the input to use in the backward pass. \n",
            "        self.input = input \n",
            " \n",
            "        # Return the vector of neuron outputs. \n",
            "        return [dot(input, self.w[o]) + self.b[o] \n",
            "                for o in range(self.output_dim)]\n",
            "The backward  method is more involved, but if you know calculus it’ s not\n",
            "dif ficult:\n",
            "    def backward(self, gradient: Tensor) -> Tensor: \n",
            "        # Each b[o] gets added to output[o], which means \n",
            "        # the gradient of b is the same as the output gradient. \n",
            "        self.b_grad = gradient \n",
            " \n",
            "        # Each w[o][i] multiplies input[i] and gets added to output[o]. \n",
            "        # So its gradient is input[i] * gradient[o]. \n",
            "        self.w_grad = [[self.input[i] * gradient[o]for i in range(self.input_dim)] \n",
            "                       for o in range(self.output_dim)] \n",
            " \n",
            "        # Each input[i] multiplies every w[o][i] and gets added to every \n",
            "        # output[o]. So its gradient is the sum of w[o][i] * gradient[o] \n",
            "        # across all the outputs. \n",
            "        return [sum(self.w[o][i] * gradient[o] for o in \n",
            "range(self.output_dim)) \n",
            "                for i in range(self.input_dim)]\n",
            "N O T E\n",
            "In a “real” tensor library , these (and many other) operations would be represented as\n",
            "matrix or tensor multiplications, which those libraries are designed to do very quickly .\n",
            "Our library is very  slow .\n",
            "Finally , here we do need to implement params  and grads . W e have two\n",
            "parameters and two corresponding gradients:\n",
            "    def params(self) -> Iterable[Tensor]: \n",
            "        return [self.w, self.b] \n",
            " \n",
            "    def grads(self) -> Iterable[Tensor]: \n",
            "        return [self.w_grad, self.b_grad]\n",
            "N e u r a l  N e t w o r k s  a s  a  S e q u e n c e  o f  L a y e r s\n",
            "W e’d  like to think of neural networks as sequences of layers, so let’ s come\n",
            "up with a way to combine multiple layers into one. The resulting neural\n",
            "network is itself a layer , and it implements the Layer  methods in the\n",
            "obvious ways:\n",
            "from typing import List \n",
            " \n",
            "class Sequential(Layer): \n",
            "    \"\"\" \n",
            "    A layer consisting of a sequence of other layers. \n",
            "    It's up to you to make sure that the output of each layer \n",
            "    makes sense as the input to the next layer.\"\"\" \n",
            "    def __init__(self, layers: List[Layer]) -> None: \n",
            "        self.layers = layers \n",
            " \n",
            "    def forward(self, input): \n",
            "        \"\"\"Just forward the input through the layers in order.\"\"\" \n",
            "        for layer in self.layers: \n",
            "            input = layer.forward(input) \n",
            "        return input \n",
            " \n",
            "    def backward(self, gradient): \n",
            "        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\" \n",
            "        for layer in reversed(self.layers): \n",
            "            gradient = layer.backward(gradient) \n",
            "        return gradient \n",
            " \n",
            "    def params(self) -> Iterable[Tensor]: \n",
            "        \"\"\"Just return the params from each layer.\"\"\" \n",
            "        return (param for layer in self.layers for param in layer.params()) \n",
            " \n",
            "    def grads(self) -> Iterable[Tensor]: \n",
            "        \"\"\"Just return the grads from each layer.\"\"\" \n",
            "        return (grad for layer in self.layers for grad in layer.grads())\n",
            "So we could represent the neural network we used for XOR as:\n",
            "xor_net = Sequential([ \n",
            "    Linear(input_dim=2, output_dim=2), \n",
            "    Sigmoid(), \n",
            "    Linear(input_dim=2, output_dim=1), \n",
            "    Sigmoid() \n",
            "])\n",
            "But we still need a little more machinery to train it.\n",
            "L o s s  a n d  O p t i m i z a t i o n\n",
            "Previously  we wrote out individual loss functions and gradient functions for\n",
            "our models. Here we’ll want to experiment with dif ferent loss functions, so\n",
            "(as usual) we’ll introduce a new Loss  abstraction that encapsulates both the\n",
            "loss computation and the gradient computation:class Loss: \n",
            "    def loss(self, predicted: Tensor, actual: Tensor) -> float: \n",
            "        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\" \n",
            "        raise NotImplementedError \n",
            " \n",
            "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor: \n",
            "        \"\"\"How does the loss change as the predictions change?\"\"\" \n",
            "        raise NotImplementedError\n",
            "W e’ve already worked many times with the loss that’ s the sum of the\n",
            "squared errors, so we should have an easy time implementing that. The only\n",
            "trick is that we’ll need to use tensor_combine :\n",
            "class SSE(Loss): \n",
            "    \"\"\"Loss function that computes the sum of the squared errors.\"\"\" \n",
            "    def loss(self, predicted: Tensor, actual: Tensor) -> float: \n",
            "        # Compute the tensor of squared differences \n",
            "        squared_errors = tensor_combine( \n",
            "            lambda predicted, actual: (predicted - actual) ** 2, \n",
            "            predicted, \n",
            "            actual) \n",
            " \n",
            "        # And just add them up \n",
            "        return tensor_sum(squared_errors) \n",
            " \n",
            "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor: \n",
            "        return tensor_combine( \n",
            "            lambda predicted, actual: 2 * (predicted - actual), \n",
            "            predicted, \n",
            "            actual)\n",
            "(W e’ll look at a dif ferent loss function in a bit.)\n",
            "The  last piece to figure out is gradient descent. Throughout the book we’ve\n",
            "done all of our gradient descent manually by having a training loop that\n",
            "involves something like:\n",
            "theta = gradient_step(theta, grad, -learning_rate)\n",
            "Here that won’ t quite work for us, for a couple reasons. The first is that our\n",
            "neural nets will have many parameters, and we’ll need to update all ofthem. The second is that we’d like to be able to use more clever variants of\n",
            "gradient descent, and we don’ t want to have to rewrite them each time.\n",
            "Accordingly , we’ll introduce a (you guessed it) Optimizer  abstraction, of\n",
            "which gradient descent will be a specific instance:\n",
            "class Optimizer: \n",
            "    \"\"\" \n",
            "    An optimizer updates the weights of a layer (in place) using information \n",
            "    known by either the layer or the optimizer (or by both). \n",
            "    \"\"\" \n",
            "    def step(self, layer: Layer) -> None: \n",
            "        raise NotImplementedError\n",
            "After that it’ s easy to implement gradient descent, again using\n",
            "tensor_combine :\n",
            "class GradientDescent(Optimizer): \n",
            "    def __init__(self, learning_rate: float = 0.1) -> None: \n",
            "        self.lr = learning_rate \n",
            " \n",
            "    def step(self, layer: Layer) -> None: \n",
            "        for param, grad in zip(layer.params(), layer.grads()): \n",
            "            # Update param using a gradient step \n",
            "            param[:] = tensor_combine( \n",
            "                lambda param, grad: param - grad * self.lr, \n",
            "                param, \n",
            "                grad)\n",
            "The only thing that’ s maybe surprising is the “slice assignment,” which is a\n",
            "reflection of the fact that reassigning a list doesn’ t change its original value.\n",
            "That is, if you just did param = tensor_combine(. . .) , you would be\n",
            "redefining the local variable param , but you would not be af fecting the\n",
            "original parameter tensor stored in the layer . If you assign to the slice [:] ,\n",
            "however , it actually changes the values inside the list.\n",
            "Here’ s a simple example to demonstrate:\n",
            "tensor = [[1, 2], [3, 4]] \n",
            " \n",
            "for row in tensor:row = [0, 0] \n",
            "assert tensor == [[1, 2], [3, 4]], \"assignment doesn't update a list\" \n",
            " \n",
            "for row in tensor: \n",
            "    row[:] = [0, 0] \n",
            "assert tensor == [[0, 0], [0, 0]], \"but slice assignment does\"\n",
            "If you are somewhat inexperienced in Python, this behavior may be\n",
            "surprising, so meditate on it and try examples yourself until it makes sense.\n",
            "T o  demonstrate the value of this abstraction, let’ s implement another\n",
            "optimizer that uses momentum . The idea is that we don’ t want to overreact\n",
            "to each new gradient, and so we maintain a running average of the gradients\n",
            "we’ve seen, updating it with each new gradient and taking a step in the\n",
            "direction of the average:\n",
            "class Momentum(Optimizer): \n",
            "    def __init__(self, \n",
            "                 learning_rate: float, \n",
            "                 momentum: float = 0.9) -> None: \n",
            "        self.lr = learning_rate \n",
            "        self.mo = momentum \n",
            "        self.updates: List[Tensor] = []  # running average \n",
            " \n",
            "    def step(self, layer: Layer) -> None: \n",
            "        # If we have no previous updates, start with all zeros \n",
            "        if not self.updates: \n",
            "            self.updates = [zeros_like(grad) for grad in layer.grads()] \n",
            " \n",
            "        for update, param, grad in zip(self.updates, \n",
            "                                       layer.params(), \n",
            "                                       layer.grads()): \n",
            "            # Apply momentum \n",
            "            update[:] = tensor_combine( \n",
            "                lambda u, g: self.mo * u + (1 - self.mo) * g, \n",
            "                update, \n",
            "                grad) \n",
            " \n",
            "            # Then take a gradient step \n",
            "            param[:] = tensor_combine( \n",
            "                lambda p, u: p - self.lr * u, \n",
            "                param, \n",
            "                update)Because we used an Optimizer  abstraction, we can easily switch between\n",
            "our dif ferent optimizers.\n",
            "E x a m p l e :  X O R  R e v i s i t e d\n",
            "Let’ s  see how easy it is to use our new framework to train a network that\n",
            "can compute XOR. W e start by re-creating the training data:\n",
            "# training data \n",
            "xs = [[0., 0], [0., 1], [1., 0], [1., 1]] \n",
            "ys = [[0.], [1.], [1.], [0.]]\n",
            "and then we define the network, although now we can leave of f the last\n",
            "sigmoid layer:\n",
            "random.seed(0) \n",
            " \n",
            "net = Sequential([ \n",
            "    Linear(input_dim=2, output_dim=2), \n",
            "    Sigmoid(), \n",
            "    Linear(input_dim=2, output_dim=1) \n",
            "])\n",
            "W e can now write a simple training loop, except that now we can use the\n",
            "abstractions of Optimizer  and Loss . This allows us to easily try dif ferent\n",
            "ones:\n",
            "import tqdm \n",
            " \n",
            "optimizer = GradientDescent(learning_rate=0.1) \n",
            "loss = SSE() \n",
            " \n",
            "with tqdm.trange(3000) as t: \n",
            "    for epoch in t: \n",
            "        epoch_loss = 0.0 \n",
            " \n",
            "        for x, y in zip(xs, ys): \n",
            "            predicted = net.forward(x) \n",
            "            epoch_loss += loss.loss(predicted, y) \n",
            "            gradient = loss.gradient(predicted, y)net.backward(gradient) \n",
            " \n",
            "            optimizer.step(net) \n",
            " \n",
            "        t.set_description(f\"xor loss {epoch_loss:.3f}\")\n",
            "This should train quickly , and you should see the loss go down. And now\n",
            "we can inspect the weights:\n",
            "for param in net.params(): \n",
            "    print(param)\n",
            "For my network I find roughly:\n",
            "hidden1 = -2.6 * x1 + -2.7 * x2 + 0.2  # NOR \n",
            "hidden2 =  2.1 * x1 +  2.1 * x2 - 3.4  # AND \n",
            "output =  -3.1 * h1 + -2.6 * h2 + 1.8  # NOR\n",
            "So hidden1  activates if neither input is 1. hidden2  activates if both inputs\n",
            "are 1. And output  activates if neither hidden output is 1—that is, if it’ s not\n",
            "the case that neither input is 1 and it’ s also not the case that both inputs are\n",
            "1. Indeed, this is exactly the logic of XOR.\n",
            "Notice that this network learned dif ferent features than the one we trained in\n",
            "Chapter 18 , but it still manages to do the same thing.\n",
            "O t h e r  A c t i v a t i o n  F u n c t i o n s\n",
            "The sigmoid  function  has fallen out of favor for a couple of reasons. One\n",
            "reason is that sigmoid(0)  equals 1/2, which means that a neuron whose\n",
            "inputs sum to 0 has a positive output. Another is that its gradient is very\n",
            "close to 0 for very lar ge and very small inputs, which means that its\n",
            "gradients can get “saturated” and its weights can get stuck.\n",
            "One  popular replacement is tanh  (“hyperbolic tangent”), which is a\n",
            "dif ferent sigmoid-shaped function that ranges from –1 to 1 and outputs 0 ifits input is 0. The derivative of tanh(x)  is just 1 - tanh(x) ** 2 , which\n",
            "makes the layer easy to write:\n",
            "import math \n",
            " \n",
            "def tanh(x: float) -> float: \n",
            "    # If x is very large or very small, tanh is (essentially) 1 or -1. \n",
            "    # We check for this because, e.g., math.exp(1000) raises an error. \n",
            "    if x < -100:  return -1 \n",
            "    elif x > 100: return 1 \n",
            " \n",
            "    em2x = math.exp(-2 * x) \n",
            "    return (1 - em2x) / (1 + em2x) \n",
            " \n",
            "class Tanh(Layer): \n",
            "    def forward(self, input: Tensor) -> Tensor: \n",
            "        # Save tanh output to use in backward pass. \n",
            "        self.tanh = tensor_apply(tanh, input) \n",
            "        return self.tanh \n",
            " \n",
            "    def backward(self, gradient: Tensor) -> Tensor: \n",
            "        return tensor_combine( \n",
            "            lambda tanh, grad: (1 - tanh ** 2) * grad, \n",
            "            self.tanh, \n",
            "            gradient)\n",
            "In lar ger networks another popular replacement is Relu , which is 0 for\n",
            "negative inputs and the identity for positive inputs:\n",
            "class Relu(Layer): \n",
            "    def forward(self, input: Tensor) -> Tensor: \n",
            "        self.input = input \n",
            "        return tensor_apply(lambda x: max(x, 0), input) \n",
            " \n",
            "    def backward(self, gradient: Tensor) -> Tensor: \n",
            "        return tensor_combine(lambda x, grad: grad if x > 0 else 0, \n",
            "                              self.input, \n",
            "                              gradient)\n",
            "There are many others. I encourage you to play around with them in your\n",
            "networks.E x a m p l e :  F i z z B u z z  R e v i s i t e d\n",
            "W e  can now use our “deep learning” framework to reproduce our solution\n",
            "from “Example: Fizz Buzz” . Let’ s set up the data:\n",
            "from scratch.neural_networks import binary_encode, fizz_buzz_encode, argmax \n",
            " \n",
            "xs = [binary_encode(n) for n in range(101, 1024)] \n",
            "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n",
            "and create the network:\n",
            "NUM_HIDDEN = 25 \n",
            " \n",
            "random.seed(0) \n",
            " \n",
            "net = Sequential([ \n",
            "    Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'), \n",
            "    Tanh(), \n",
            "    Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform'), \n",
            "    Sigmoid() \n",
            "])\n",
            "As we’re training, let’ s also track our accuracy on the training set:\n",
            "def fizzbuzz_accuracy(low: int, hi: int, net: Layer) -> float: \n",
            "    num_correct = 0 \n",
            "    for n in range(low, hi): \n",
            "        x = binary_encode(n) \n",
            "        predicted = argmax(net.forward(x)) \n",
            "        actual = argmax(fizz_buzz_encode(n)) \n",
            "        if predicted == actual: \n",
            "            num_correct += 1 \n",
            " \n",
            "    return num_correct / (hi - low)\n",
            "optimizer = Momentum(learning_rate=0.1, momentum=0.9) \n",
            "loss = SSE() \n",
            " \n",
            "with tqdm.trange(1000) as t: \n",
            "    for epoch in t: \n",
            "        epoch_loss = 0.0for x, y in zip(xs, ys): \n",
            "            predicted = net.forward(x) \n",
            "            epoch_loss += loss.loss(predicted, y) \n",
            "            gradient = loss.gradient(predicted, y) \n",
            "            net.backward(gradient) \n",
            " \n",
            "            optimizer.step(net) \n",
            " \n",
            "        accuracy = fizzbuzz_accuracy(101, 1024, net) \n",
            "        t.set_description(f\"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}\") \n",
            " \n",
            "# Now check results on the test set \n",
            "print(\"test results\", fizzbuzz_accuracy(1, 101, net))\n",
            "After 1,000 training iterations, the model gets 90% accuracy on the test set;\n",
            "if you keep training it longer , it should do even better . (I don’ t think it’ s\n",
            "possible to train to 100% accuracy with only 25 hidden units, but it’ s\n",
            "definitely possible if you go up to 50 hidden units.)\n",
            "S o f t m a x e s  a n d  C r o s s - E n t r o p y\n",
            "The  neural net we used in the previous section ended in a Sigmoid  layer ,\n",
            "which means that its output was a vector of numbers between 0 and 1. In\n",
            "particular , it could output a vector that was entirely 0s, or it could output a\n",
            "vector that was entirely 1s. Y et when we’re doing classification problems,\n",
            "we’d like to output a 1 for the correct class and a 0 for all the incorrect\n",
            "classes. Generally our predictions will not be so perfect, but we’d at least\n",
            "like to predict an actual probability distribution over the classes.\n",
            "For example, if we have two classes, and our model outputs [0, 0] , it’ s\n",
            "hard to make much sense of that. It doesn’ t think the output belongs in\n",
            "either class?\n",
            "But if our model outputs [0.4, 0.6] , we can interpret it as a prediction\n",
            "that there’ s a probability of 0.4 that our input belongs to the first class and\n",
            "0.6 that our input belongs to the second class.\n",
            "In order to accomplish this, we typically for go the final Sigmoid  layer and\n",
            "instead use the softmax  function, which converts a vector of real numbersto a vector of probabilities. W e compute exp(x)  for each number in the\n",
            "vector , which results in a vector of positive numbers. After that, we just\n",
            "divide each of those positive numbers by the sum, which gives us a bunch\n",
            "of positive numbers that add up to 1—that is, a vector of probabilities.\n",
            "If we ever end up trying to compute, say , exp(1000)  we will get a Python\n",
            "error , so before taking the exp  we subtract of f the lar gest value. This turns\n",
            "out to result in the same probabilities; it’ s just safer to compute in Python:\n",
            "def softmax(tensor: Tensor) -> Tensor: \n",
            "    \"\"\"Softmax along the last dimension\"\"\" \n",
            "    if is_1d(tensor): \n",
            "        # Subtract largest value for numerical stability. \n",
            "        largest = max(tensor) \n",
            "        exps = [math.exp(x - largest) for x in tensor] \n",
            " \n",
            "        sum_of_exps = sum(exps)                 # This is the total \"weight.\" \n",
            "        return [exp_i / sum_of_exps             # Probability is the fraction \n",
            "                for exp_i in exps]              # of the total weight. \n",
            "    else: \n",
            "        return [softmax(tensor_i) for tensor_i in tensor]\n",
            "Once our network produces probabilities, we often use a dif ferent loss\n",
            "function called cr oss-entr opy  (or sometimes “negative log likelihood”).\n",
            "Y ou may recall that in “Maximum Likelihood Estimation” , we justified the\n",
            "use of least squares in linear regression by appealing to the fact that (under\n",
            "certain assumptions) the least squares coef ficients maximized the likelihood\n",
            "of the observed data.\n",
            "Here we can do something similar: if our network outputs are probabilities,\n",
            "the cross-entropy loss represents the negative log likelihood of the observed\n",
            "data, which means that minimizing that loss is the same as maximizing the\n",
            "log likelihood (and hence the likelihood) of the training data.\n",
            "T ypically we won’ t include the softmax  function as part of the neural\n",
            "network itself. This is because it turns out that if softmax  is part of your\n",
            "loss function but not part of the network itself, the gradients of the loss with\n",
            "respect to the network outputs are very easy to compute.class SoftmaxCrossEntropy(Loss): \n",
            "    \"\"\" \n",
            "    This is the negative-log-likelihood of the observed values, given the \n",
            "    neural net model. So if we choose weights to minimize it, our model will \n",
            "    be maximizing the likelihood of the observed data. \n",
            "    \"\"\" \n",
            "    def loss(self, predicted: Tensor, actual: Tensor) -> float: \n",
            "        # Apply softmax to get probabilities \n",
            "        probabilities = softmax(predicted) \n",
            " \n",
            "        # This will be log p_i for the actual class i and 0 for the other \n",
            "        # classes. We add a tiny amount to p to avoid taking log(0). \n",
            "        likelihoods = tensor_combine(lambda p, act: math.log(p + 1e-30) * act, \n",
            "                                     probabilities, \n",
            "                                     actual) \n",
            " \n",
            "        # And then we just sum up the negatives. \n",
            "        return -tensor_sum(likelihoods) \n",
            " \n",
            "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor: \n",
            "        probabilities = softmax(predicted) \n",
            " \n",
            "        # Isn't this a pleasant equation? \n",
            "        return tensor_combine(lambda p, actual: p - actual, \n",
            "                              probabilities, \n",
            "                              actual)\n",
            "If I now train the same Fizz Buzz network using SoftmaxCrossEntropy\n",
            "loss, I find that it typically trains much faster (that is, in many fewer\n",
            "epochs). Presumably this is because it is much easier to find weights that\n",
            "softmax  to a given distribution than it is to find weights that sigmoid  to a\n",
            "given distribution.\n",
            "That is, if I need to predict class 0 (a vector with a 1 in the first position and\n",
            "0s in the remaining positions), in the linear  + sigmoid  case I need the first\n",
            "output to be a lar ge positive number and the remaining outputs to be lar ge\n",
            "negative numbers. In the softmax  case, however , I just need the first output\n",
            "to be lar ger than  the remaining outputs. Clearly there are a lot more ways\n",
            "for the second case to happen, which suggests that it should be easier to find\n",
            "weights that make it so:random.seed(0) \n",
            " \n",
            "net = Sequential([ \n",
            "    Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'), \n",
            "    Tanh(), \n",
            "    Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform') \n",
            "    # No final sigmoid layer now \n",
            "]) \n",
            " \n",
            "optimizer = Momentum(learning_rate=0.1, momentum=0.9) \n",
            "loss = SoftmaxCrossEntropy() \n",
            " \n",
            "with tqdm.trange(100) as t: \n",
            "    for epoch in t: \n",
            "        epoch_loss = 0.0 \n",
            " \n",
            "        for x, y in zip(xs, ys): \n",
            "            predicted = net.forward(x) \n",
            "            epoch_loss += loss.loss(predicted, y) \n",
            "            gradient = loss.gradient(predicted, y) \n",
            "            net.backward(gradient) \n",
            " \n",
            "            optimizer.step(net) \n",
            " \n",
            "        accuracy = fizzbuzz_accuracy(101, 1024, net) \n",
            "        t.set_description(f\"fb loss: {epoch_loss:.3f} acc: {accuracy:.2f}\") \n",
            " \n",
            "# Again check results on the test set \n",
            "print(\"test results\", fizzbuzz_accuracy(1, 101, net))\n",
            "D r o p o u t\n",
            "Like  most machine learning models, neural networks are prone to\n",
            "overfitting to their training data. W e’ve previously seen ways to ameliorate\n",
            "this; for example, in “Regularization”  we penalized lar ge weights and that\n",
            "helped prevent overfitting.\n",
            "A common way of regularizing neural networks is using dr opout . At\n",
            "training time, we randomly turn of f each neuron (that is, replace its output\n",
            "with 0) with some fixed probability . This means that the network can’ t learn\n",
            "to depend on any individual neuron, which seems to help with overfitting.At evaluation time, we don’ t want to dropout any neurons, so a Dropout\n",
            "layer will need to know whether it’ s training or not. In addition, at training\n",
            "time a Dropout  layer only passes on some random fraction of its input. T o\n",
            "make its output comparable during evaluation, we’ll scale down the outputs\n",
            "(uniformly) using that same fraction:\n",
            "class Dropout(Layer): \n",
            "    def __init__(self, p: float) -> None: \n",
            "        self.p = p \n",
            "        self.train = True \n",
            " \n",
            "    def forward(self, input: Tensor) -> Tensor: \n",
            "        if self.train: \n",
            "            # Create a mask of 0s and 1s shaped like the input \n",
            "            # using the specified probability. \n",
            "            self.mask = tensor_apply( \n",
            "                lambda _: 0 if random.random() < self.p else 1, \n",
            "                input) \n",
            "            # Multiply by the mask to dropout inputs. \n",
            "            return tensor_combine(operator.mul, input, self.mask) \n",
            "        else: \n",
            "            # During evaluation just scale down the outputs uniformly. \n",
            "            return tensor_apply(lambda x: x * (1 - self.p), input) \n",
            " \n",
            "    def backward(self, gradient: Tensor) -> Tensor: \n",
            "        if self.train: \n",
            "            # Only propagate the gradients where mask == 1. \n",
            "            return tensor_combine(operator.mul, gradient, self.mask) \n",
            "        else: \n",
            "            raise RuntimeError(\"don't call backward when not in train mode\")\n",
            "W e’ll use this to help prevent our deep learning models from overfitting.\n",
            " \n",
            "E x a m p l e :  M N I S T\n",
            "MNIST  is a  dataset of handwritten digits that everyone uses to learn deep\n",
            "learning.It is available in a somewhat tricky binary format, so we’ll install the mnist\n",
            "library to work with it. (Y es, this part is technically not “from scratch.”)\n",
            "python -m pip install mnist\n",
            "And then we can load the data:\n",
            "import mnist \n",
            " \n",
            "# This will download the data; change this to where you want it. \n",
            "# (Yes, it's a 0-argument function, that's what the library expects.) \n",
            "# (Yes, I'm assigning a lambda to a variable, like I said never to do.) \n",
            "mnist.temporary_dir = lambda: '/tmp' \n",
            " \n",
            "# Each of these functions first downloads the data and returns a numpy array. \n",
            "# We call .tolist() because our \"tensors\" are just lists. \n",
            "train_images = mnist.train_images().tolist() \n",
            "train_labels = mnist.train_labels().tolist() \n",
            " \n",
            "assert shape(train_images) == [60000, 28, 28] \n",
            "assert shape(train_labels) == [60000]\n",
            "Let’ s plot the first 100 training images to see what they look like\n",
            "( Figure 19-1 ):\n",
            "import matplotlib.pyplot as plt \n",
            " \n",
            "fig, ax = plt.subplots(10, 10) \n",
            " \n",
            "for i in range(10): \n",
            "    for j in range(10): \n",
            "        # Plot each image in black and white and hide the axes. \n",
            "        ax[i][j].imshow(train_images[10 * i + j], cmap='Greys') \n",
            "        ax[i][j].xaxis.set_visible(False) \n",
            "        ax[i][j].yaxis.set_visible(False) \n",
            " \n",
            "plt.show()Figur e 19-1. MNIST images\n",
            "Y ou can see that indeed they look like handwritten digits.\n",
            "N O T E\n",
            "My first attempt at showing the images resulted in yellow numbers on black\n",
            "backgrounds. I am neither clever nor subtle enough to know that I needed to add\n",
            "cmap=Greys  to get black-and-white images; I Googled it and found the solution on\n",
            "Stack Overflow . As a data scientist you will become quite adept at this workflow .\n",
            "W e also need to load the test images:\n",
            "test_images = mnist.test_images().tolist() \n",
            "test_labels = mnist.test_labels().tolist() \n",
            " \n",
            "assert shape(test_images) == [10000, 28, 28] \n",
            "assert shape(test_labels) == [10000]Each image is 28 × 28 pixels, but our linear layers can only deal with one-\n",
            "dimensional inputs, so we’ll just flatten them (and also divide by 256 to get\n",
            "them between 0 and 1). In addition, our neural net will train better if our\n",
            "inputs are 0 on average, so we’ll subtract out the average value:\n",
            "# Compute the average pixel value \n",
            "avg = tensor_sum(train_images) / 60000 / 28 / 28 \n",
            " \n",
            "# Recenter, rescale, and flatten \n",
            "train_images = [[(pixel - avg) / 256 for row in image for pixel in row] \n",
            "                for image in train_images] \n",
            "test_images = [[(pixel - avg) / 256 for row in image for pixel in row] \n",
            "               for image in test_images] \n",
            " \n",
            "assert shape(train_images) == [60000, 784], \"images should be flattened\" \n",
            "assert shape(test_images) == [10000, 784], \"images should be flattened\" \n",
            " \n",
            "# After centering, average pixel should be very close to 0 \n",
            "assert -0.0001 < tensor_sum(train_images) < 0.0001\n",
            "W e also want to one-hot-encode the tar gets, since we have 10 outputs. First\n",
            "let’ s write a one_hot_encode  function:\n",
            "def one_hot_encode(i: int, num_labels: int = 10) -> List[float]: \n",
            "    return [1.0 if j == i else 0.0 for j in range(num_labels)] \n",
            " \n",
            "assert one_hot_encode(3) == [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] \n",
            "assert one_hot_encode(2, num_labels=5) == [0, 0, 1, 0, 0]\n",
            "and then apply it to our data:\n",
            "train_labels = [one_hot_encode(label) for label in train_labels] \n",
            "test_labels = [one_hot_encode(label) for label in test_labels] \n",
            " \n",
            "assert shape(train_labels) == [60000, 10] \n",
            "assert shape(test_labels) == [10000, 10]\n",
            "One of the strengths of our abstractions is that we can use the same\n",
            "training/evaluation loop with a variety of models. So let’ s write that first.\n",
            "W e’ll pass it our model, the data, a loss function, and (if we’re training) an\n",
            "optimizer .It will make a pass through our data, track performance, and (if we passed\n",
            "in an optimizer) update our parameters:\n",
            "import tqdm \n",
            " \n",
            "def loop(model: Layer, \n",
            "         images: List[Tensor], \n",
            "         labels: List[Tensor], \n",
            "         loss: Loss, \n",
            "         optimizer: Optimizer = None) -> None: \n",
            "    correct = 0         # Track number of correct predictions. \n",
            "    total_loss = 0.0    # Track total loss. \n",
            " \n",
            "    with tqdm.trange(len(images)) as t: \n",
            "        for i in t: \n",
            "            predicted = model.forward(images[i])             # Predict. \n",
            "            if argmax(predicted) == argmax(labels[i]):       # Check for \n",
            "                correct += 1                                 # correctness. \n",
            "            total_loss += loss.loss(predicted, labels[i])    # Compute loss. \n",
            " \n",
            "            # If we're training, backpropagate gradient and update weights. \n",
            "            if optimizer is not None: \n",
            "                gradient = loss.gradient(predicted, labels[i]) \n",
            "                model.backward(gradient) \n",
            "                optimizer.step(model) \n",
            " \n",
            "            # And update our metrics in the progress bar. \n",
            "            avg_loss = total_loss / (i + 1) \n",
            "            acc = correct / (i + 1) \n",
            "            t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")\n",
            "As a baseline, we can use our deep learning library to train a (multiclass)\n",
            "logistic regression model, which is just a single linear layer followed by a\n",
            "softmax. This model (in essence) just looks for 10 linear functions such that\n",
            "if the input represents, say , a 5, then the 5th linear function produces the\n",
            "lar gest output.\n",
            "One pass through our 60,000 training examples should be enough to learn\n",
            "the model:\n",
            "random.seed(0) \n",
            " \n",
            "# Logistic regression is just a linear layer followed by softmaxmodel = Linear(784, 10) \n",
            "loss = SoftmaxCrossEntropy() \n",
            " \n",
            "# This optimizer seems to work \n",
            "optimizer = Momentum(learning_rate=0.01, momentum=0.99) \n",
            " \n",
            "# Train on the training data \n",
            "loop(model, train_images, train_labels, loss, optimizer) \n",
            " \n",
            "# Test on the test data (no optimizer means just evaluate) \n",
            "loop(model, test_images, test_labels, loss)\n",
            "This gets about 89% accuracy . Let’ s see if we can do better with a deep\n",
            "neural network. W e’ll use two hidden layers, the first with 30 neurons, and\n",
            "the second with 10 neurons. And we’ll use our Tanh  activation:\n",
            "random.seed(0) \n",
            " \n",
            "# Name them so we can turn train on and off \n",
            "dropout1 = Dropout(0.1) \n",
            "dropout2 = Dropout(0.1) \n",
            " \n",
            "model = Sequential([ \n",
            "    Linear(784, 30),  # Hidden layer 1: size 30 \n",
            "    dropout1, \n",
            "    Tanh(), \n",
            "    Linear(30, 10),   # Hidden layer 2: size 10 \n",
            "    dropout2, \n",
            "    Tanh(), \n",
            "    Linear(10, 10)    # Output layer: size 10 \n",
            "])\n",
            "And we can just use the same training loop!\n",
            "optimizer = Momentum(learning_rate=0.01, momentum=0.99) \n",
            "loss = SoftmaxCrossEntropy() \n",
            " \n",
            "# Enable dropout and train (takes > 20 minutes on my laptop!) \n",
            "dropout1.train = dropout2.train = True \n",
            "loop(model, train_images, train_labels, loss, optimizer) \n",
            " \n",
            "# Disable dropout and evaluate \n",
            "dropout1.train = dropout2.train = False \n",
            "loop(model, test_images, test_labels, loss)Our deep model gets better than 92% accuracy on the test set, which is a\n",
            "nice improvement from the simple logistic model.\n",
            "The MNIST website  describes a variety of models that outperform these.\n",
            "Many of them could be implemented using the machinery we’ve developed\n",
            "so far but would take an extremely long time to train in our lists-as-tensors\n",
            "framework. Some  of the best models involve convolutional  layers, which\n",
            "are important but unfortunately quite out of scope for an introductory book\n",
            "on data science.\n",
            "S a v i n g  a n d  L o a d i n g  M o d e l s\n",
            "These  models take a long time to train, so it would be nice if we could save\n",
            "them so that we don’ t have to train them every time. Luckily , we can use\n",
            "the json  module to easily serialize model weights to a file.\n",
            "For saving, we can use Layer.params  to collect the weights, stick them in a\n",
            "list, and use json.dump  to save that list to a file:\n",
            "import json \n",
            " \n",
            "def save_weights(model: Layer, filename: str) -> None: \n",
            "    weights = list(model.params()) \n",
            "    with open(filename, 'w') as f: \n",
            "        json.dump(weights, f)\n",
            "Loading the weights back is only a little more work. W e just use json.load\n",
            "to get the list of weights back from the file and slice assignment to set the\n",
            "weights of our model.\n",
            "(In particular , this means that we have to instantiate the model ourselves\n",
            "and then  load the weights. An alternative approach would be to also save\n",
            "some representation of the model architecture and use that to instantiate the\n",
            "model. That’ s not a terrible idea, but it would require a lot more code and\n",
            "changes to all our Layer s, so we’ll stick with the simpler way .)Before we load the weights, we’d like to check that they have the same\n",
            "shapes as the model params we’re loading them into. (This is a safeguard\n",
            "against, for example, trying to load the weights for a saved deep network\n",
            "into a shallow network, or similar issues.)\n",
            "def load_weights(model: Layer, filename: str) -> None: \n",
            "    with open(filename) as f: \n",
            "        weights = json.load(f) \n",
            " \n",
            "    # Check for consistency \n",
            "    assert all(shape(param) == shape(weight) \n",
            "               for param, weight in zip(model.params(), weights)) \n",
            " \n",
            "    # Then load using slice assignment \n",
            "    for param, weight in zip(model.params(), weights): \n",
            "        param[:] = weight\n",
            "N O T E\n",
            "JSON stores your data as text, which makes it an extremely inef ficient representation. In\n",
            "real applications you’d probably use the pickle  serialization library , which serializes\n",
            "things to a more ef ficient binary format. Here I decided to keep it simple and human-\n",
            "readable.\n",
            "Y ou can download the weights for the various networks we train from the\n",
            "book’ s GitHub repository .\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "Deep learning  is really hot right now , and in this chapter we barely\n",
            "scratched its surface. There are many good books and blog posts (and many ,\n",
            "many bad blog posts) about almost any aspect of deep learning you’d like to\n",
            "know about.\n",
            "The canonical textbook Deep Learning , by Ian Goodfellow ,\n",
            "Y oshua Bengio, and Aaron Courville (MIT Press), is freelyavailable online. It is very good, but it involves quite a bit of\n",
            "mathematics.\n",
            "Francois Chollet’ s Deep Learning with Python  (Manning) is a great\n",
            "introduction to the Keras library , after which our deep learning\n",
            "library is sort of patterned.\n",
            "I myself  mostly use PyT orch  for deep learning. Its website has lots\n",
            "of documentation and tutorials.Chapter 20. Clustering\n",
            "Wher e we such clusters had\n",
            "As made us nobly wild, not mad\n",
            "— Robert Herrick\n",
            "Most  of the algorithms in this book are what’ s known as supervised\n",
            "learning  algorithms, in that they start with a set of labeled data and use that\n",
            "as the basis for making predictions about new , unlabeled data. Clustering,\n",
            "however , is an example of unsupervised learning , in which we work with\n",
            "completely unlabeled data (or in which our data has labels but we ignore\n",
            "them).\n",
            "T h e  I d e a\n",
            "Whenever  you look at some source of data, it’ s likely that the data will\n",
            "somehow form clusters . A dataset showing where millionaires live probably\n",
            "has clusters in places like Beverly Hills and Manhattan. A dataset showing\n",
            "how many hours people work each week probably has a cluster around 40\n",
            "(and if it’ s taken from a state with laws mandating special benefits for\n",
            "people who work at least 20 hours a week, it probably has another cluster\n",
            "right around 19). A dataset of demographics of registered voters likely\n",
            "forms a variety of clusters (e.g., “soccer moms,” “bored retirees,”\n",
            "“unemployed millennials”) that pollsters and political consultants consider\n",
            "relevant.\n",
            "Unlike some of the problems we’ve looked at, there is generally no\n",
            "“correct” clustering. An alternative clustering scheme might group some of\n",
            "the “unemployed millennials” with “grad students,” and others with\n",
            "“parents’ basement dwellers.” Neither scheme is necessarily more correct—\n",
            "instead, each is likely more optimal with respect to its own “how good are\n",
            "the clusters?” metric.Furthermore, the clusters won’ t label themselves. Y ou’ll have to do that by\n",
            "looking at the data underlying each one.\n",
            "T h e  M o d e l\n",
            "For  us, each input  will be a vector in d -dimensional space, which, as usual,\n",
            "we will represent as a list of numbers. Our goal will be to identify clusters\n",
            "of similar inputs and (sometimes) to find a representative value for each\n",
            "cluster .\n",
            "For example, each input could be a numeric vector that represents the title\n",
            "of a blog post, in which case the goal might be to find clusters of similar\n",
            "posts, perhaps in order to understand what our users are blogging about. Or\n",
            "imagine that we have a picture containing thousands of (red, green,\n",
            "blue)  colors and that we need to screen-print a 10-color version of it.\n",
            "Clustering can help us choose 10 colors that will minimize the total “color\n",
            "error .”\n",
            "One  of the simplest clustering methods is k -means, in which the number of\n",
            "clusters k  is chosen in advance, after which the goal is to partition the inputs\n",
            "into sets S1,...,Sk  in a way that minimizes the total sum of squared\n",
            "distances from each point to the mean of its assigned cluster .\n",
            "There are a lot of ways to assign n  points to k  clusters, which means that\n",
            "finding an optimal clustering is a very hard problem. W e’ll settle for an\n",
            "iterative algorithm that usually finds a good clustering:\n",
            "1 . Start with a set of k -means, which are points in d -dimensional\n",
            "space.\n",
            "2 . Assign each point to the mean to which it is closest.\n",
            "3 . If no point’ s assignment has changed, stop and keep the clusters.\n",
            "4 . If some point’ s assignment has changed, recompute the means and\n",
            "return to step 2.Using the vector_mean  function from Chapter 4 , it’ s pretty simple to create\n",
            "a class that does this.\n",
            "T o start with, we’ll create a helper function that measures how many\n",
            "coordinates two vectors dif fer in. W e’ll use this to track our training\n",
            "progress:\n",
            "from scratch.linear_algebra import Vector \n",
            " \n",
            "def num_differences(v1: Vector, v2: Vector) -> int: \n",
            "    assert len(v1) == len(v2) \n",
            "    return len([x1 for x1, x2 in zip(v1, v2) if x1 != x2]) \n",
            " \n",
            "assert num_differences([1, 2, 3], [2, 1, 3]) == 2 \n",
            "assert num_differences([1, 2], [1, 2]) == 0\n",
            "W e also need a function that, given some vectors and their assignments to\n",
            "clusters, computes the means of the clusters. It may be the case that some\n",
            "cluster has no points assigned to it. W e can’ t take the mean of an empty\n",
            "collection, so in that case we’ll just randomly pick one of the points to serve\n",
            "as the “mean” of that cluster:\n",
            "from typing import List \n",
            "from scratch.linear_algebra import vector_mean \n",
            " \n",
            "def cluster_means(k: int, \n",
            "                  inputs: List[Vector], \n",
            "                  assignments: List[int]) -> List[Vector]: \n",
            "    # clusters[i] contains the inputs whose assignment is i \n",
            "    clusters = [[] for i in range(k)] \n",
            "    for input, assignment in zip(inputs, assignments): \n",
            "        clusters[assignment].append(input) \n",
            " \n",
            "    # if a cluster is empty, just use a random point \n",
            "    return [vector_mean(cluster) if cluster else random.choice(inputs) \n",
            "            for cluster in clusters]\n",
            "And now we’re ready to code up our clusterer . As usual, we’ll use tqdm  to\n",
            "track our progress, but here we don’ t know how many iterations it will take,\n",
            "so we then use itertools.count , which creates an infinite iterable, and\n",
            "we’ll return  out of it when we’re done:import itertools \n",
            "import random \n",
            "import tqdm \n",
            "from scratch.linear_algebra import squared_distance \n",
            " \n",
            "class KMeans: \n",
            "    def __init__(self, k: int) -> None: \n",
            "        self.k = k                      # number of clusters \n",
            "        self.means = None \n",
            " \n",
            "    def classify(self, input: Vector) -> int: \n",
            "        \"\"\"return the index of the cluster closest to the input\"\"\" \n",
            "        return min(range(self.k), \n",
            "                   key=lambda i: squared_distance(input, self.means[i])) \n",
            " \n",
            "    def train(self, inputs: List[Vector]) -> None: \n",
            "        # Start with random assignments \n",
            "        assignments = [random.randrange(self.k) for _ in inputs] \n",
            " \n",
            "        with tqdm.tqdm(itertools.count()) as t: \n",
            "            for _ in t: \n",
            "                # Compute means and find new assignments \n",
            "                self.means = cluster_means(self.k, inputs, assignments) \n",
            "                new_assignments = [self.classify(input) for input in inputs] \n",
            " \n",
            "                # Check how many assignments changed and if we're done \n",
            "                num_changed = num_differences(assignments, new_assignments) \n",
            "                if num_changed == 0: \n",
            "                    return \n",
            " \n",
            "                # Otherwise keep the new assignments, and compute new means \n",
            "                assignments = new_assignments \n",
            "                self.means = cluster_means(self.k, inputs, assignments) \n",
            "                t.set_description(f\"changed: {num_changed} / {len(inputs)}\")\n",
            "Let’ s take a look at how this works.\n",
            "E x a m p l e :  M e e t u p s\n",
            "T o  celebrate DataSciencester ’ s growth, your VP of User Rewards wants to\n",
            "or ganize several in-person meetups for your hometown users, complete\n",
            "with beer , pizza, and DataSciencester t-shirts. Y ou know the locations of allyour local users ( Figure 20-1 ), and she’d like you to choose meetup\n",
            "locations that make it convenient for everyone to attend.\n",
            "Figur e 20-1. The locations of your hometown users\n",
            "Depending on how you look at it, you probably see two or three clusters.\n",
            "(It’ s easy to do visually because the data is only two-dimensional. W ith\n",
            "more dimensions, it would be a lot harder to eyeball.)\n",
            "Imagine first that she has enough budget for three meetups. Y ou go to your\n",
            "computer and try this:\n",
            "random.seed(12)                   # so you get the same results as me \n",
            "clusterer = KMeans(k=3) \n",
            "clusterer.train(inputs) \n",
            "means = sorted(clusterer.means)   # sort for the unit test \n",
            " \n",
            "assert len(means) == 3# Check that the means are close to what we expect \n",
            "assert squared_distance(means[0], [-44, 5]) < 1 \n",
            "assert squared_distance(means[1], [-16, -10]) < 1 \n",
            "assert squared_distance(means[2], [18, 20]) < 1\n",
            "Y ou find three clusters centered at [–44, 5], [–16, –10], and [18, 20], and\n",
            "you look for meetup venues near those locations ( Figure 20-2 ).\n",
            "Figur e 20-2. User locations gr ouped into thr ee clusters\n",
            "Y ou show your results to the VP , who informs you that now she only has\n",
            "enough budgeted for two  meetups.\n",
            "“No problem,” you say:\n",
            "random.seed(0) \n",
            "clusterer = KMeans(k=2) \n",
            "clusterer.train(inputs) \n",
            "means = sorted(clusterer.means)assert len(means) == 2 \n",
            "assert squared_distance(means[0], [-26, -5]) < 1 \n",
            "assert squared_distance(means[1], [18, 20]) < 1\n",
            "As shown in Figure 20-3 , one meetup should still be near [18, 20], but now\n",
            "the other should be near [–26, –5].\n",
            "Figur e 20-3. User locations gr ouped into two clusters\n",
            "C h o o s i n g  k\n",
            "In  the previous example, the choice of k  was driven by factors outside of\n",
            "our control. In general, this won’ t be the case. There are various ways to\n",
            "choose a k . One that’ s reasonably easy to understand involves plotting the\n",
            "sum of squared errors (between each point and the mean of its cluster) as a\n",
            "function of k  and looking at where the graph “bends”:from matplotlib import pyplot as plt \n",
            " \n",
            "def squared_clustering_errors(inputs: List[Vector], k: int) -> float: \n",
            "    \"\"\"finds the total squared error from k-means clustering the inputs\"\"\" \n",
            "    clusterer = KMeans(k) \n",
            "    clusterer.train(inputs) \n",
            "    means = clusterer.means \n",
            "    assignments = [clusterer.classify(input) for input in inputs] \n",
            " \n",
            "    return sum(squared_distance(input, means[cluster]) \n",
            "               for input, cluster in zip(inputs, assignments))\n",
            "which we can apply to our previous example:\n",
            "# now plot from 1 up to len(inputs) clusters \n",
            " \n",
            "ks = range(1, len(inputs) + 1) \n",
            "errors = [squared_clustering_errors(inputs, k) for k in ks] \n",
            " \n",
            "plt.plot(ks, errors) \n",
            "plt.xticks(ks) \n",
            "plt.xlabel(\"k\") \n",
            "plt.ylabel(\"total squared error\") \n",
            "plt.title(\"Total Error vs. # of Clusters\") \n",
            "plt.show()\n",
            "Looking at Figure 20-4 , this method agrees with our original eyeballing that\n",
            "three is the “right” number of clusters.Figur e 20-4. Choosing a k\n",
            "E x a m p l e :  C l u s t e r i n g  C o l o r s\n",
            "The  VP of Swag has designed attractive DataSciencester stickers that he’d\n",
            "like you to hand out at meetups. Unfortunately , your sticker printer can\n",
            "print at most five colors per sticker . And since the VP of Art is on\n",
            "sabbatical, the VP of Swag asks if there’ s some way you can take his design\n",
            "and modify it so that it contains only five colors.\n",
            "Computer images can be represented as two-dimensional arrays of pixels,\n",
            "where each pixel is itself a three-dimensional vector (red, green, blue)\n",
            "indicating its color .\n",
            "Creating a five-color version of the image, then, entails:\n",
            "1 . Choosing five colors.2 . Assigning one of those colors to each pixel.\n",
            "It turns out this is a great task for k -means clustering, which can partition\n",
            "the pixels into five clusters in red-green-blue space. If we then recolor the\n",
            "pixels in each cluster to the mean color , we’re done.\n",
            "T o start with, we’ll need a way to load an image into Python. W e can do this\n",
            "with matplotlib, if we first install the pillow library:\n",
            "python -m pip install pillow\n",
            "Then we can just use matplotlib.image.imread :\n",
            "image_path = r\"girl_with_book.jpg\"    # wherever your image is \n",
            "import matplotlib.image as mpimg \n",
            "img = mpimg.imread(image_path) / 256  # rescale to between 0 and 1\n",
            "Behind the scenes img  is a NumPy array , but for our purposes, we can treat\n",
            "it as a list of lists of lists.\n",
            "img[i][j]  is the pixel in the i th row and j th column, and each pixel is a list\n",
            "[red, green, blue]  of numbers between 0 and 1 indicating the color of\n",
            "that pixel :\n",
            "top_row = img[0] \n",
            "top_left_pixel = top_row[0] \n",
            "red, green, blue = top_left_pixel\n",
            "In particular , we can get a flattened list of all the pixels as:\n",
            "# .tolist() converts a NumPy array to a Python list \n",
            "pixels = [pixel.tolist() for row in img for pixel in row]\n",
            "and then feed them to our clusterer:\n",
            "clusterer = KMeans(5) \n",
            "clusterer.train(pixels)   # this might take a whileOnce it finishes, we just construct a new image with the same format:\n",
            "def recolor(pixel: Vector) -> Vector: \n",
            "    cluster = clusterer.classify(pixel)        # index of the closest cluster \n",
            "    return clusterer.means[cluster]            # mean of the closest cluster \n",
            " \n",
            "new_img = [[recolor(pixel) for pixel in row]   # recolor this row of pixels \n",
            "           for row in img]                     # for each row in the image\n",
            "and display it, using plt.imshow :\n",
            "plt.imshow(new_img) \n",
            "plt.axis('off') \n",
            "plt.show()\n",
            "It is dif ficult to show color results in a black-and-white book, but Figure 20-\n",
            "5  shows grayscale versions of a full-color picture and the output of using\n",
            "this process to reduce it to five colors.\n",
            "Figur e 20-5. Original pictur e and its 5-means decoloring\n",
            "B o t t o m - U p  H i e r a r c h i c a l  C l u s t e r i n g\n",
            "An  alternative approach to clustering is to “grow” clusters from the bottom\n",
            "up. W e can do this in the following way:1 . Make each input its own cluster of one.\n",
            "2 . As long as there are multiple clusters remaining, find the two\n",
            "closest clusters and mer ge them.\n",
            "At the end, we’ll have one giant cluster containing all the inputs. If we keep\n",
            "track of the mer ge order , we can re-create any number of clusters by\n",
            "unmer ging. For example, if we want three clusters, we can just undo the last\n",
            "two mer ges.\n",
            "W e’ll use a really simple representation of clusters. Our values will live in\n",
            "leaf  clusters, which we will represent as NamedTuple s:\n",
            "from typing import NamedTuple, Union \n",
            " \n",
            "class Leaf(NamedTuple): \n",
            "    value: Vector \n",
            " \n",
            "leaf1 = Leaf([10,  20]) \n",
            "leaf2 = Leaf([30, -15])\n",
            "W e’ll use these to grow mer ged  clusters, which we will also represent as\n",
            "NamedTuple s:\n",
            "class Merged(NamedTuple): \n",
            "    children: tuple \n",
            "    order: int \n",
            " \n",
            "merged = Merged((leaf1, leaf2), order=1) \n",
            " \n",
            "Cluster = Union[Leaf, Merged]\n",
            "N O T E\n",
            "This is another case where Python’ s type annotations have let us down. Y ou’d like to\n",
            "type hint Merged.children  as Tuple[Cluster, Cluster]  but mypy  doesn’ t allow\n",
            "recursive types like that.W e’ll talk about mer ge order in a bit, but first let’ s create a helper function\n",
            "that recursively returns all the values contained in a (possibly mer ged)\n",
            "cluster:\n",
            "def get_values(cluster: Cluster) -> List[Vector]: \n",
            "    if isinstance(cluster, Leaf): \n",
            "        return [cluster.value] \n",
            "    else: \n",
            "        return [value \n",
            "                for child in cluster.children \n",
            "                for value in get_values(child)] \n",
            " \n",
            "assert get_values(merged) == [[10, 20], [30, -15]]\n",
            "In order to mer ge the closest clusters, we need some notion of the distance\n",
            "between clusters. W e’ll use the minimum  distance between elements of the\n",
            "two clusters, which mer ges the two clusters that are closest to touching (but\n",
            "will sometimes produce lar ge chain-like clusters that aren’ t very tight). If\n",
            "we wanted tight spherical clusters, we might use the maximum  distance\n",
            "instead, as it mer ges the two clusters that fit in the smallest ball. Both are\n",
            "common choices, as is the average  distance:\n",
            "from typing import Callable \n",
            "from scratch.linear_algebra import distance \n",
            " \n",
            "def cluster_distance(cluster1: Cluster, \n",
            "                     cluster2: Cluster, \n",
            "                     distance_agg: Callable = min) -> float: \n",
            "    \"\"\" \n",
            "    compute all the pairwise distances between cluster1 and cluster2 \n",
            "    and apply the aggregation function _distance_agg_ to the resulting list \n",
            "    \"\"\" \n",
            "    return distance_agg([distance(v1, v2) \n",
            "                         for v1 in get_values(cluster1) \n",
            "                         for v2 in get_values(cluster2)])\n",
            "W e’ll use the mer ge order slot to track the order in which we did the\n",
            "mer ging. Smaller numbers will represent later  mer ges. This means when we\n",
            "want to unmer ge clusters, we do so from lowest mer ge order to highest.\n",
            "Since Leaf  clusters were never mer ged, we’ll assign them infinity , thehighest possible value. And since they don’ t have an .order  property , we’ll\n",
            "create a helper function:\n",
            "def get_merge_order(cluster: Cluster) -> float: \n",
            "    if isinstance(cluster, Leaf): \n",
            "        return float('inf')  # was never merged \n",
            "    else: \n",
            "        return cluster.order\n",
            "Similarly , since Leaf  clusters don’ t have children, we’ll create and add a\n",
            "helper function for that:\n",
            "from typing import Tuple \n",
            " \n",
            "def get_children(cluster: Cluster): \n",
            "    if isinstance(cluster, Leaf): \n",
            "        raise TypeError(\"Leaf has no children\") \n",
            "    else: \n",
            "        return cluster.children\n",
            "Now we’re ready to create the clustering algorithm:\n",
            "def bottom_up_cluster(inputs: List[Vector], \n",
            "                      distance_agg: Callable = min) -> Cluster: \n",
            "    # Start with all leaves \n",
            "    clusters: List[Cluster] = [Leaf(input) for input in inputs] \n",
            " \n",
            "    def pair_distance(pair: Tuple[Cluster, Cluster]) -> float: \n",
            "        return cluster_distance(pair[0], pair[1], distance_agg) \n",
            " \n",
            "    # as long as we have more than one cluster left... \n",
            "    while len(clusters) > 1: \n",
            "        # find the two closest clusters \n",
            "        c1, c2 = min(((cluster1, cluster2) \n",
            "                      for i, cluster1 in enumerate(clusters) \n",
            "                      for cluster2 in clusters[:i]), \n",
            "                      key=pair_distance) \n",
            " \n",
            "        # remove them from the list of clusters \n",
            "        clusters = [c for c in clusters if c != c1 and c != c2] \n",
            " \n",
            "        # merge them, using merge_order = # of clusters left \n",
            "        merged_cluster = Merged((c1, c2), order=len(clusters))# and add their merge \n",
            "        clusters.append(merged_cluster) \n",
            " \n",
            "    # when there's only one cluster left, return it \n",
            "    return clusters[0]\n",
            "Its use is very simple:\n",
            "base_cluster = bottom_up_cluster(inputs)\n",
            "This produces a clustering that looks as follows:\n",
            "  0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 \n",
            "──┬──┬─────┬────────────────────────────────┬───────────┬─ [19, 28] \n",
            "  │  │     │                                │           └─ [21, 27] \n",
            "  │  │     │                                └─ [20, 23] \n",
            "  │  │     └─ [26, 13] \n",
            "  │  └────────────────────────────────────────────┬─ [11, 15] \n",
            "  │                                               └─ [13, 13] \n",
            "  └─────┬─────┬──┬───────────┬─────┬─ [-49, 0] \n",
            "        │     │  │           │     └─ [-46, 5] \n",
            "        │     │  │           └─ [-41, 8] \n",
            "        │     │  └─ [-49, 15] \n",
            "        │     └─ [-34, 1] \n",
            "        └───────────┬──┬──┬─────┬─ [-22, -16] \n",
            "                    │  │  │     └─ [-19, -11] \n",
            "                    │  │  └─ [-25, -9] \n",
            "                    │  └─────────────────┬─────┬─────┬─ [-11, -6] \n",
            "                    │                    │     │     └─ [-12, -8] \n",
            "                    │                    │     └─ [-14, 5] \n",
            "                    │                    └─ [-18, -3] \n",
            "                    └─────────────────┬─ [-13, -19] \n",
            "                                      └─ [-9, -16]\n",
            "The numbers at the top indicate “mer ge order .” Since we had 20 inputs, it\n",
            "took 19 mer ges to get to this one cluster . The first mer ge created cluster 18\n",
            "by combining the leaves [19, 28] and [21, 27]. And the last mer ge created\n",
            "cluster 0.\n",
            "If you wanted only two clusters, you’d split at the first fork (“0”), creating\n",
            "one cluster with six points and a second with the rest. For three clusters,\n",
            "you’d continue to the second fork (“1”), which indicates to split that firstcluster into the cluster with ([19, 28], [21, 27], [20, 23], [26, 13]) and the\n",
            "cluster with ([1 1, 15], [13, 13]). And so on.\n",
            "Generally , though, we don’ t want to be squinting at nasty text\n",
            "representations like this. Instead, let’ s write a function that generates any\n",
            "number of clusters by performing the appropriate number of unmer ges:\n",
            "def generate_clusters(base_cluster: Cluster, \n",
            "                      num_clusters: int) -> List[Cluster]: \n",
            "    # start with a list with just the base cluster \n",
            "    clusters = [base_cluster] \n",
            " \n",
            "    # as long as we don't have enough clusters yet... \n",
            "    while len(clusters) < num_clusters: \n",
            "        # choose the last-merged of our clusters \n",
            "        next_cluster = min(clusters, key=get_merge_order) \n",
            "        # remove it from the list \n",
            "        clusters = [c for c in clusters if c != next_cluster] \n",
            " \n",
            "        # and add its children to the list (i.e., unmerge it) \n",
            "        clusters.extend(get_children(next_cluster)) \n",
            " \n",
            "    # once we have enough clusters... \n",
            "    return clusters\n",
            "So, for example, if we want to generate three clusters, we can just do:\n",
            "three_clusters = [get_values(cluster) \n",
            "                  for cluster in generate_clusters(base_cluster, 3)]\n",
            "which we can easily plot:\n",
            "for i, cluster, marker, color in zip([1, 2, 3], \n",
            "                                     three_clusters, \n",
            "                                     ['D','o','*'], \n",
            "                                     ['r','g','b']): \n",
            "    xs, ys = zip(*cluster)  # magic unzipping trick \n",
            "    plt.scatter(xs, ys, color=color, marker=marker) \n",
            " \n",
            "    # put a number at the mean of the cluster \n",
            "    x, y = vector_mean(cluster) \n",
            "    plt.plot(x, y, marker='$' + str(i) + '$', color='black')plt.title(\"User Locations -- 3 Bottom-Up Clusters, Min\") \n",
            "plt.xlabel(\"blocks east of city center\") \n",
            "plt.ylabel(\"blocks north of city center\") \n",
            "plt.show()\n",
            "This gives very dif ferent results than k -means did, as shown in Figure 20-6 .\n",
            "Figur e 20-6. Thr ee bottom-up clusters using min distance\n",
            "As mentioned previously , this is because using min  in cluster_distance\n",
            "tends to give chain-like clusters. If we instead use max  (which gives tight\n",
            "clusters), it looks the same as the 3-means result ( Figure 20-7 ).N O T E\n",
            "The previous bottom_up_clustering  implementation is relatively simple, but also\n",
            "shockingly inef ficient. In particular , it recomputes the distance between each pair of\n",
            "inputs at every step. A more ef ficient implementation might instead precompute the\n",
            "distances between each pair of inputs and then perform a lookup inside\n",
            "cluster_distance . A r eally  ef ficient implementation would likely also remember the\n",
            "cluster_distance s from the previous step.\n",
            "Figur e 20-7. Thr ee bottom-up clusters using max distance\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "scikit-learn has  an entire module, sklearn.cluster , that contains\n",
            "several clustering algorithms including KMeans  and the Wardhierarchical clustering algorithm (which uses a dif ferent criterion\n",
            "for mer ging clusters than ours did).\n",
            "SciPy  has two  clustering models: scipy.cluster.vq , which does\n",
            "k -means, and scipy.cluster.hierarchy , which has a variety of\n",
            "hierarchical clustering algorithms.Chapter 21. Natural Language\n",
            "Processing\n",
            "They have been at a gr eat feast of languages, and stolen the scraps.\n",
            "— W illiam Shakespeare\n",
            "Natural language pr ocessing  (NLP) refers  to computational techniques\n",
            "involving language. It’ s a broad field, but we’ll look at a few techniques,\n",
            "both simple and not simple.\n",
            "W o r d  C l o u d s\n",
            "In Chapter 1 , we  computed word counts of users’ interests. One approach to\n",
            "visualizing words and counts is wor d clouds , which artistically depict the\n",
            "words at sizes proportional to their counts.\n",
            "Generally , though, data scientists don’ t think much of word clouds, in lar ge\n",
            "part because the placement of the words doesn’ t mean anything other than\n",
            "“here’ s some space where I was able to fit a word.”\n",
            "If you ever are forced to create a word cloud, think about whether you can\n",
            "make the axes convey something. For example, imagine that, for each of\n",
            "some collection of data science–related buzzwords, you have two numbers\n",
            "between 0 and 100—the first representing how frequently it appears in job\n",
            "postings, and the second how frequently it appears on résumés:\n",
            "data = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50), \n",
            "         (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60), \n",
            "         (\"data science\", 60, 70), (\"analytics\", 90, 3), \n",
            "         (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0), \n",
            "         (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10), \n",
            "         (\"self-starter\", 30, 50), (\"customer focus\", 65, 15), \n",
            "         (\"thought leadership\", 35, 35)]The  word cloud approach is just to arrange the words on a page in a cool-\n",
            "looking font ( Figure 21-1 ).\n",
            "Figur e 21-1. Buzzwor d cloud\n",
            "This looks neat but doesn’ t really tell us anything. A more interesting\n",
            "approach might be to scatter them so that horizontal position indicates\n",
            "posting popularity and vertical position indicates résumé popularity , which\n",
            "produces a visualization that conveys a few insights ( Figure 21-2 ):\n",
            "from matplotlib import pyplot as plt \n",
            " \n",
            "def text_size(total: int) -> float: \n",
            "    \"\"\"equals 8 if total is 0, 28 if total is 200\"\"\" \n",
            "    return 8 + total / 200 * 20 \n",
            " \n",
            "for word, job_popularity, resume_popularity in data: \n",
            "    plt.text(job_popularity, resume_popularity, word, \n",
            "             ha='center', va='center', \n",
            "             size=text_size(job_popularity + resume_popularity)) \n",
            "plt.xlabel(\"Popularity on Job Postings\") \n",
            "plt.ylabel(\"Popularity on Resumes\")plt.axis([0, 100, 0, 100]) \n",
            "plt.xticks([]) \n",
            "plt.yticks([]) \n",
            "plt.show()\n",
            "Figur e 21-2. A mor e meaningful (if less attractive) wor d cloud\n",
            "n - G r a m  L a n g u a g e  M o d e l s\n",
            "The  DataSciencester VP of Search Engine Marketing wants to create\n",
            "thousands of web pages about data science so that your site will rank higher\n",
            "in search results for data science–related terms. (Y ou attempt to explain to\n",
            "her that search engine algorithms are clever enough that this won’ t actually\n",
            "work, but she refuses to listen.)\n",
            "Of course, she doesn’ t want to write thousands of web pages, nor does she\n",
            "want to pay a horde of “content strategists” to do so. Instead, she asks you\n",
            "whether you can somehow programmatically generate these web pages. T o\n",
            "do this, we’ll need some way of modeling language.One approach is to start with a corpus of documents and learn  a statistical\n",
            "model of language. In our case, we’ll start with Mike Loukides’ s essay\n",
            "“What Is Data Science?”\n",
            "As in Chapter 9 , we’ll use the Requests and Beautiful Soup libraries to\n",
            "retrieve the data. There are a couple of issues worth calling attention to.\n",
            "The first is that the apostrophes in the text are actually the Unicode\n",
            "character u\"\\u2019\" . W e’ll create a helper function to replace them with\n",
            "normal apostrophes:\n",
            "def fix_unicode(text: str) -> str: \n",
            "    return text.replace(u\"\\u2019\", \"'\")\n",
            "The second issue is that once we get the text of the web page, we’ll want to\n",
            "split it into a sequence of words and periods (so that we can tell where\n",
            "sentences end). W e can do this using re.findall :\n",
            "import re \n",
            "from bs4 import BeautifulSoup \n",
            "import requests \n",
            " \n",
            "url = \"https://www.oreilly.com/ideas/what-is-data-science\" \n",
            "html = requests.get(url).text \n",
            "soup = BeautifulSoup(html, 'html5lib') \n",
            " \n",
            "content = soup.find(\"div\", \"article-body\")   # find article-body div \n",
            "regex = r\"[\\w']+|[\\.]\"                       # matches a word or a period \n",
            " \n",
            "document = [] \n",
            " \n",
            "for paragraph in content(\"p\"): \n",
            "    words = re.findall(regex, fix_unicode(paragraph.text)) \n",
            "    document.extend(words)\n",
            "W e certainly could (and likely should) clean this data further . There is still\n",
            "some amount of extraneous text in the document (for example, the first\n",
            "word is Section ), and we’ve split on midsentence periods (for example, in\n",
            "W eb 2.0 ), and there are a handful of captions and lists sprinkled throughout.\n",
            "Having said that, we’ll work with the document as it is.Now that we have the text as a sequence of words, we can model language\n",
            "in the following way: given some starting word (say , book ) we look at all\n",
            "the words that follow it in the source document. W e randomly choose one\n",
            "of these to be the next word, and we repeat the process until we get to a\n",
            "period, which signifies the end of the sentence. W e  call this a bigram model ,\n",
            "as it is determined completely by the frequencies of the bigrams (word\n",
            "pairs) in the original data.\n",
            "What about a starting word? W e can just pick randomly from words that\n",
            "follow  a period. T o start, let’ s precompute the possible word transitions.\n",
            "Recall that zip  stops when any of its inputs is done, so that zip(document,\n",
            "document[1:])  gives us precisely the pairs of consecutive elements of\n",
            "document :\n",
            "from collections import defaultdict \n",
            " \n",
            "transitions = defaultdict(list) \n",
            "for prev, current in zip(document, document[1:]): \n",
            "    transitions[prev].append(current)\n",
            "Now we’re ready to generate sentences:\n",
            "def generate_using_bigrams() -> str: \n",
            "    current = \".\"   # this means the next word will start a sentence \n",
            "    result = [] \n",
            "    while True: \n",
            "        next_word_candidates = transitions[current]    # bigrams (current, _) \n",
            "        current = random.choice(next_word_candidates)  # choose one at random \n",
            "        result.append(current)                         # append it to results \n",
            "        if current == \".\": return \" \".join(result)     # if \".\" we're done\n",
            "The sentences it produces are gibberish, but they’re the kind of gibberish\n",
            "you might put on your website if you were trying to sound data-sciencey .\n",
            "For example:If you may know which ar e you want to data sort the data feeds web\n",
            "friend someone on tr ending topics as the data in Hadoop is the data\n",
            "science r equir es a book demonstrates why visualizations ar e but we do\n",
            "massive corr elations acr oss many commer cial disk drives in Python\n",
            "language and cr eates mor e tractable form making connections then use\n",
            "and uses it to solve a data.\n",
            "— Bigram Model\n",
            "W e  can make the sentences less gibberishy by looking at trigrams , triplets\n",
            "of consecutive words. (More generally , you might look at n-grams\n",
            "consisting of n  consecutive words, but three will be plenty for us.) Now the\n",
            "transitions will depend on the previous two  words:\n",
            "trigram_transitions = defaultdict(list) \n",
            "starts = [] \n",
            " \n",
            "for prev, current, next in zip(document, document[1:], document[2:]): \n",
            " \n",
            "    if prev == \".\":              # if the previous \"word\" was a period \n",
            "        starts.append(current)   # then this is a start word \n",
            " \n",
            "    trigram_transitions[(prev, current)].append(next)\n",
            "Notice that now we have to track the starting words separately . W e can\n",
            "generate sentences in pretty much the same way:\n",
            "def generate_using_trigrams() -> str: \n",
            "    current = random.choice(starts)   # choose a random starting word \n",
            "    prev = \".\"                        # and precede it with a '.' \n",
            "    result = [current] \n",
            "    while True: \n",
            "        next_word_candidates = trigram_transitions[(prev, current)] \n",
            "        next_word = random.choice(next_word_candidates) \n",
            " \n",
            "        prev, current = current, next_word \n",
            "        result.append(current) \n",
            " \n",
            "        if current == \".\": \n",
            "            return \" \".join(result)\n",
            "This produces better sentences like:In hindsight MapReduce seems like an epidemic and if so does that give\n",
            "us new insights into how economies work That’ s not a question we could\n",
            "even have asked a few years ther e has been instrumented.\n",
            "— T rigram Model\n",
            "Of course, they sound better because at each step the generation process has\n",
            "fewer choices, and at many steps only a single choice. This means that we\n",
            "frequently generate sentences (or at least long phrases) that were seen\n",
            "verbatim in the original data. Having more data would help; it would also\n",
            "work better if we collected n -grams from multiple essays about data\n",
            "science.\n",
            "G r a m m a r s\n",
            "A  dif ferent approach to modeling language is with grammars , rules for\n",
            "generating acceptable sentences. In elementary school, you probably\n",
            "learned about parts of speech and how to combine them. For example, if\n",
            "you had a really bad English teacher , you might say that a sentence\n",
            "necessarily consists of a noun  followed by a verb . If you then have a list of\n",
            "nouns and verbs, you can generate sentences according to the rule.\n",
            "W e’ll define a slightly more complicated grammar:\n",
            "from typing import List, Dict \n",
            " \n",
            "# Type alias to refer to grammars later \n",
            "Grammar = Dict[str, List[str]] \n",
            " \n",
            "grammar = { \n",
            "    \"_S\"  : [\"_NP _VP\"], \n",
            "    \"_NP\" : [\"_N\", \n",
            "             \"_A _NP _P _A _N\"], \n",
            "    \"_VP\" : [\"_V\", \n",
            "             \"_V _NP\"], \n",
            "    \"_N\"  : [\"data science\", \"Python\", \"regression\"], \n",
            "    \"_A\"  : [\"big\", \"linear\", \"logistic\"], \n",
            "    \"_P\"  : [\"about\", \"near\"], \n",
            "    \"_V\"  : [\"learns\", \"trains\", \"tests\", \"is\"] \n",
            "}I made up the convention that names starting with underscores refer to rules\n",
            "that need further expanding, and that other names are terminals  that don’ t\n",
            "need further processing.\n",
            "So, for example, \"_S\"  is the “sentence” rule, which produces an \"_NP\"\n",
            "(“noun phrase”) rule followed by a \"_VP\"  (“verb phrase”) rule.\n",
            "The verb phrase rule can produce either the \"_V\"  (“verb”) rule, or the verb\n",
            "rule followed by the noun phrase rule.\n",
            "Notice that the \"_NP\"  rule contains itself in one of its productions.\n",
            "Grammars can be recursive, which allows even finite grammars like this to\n",
            "generate infinitely many dif ferent sentences.\n",
            "How do we generate sentences from this grammar? W e’ll start with a list\n",
            "containing the sentence rule [\"_S\"] . And then we’ll repeatedly expand\n",
            "each rule by replacing it with a randomly chosen one of its productions. W e\n",
            "stop when we have a list consisting solely of terminals.\n",
            "For example, one such progression might look like:\n",
            "['_S'] \n",
            "['_NP','_VP'] \n",
            "['_N','_VP'] \n",
            "['Python','_VP'] \n",
            "['Python','_V','_NP'] \n",
            "['Python','trains','_NP'] \n",
            "['Python','trains','_A','_NP','_P','_A','_N'] \n",
            "['Python','trains','logistic','_NP','_P','_A','_N'] \n",
            "['Python','trains','logistic','_N','_P','_A','_N'] \n",
            "['Python','trains','logistic','data science','_P','_A','_N'] \n",
            "['Python','trains','logistic','data science','about','_A', '_N'] \n",
            "['Python','trains','logistic','data science','about','logistic','_N'] \n",
            "['Python','trains','logistic','data science','about','logistic','Python']\n",
            "How do we implement this? W ell, to start, we’ll create a simple helper\n",
            "function to identify terminals:\n",
            "def is_terminal(token: str) -> bool: \n",
            "    return token[0] != \"_\"Next we need to write a function to turn a list of tokens into a sentence.\n",
            "W e’ll look for the first nonterminal token. If we can’ t find one, that means\n",
            "we have a completed sentence and we’re done.\n",
            "If we do find a nonterminal, then we randomly choose one of its\n",
            "productions. If that production is a terminal (i.e., a word), we simply\n",
            "replace the token with it. Otherwise, it’ s a sequence of space-separated\n",
            "nonterminal tokens that we need to split  and then splice into the current\n",
            "tokens. Either way , we repeat the process on the new set of tokens.\n",
            "Putting it all together , we get:\n",
            "def expand(grammar: Grammar, tokens: List[str]) -> List[str]: \n",
            "    for i, token in enumerate(tokens): \n",
            "        # If this is a terminal token, skip it. \n",
            "        if is_terminal(token): continue \n",
            " \n",
            "        # Otherwise, it's a nonterminal token, \n",
            "        # so we need to choose a replacement at random. \n",
            "        replacement = random.choice(grammar[token]) \n",
            " \n",
            "        if is_terminal(replacement): \n",
            "            tokens[i] = replacement \n",
            "        else: \n",
            "            # Replacement could be, e.g., \"_NP _VP\", so we need to \n",
            "            # split it on spaces and splice it in. \n",
            "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):] \n",
            " \n",
            "        # Now call expand on the new list of tokens. \n",
            "        return expand(grammar, tokens) \n",
            " \n",
            "    # If we get here, we had all terminals and are done. \n",
            "    return tokens\n",
            "And now we can start generating sentences:\n",
            "def generate_sentence(grammar: Grammar) -> List[str]: \n",
            "    return expand(grammar, [\"_S\"])\n",
            "T ry changing the grammar—add more words, add more rules, add your\n",
            "own parts of speech—until you’re ready to generate as many web pages as\n",
            "your company needs.Grammars are actually more interesting when they’re used in the other\n",
            "direction. Given a sentence, we can use a grammar to parse  the sentence.\n",
            "This then allows us to identify subjects and verbs and helps us make sense\n",
            "of the sentence.\n",
            "Using data science to generate text is a neat trick; using it to understand\n",
            "text is more magical. (See “For Further Exploration”  for libraries that you\n",
            "could use for this.)\n",
            "A n  A s i d e :  G i b b s  S a m p l i n g\n",
            "Generating  samples from some distributions is easy . W e can get uniform\n",
            "random variables with:\n",
            "random.random()\n",
            "and normal random variables with:\n",
            "inverse_normal_cdf(random.random())\n",
            "But some distributions are harder to sample from. Gibbs sampling  is a\n",
            "technique for generating samples from multidimensional distributions when\n",
            "we only know some of the conditional distributions.\n",
            "For example, imagine rolling two dice. Let x  be the value of the first die and\n",
            "y  be the sum of the dice, and imagine you wanted to generate lots of ( x , y )\n",
            "pairs. In this case it’ s easy to generate the samples directly:\n",
            "from typing import Tuple \n",
            "import random \n",
            " \n",
            "def roll_a_die() -> int: \n",
            "    return random.choice([1, 2, 3, 4, 5, 6]) \n",
            " \n",
            "def direct_sample() -> Tuple[int, int]: \n",
            "    d1 = roll_a_die() \n",
            "    d2 = roll_a_die() \n",
            "    return d1, d1 + d2But imagine that you only knew the conditional distributions. The\n",
            "distribution of y  conditional on x  is easy—if you know the value of x , y  is\n",
            "equally likely to be x  + 1, x  + 2, x  + 3, x  + 4, x  + 5, or x  + 6:\n",
            "def random_y_given_x(x: int) -> int: \n",
            "    \"\"\"equally likely to be x + 1, x + 2, ... , x + 6\"\"\" \n",
            "    return x + roll_a_die()\n",
            "The other direction is more complicated. For example, if you know that y  is\n",
            "2, then necessarily x  is 1 (since the only way two dice can sum to 2 is if\n",
            "both of them are 1). If you know y  is 3, then x  is equally likely to be 1 or 2.\n",
            "Similarly , if y  is 1 1, then x  has to be either 5 or 6:\n",
            "def random_x_given_y(y: int) -> int: \n",
            "    if y <= 7: \n",
            "        # if the total is 7 or less, the first die is equally likely to be \n",
            "        # 1, 2, ..., (total - 1) \n",
            "        return random.randrange(1, y) \n",
            "    else: \n",
            "        # if the total is 7 or more, the first die is equally likely to be \n",
            "        # (total - 6), (total - 5), ..., 6 \n",
            "        return random.randrange(y - 6, 7)\n",
            "The way Gibbs sampling works is that we start with any (valid) values for x\n",
            "and y  and then repeatedly alternate replacing x  with a random value picked\n",
            "conditional on y  and replacing y  with a random value picked conditional on\n",
            "x . After a number of iterations, the resulting values of x  and y  will represent\n",
            "a sample from the unconditional joint distribution:\n",
            "def gibbs_sample(num_iters: int = 100) -> Tuple[int, int]: \n",
            "    x, y = 1, 2 # doesn't really matter \n",
            "    for _ in range(num_iters): \n",
            "        x = random_x_given_y(y) \n",
            "        y = random_y_given_x(x) \n",
            "    return x, y\n",
            "Y ou can check that this gives similar results to the direct sample:\n",
            "def compare_distributions(num_samples: int = 1000) -> Dict[int, List[int]]: \n",
            "    counts = defaultdict(lambda: [0, 0])for _ in range(num_samples): \n",
            "        counts[gibbs_sample()][0] += 1 \n",
            "        counts[direct_sample()][1] += 1 \n",
            "    return counts\n",
            "W e’ll use this technique in the next section.\n",
            "T o p i c  M o d e l i n g\n",
            "When  we built our “Data Scientists Y ou May Know” recommender in\n",
            "Chapter 1 , we simply looked for exact matches in people’ s stated interests.\n",
            "A  more sophisticated approach to understanding our users’ interests might\n",
            "try to identify the topics  that underlie those interests. A technique called\n",
            "latent Dirichlet allocation  (LDA) is commonly used to identify common\n",
            "topics in a set of documents. W e’ll apply it to documents that consist of\n",
            "each user ’ s interests.\n",
            "LDA has some similarities to the Naive Bayes classifier we built in\n",
            "Chapter 13 , in that it assumes a probabilistic model for documents. W e’ll\n",
            "gloss over the hairier mathematical details, but for our purposes the model\n",
            "assumes that:\n",
            "There is some fixed number K  of topics.\n",
            "There is a random variable that assigns each topic an associated\n",
            "probability distribution over words. Y ou should think of this\n",
            "distribution as the probability of seeing word w  given topic k .\n",
            "There is another random variable that assigns each document a\n",
            "probability distribution over topics. Y ou should think of this\n",
            "distribution as the mixture of topics in document d .\n",
            "Each word in a document was generated by first randomly picking\n",
            "a topic (from the document’ s distribution of topics) and then\n",
            "randomly picking a word (from the topic’ s distribution of words).In particular , we have a collection of documents , each of which is a list  of\n",
            "words. And we have a corresponding collection of document_topics  that\n",
            "assigns a topic (here a number between 0 and K  – 1) to each word in each\n",
            "document.\n",
            "So, the fifth word in the fourth document is:\n",
            "documents[3][4]\n",
            "and the topic from which that word was chosen is:\n",
            "document_topics[3][4]\n",
            "This very explicitly defines each document’ s distribution over topics, and it\n",
            "implicitly defines each topic’ s distribution over words.\n",
            "W e can estimate the likelihood that topic 1 produces a certain word by\n",
            "comparing how many times topic 1 produces that word with how many\n",
            "times topic 1 produces any  word. (Similarly , when we built a spam filter in\n",
            "Chapter 13 , we compared how many times each word appeared in spams\n",
            "with the total number of words appearing in spams.)\n",
            "Although these topics are just numbers, we can give them descriptive\n",
            "names by looking at the words on which they put the heaviest weight. W e\n",
            "just have to somehow generate the document_topics . This is where Gibbs\n",
            "sampling comes into play .\n",
            "W e start by assigning every word in every document a topic completely at\n",
            "random. Now we go through each document one word at a time. For that\n",
            "word and document, we construct weights for each topic that depend on the\n",
            "(current) distribution of topics in that document and the (current)\n",
            "distribution of words for that topic. W e then use those weights to sample a\n",
            "new topic for that word. If we iterate this process many times, we will end\n",
            "up with a joint sample from the topic–word distribution and the document–\n",
            "topic distribution.T o start with, we’ll need a function to randomly choose an index based on\n",
            "an arbitrary set of weights:\n",
            "def sample_from(weights: List[float]) -> int: \n",
            "    \"\"\"returns i with probability weights[i] / sum(weights)\"\"\" \n",
            "    total = sum(weights) \n",
            "    rnd = total * random.random()      # uniform between 0 and total \n",
            "    for i, w in enumerate(weights): \n",
            "        rnd -= w                       # return the smallest i such that \n",
            "        if rnd <= 0: return i          # weights[0] + ... + weights[i] >= rnd\n",
            "For instance, if you give it weights [1, 1, 3] , then one-fifth of the time it\n",
            "will return 0, one-fifth of the time it will return 1, and three-fifths of the\n",
            "time it will return 2. Let’ s write a test:\n",
            "from collections import Counter \n",
            " \n",
            "# Draw 1000 times and count \n",
            "draws = Counter(sample_from([0.1, 0.1, 0.8]) for _ in range(1000)) \n",
            "assert 10 < draws[0] < 190   # should be ~10%, this is a really loose test \n",
            "assert 10 < draws[1] < 190   # should be ~10%, this is a really loose test \n",
            "assert 650 < draws[2] < 950  # should be ~80%, this is a really loose test \n",
            "assert draws[0] + draws[1] + draws[2] == 1000\n",
            "Our documents are our users’ interests, which look like:\n",
            "documents = [ \n",
            "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"], \n",
            "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"], \n",
            "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"], \n",
            "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"], \n",
            "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"], \n",
            "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"], \n",
            "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"], \n",
            "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"], \n",
            "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial \n",
            "intelligence\"], \n",
            "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"], \n",
            "    [\"statistics\", \"R\", \"statsmodels\"], \n",
            "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"], \n",
            "    [\"pandas\", \"R\", \"Python\"], \n",
            "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],[\"libsvm\", \"regression\", \"support vector machines\"] \n",
            "]\n",
            "And we’ll try to find:\n",
            "K = 4\n",
            "topics. In order to calculate the sampling weights, we’ll need to keep track\n",
            "of several counts. Let’ s first create the data structures for them.\n",
            "How many times each topic is assigned to each document:\n",
            "# a list of Counters, one for each document \n",
            "document_topic_counts = [Counter() for _ in documents]\n",
            "How many times each word is assigned to each topic:\n",
            "# a list of Counters, one for each topic \n",
            "topic_word_counts = [Counter() for _ in range(K)]\n",
            "The total number of words assigned to each topic:\n",
            "# a list of numbers, one for each topic \n",
            "topic_counts = [0 for _ in range(K)]\n",
            "The total number of words contained in each document:\n",
            "# a list of numbers, one for each document \n",
            "document_lengths = [len(document) for document in documents]\n",
            "The number of distinct words:\n",
            "distinct_words = set(word for document in documents for word in \n",
            "document) \n",
            "W = len(distinct_words)And the number of documents:\n",
            "D = len(documents)\n",
            "Once we populate these, we can find, for example, the number of words in\n",
            "documents[3]  associated with topic 1 as follows:\n",
            "document_topic_counts[3][1]\n",
            "And we can find the number of times nlp  is associated with topic 2 as\n",
            "follows:\n",
            "topic_word_counts[2][\"nlp\"]\n",
            "Now we’re ready to define our conditional probability functions. As in\n",
            "Chapter 13 , each has a smoothing term that ensures every topic has a\n",
            "nonzero chance of being chosen in any document and that every word has a\n",
            "nonzero chance of being chosen for any topic:\n",
            "def p_topic_given_document(topic: int, d: int, alpha: float = 0.1) -> float: \n",
            "    \"\"\" \n",
            "    The fraction of words in document 'd' \n",
            "    that are assigned to 'topic' (plus some smoothing) \n",
            "    \"\"\" \n",
            "    return ((document_topic_counts[d][topic] + alpha) / \n",
            "            (document_lengths[d] + K * alpha)) \n",
            " \n",
            "def p_word_given_topic(word: str, topic: int, beta: float = 0.1) -> float: \n",
            "    \"\"\" \n",
            "    The fraction of words assigned to 'topic' \n",
            "    that equal 'word' (plus some smoothing) \n",
            "    \"\"\" \n",
            "    return ((topic_word_counts[topic][word] + beta) / \n",
            "            (topic_counts[topic] + W * beta))\n",
            "W e’ll use these to create the weights for updating topics:\n",
            "def topic_weight(d: int, word: str, k: int) -> float: \n",
            "    \"\"\" \n",
            "    Given a document and a word in that document,return the weight for the kth topic \n",
            "    \"\"\" \n",
            "    return p_word_given_topic(word, k) * p_topic_given_document(k, d) \n",
            " \n",
            "def choose_new_topic(d: int, word: str) -> int: \n",
            "    return sample_from([topic_weight(d, word, k) \n",
            "                        for k in range(K)])\n",
            "There are solid mathematical reasons why topic_weight  is defined the\n",
            "way it is, but their details would lead us too far afield. Hopefully it makes at\n",
            "least intuitive sense that—given a word and its document—the likelihood of\n",
            "any topic choice depends on both how likely that topic is for the document\n",
            "and how likely that word is for the topic.\n",
            "This is all the machinery we need. W e start by assigning every word to a\n",
            "random topic and populating our counters appropriately:\n",
            "random.seed(0) \n",
            "document_topics = [[random.randrange(K) for word in document] \n",
            "                   for document in documents] \n",
            " \n",
            "for d in range(D): \n",
            "    for word, topic in zip(documents[d], document_topics[d]): \n",
            "        document_topic_counts[d][topic] += 1 \n",
            "        topic_word_counts[topic][word] += 1 \n",
            "        topic_counts[topic] += 1\n",
            "Our goal is to get a joint sample of the topics–word distribution and the\n",
            "documents–topic distribution. W e do this using a form of Gibbs sampling\n",
            "that uses the conditional probabilities defined previously:\n",
            "import tqdm \n",
            " \n",
            "for iter in tqdm.trange(1000): \n",
            "    for d in range(D): \n",
            "        for i, (word, topic) in enumerate(zip(documents[d], \n",
            "                                              document_topics[d])): \n",
            " \n",
            "            # remove this word / topic from the counts \n",
            "            # so that it doesn't influence the weights \n",
            "            document_topic_counts[d][topic] -= 1 \n",
            "            topic_word_counts[topic][word] -= 1topic_counts[topic] -= 1 \n",
            "            document_lengths[d] -= 1 \n",
            " \n",
            "            # choose a new topic based on the weights \n",
            "            new_topic = choose_new_topic(d, word) \n",
            "            document_topics[d][i] = new_topic \n",
            " \n",
            "            # and now add it back to the counts \n",
            "            document_topic_counts[d][new_topic] += 1 \n",
            "            topic_word_counts[new_topic][word] += 1 \n",
            "            topic_counts[new_topic] += 1 \n",
            "            document_lengths[d] += 1\n",
            "What are the topics? They’re just numbers 0, 1, 2, and 3. If we want names\n",
            "for them, we have to do that ourselves. Let’ s look at the five most heavily\n",
            "weighted words for each ( T able 21-1 ):\n",
            "for k, word_counts in enumerate(topic_word_counts): \n",
            "    for word, count in word_counts.most_common(): \n",
            "        if count > 0: \n",
            "            print(k, word, count)\n",
            "T able 21-1. Most common wor ds per topic\n",
            "T opic 0 T opic 1 T opic 2 T opic 3\n",
            "Java R HBase regression\n",
            "Big Data statistics Postgres libsvm\n",
            "Hadoop Python MongoDB scikit-learn\n",
            "deep learning probability Cassandra machine learning\n",
            "artificial intelligence pandas NoSQL neural networks\n",
            "Based on these I’d probably assign topic names:\n",
            "topic_names = [\"Big Data and programming languages\", \n",
            "               \"Python and statistics\", \n",
            "               \"databases\", \n",
            "               \"machine learning\"]at which point we can see how the model assigns topics to each user ’ s\n",
            "interests:\n",
            "for document, topic_counts in zip(documents, document_topic_counts): \n",
            "    print(document) \n",
            "    for topic, count in topic_counts.most_common(): \n",
            "        if count > 0: \n",
            "            print(topic_names[topic], count) \n",
            "    print()\n",
            "which gives:\n",
            "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra'] \n",
            "Big Data and programming languages 4 databases 3 \n",
            "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres'] \n",
            "databases 5 \n",
            "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas'] \n",
            "Python and statistics 5 machine learning 1\n",
            "and so on. Given the “ands” we needed in some of our topic names, it’ s\n",
            "possible we should use more topics, although most likely we don’ t have\n",
            "enough data to successfully learn them.\n",
            "W o r d  V e c t o r s\n",
            "A  lot of recent advances in NLP involve deep learning. In the rest of this\n",
            "chapter we’ll look at a couple of them using the machinery we developed in\n",
            "Chapter 19 .\n",
            "One important innovation involves representing words as low-dimensional\n",
            "vectors. These vectors can be compared, added together , fed into machine\n",
            "learning models, or anything else you want to do with them. They usually\n",
            "have nice properties; for example, similar words tend to have similar\n",
            "vectors. That is, typically the word vector for big  is pretty close to the word\n",
            "vector for lar ge , so that a model operating on word vectors can (to some\n",
            "degree) handle things like synonymy for free.Frequently the vectors will exhibit delightful arithmetic properties as well.\n",
            "For instance, in some such models if you take the vector for king , subtract\n",
            "the vector for man , and add the vector for woman , you will end up with a\n",
            "vector that’ s very close to the vector for queen . It can be interesting to\n",
            "ponder what this means about what the word vectors actually “learn,”\n",
            "although we won’ t spend time on that here.\n",
            "Coming up with such vectors for a lar ge vocabulary of words is a dif ficult\n",
            "undertaking, so typically we’ll learn  them from a corpus of text. There are a\n",
            "couple of dif ferent schemes, but at a high level the task typically looks\n",
            "something like this:\n",
            "1 . Get a bunch of text.\n",
            "2 . Create a dataset where the goal is to predict a word given nearby\n",
            "words (or alternatively , to predict nearby words given a word).\n",
            "3 . T rain a neural net to do well on this task.\n",
            "4 . T ake the internal states of the trained neural net as the word\n",
            "vectors.\n",
            "In particular , because the task is to predict a word given nearby words,\n",
            "words that occur in similar contexts (and hence have similar nearby words)\n",
            "should have similar internal states and therefore similar word vectors.\n",
            "Here  we’ll measure “similarity” using cosine similarity , which is a number\n",
            "between –1 and 1 that measures the degree to which two vectors point in\n",
            "the same direction:\n",
            "from scratch.linear_algebra import dot, Vector \n",
            "import math \n",
            " \n",
            "def cosine_similarity(v1: Vector, v2: Vector) -> float: \n",
            "    return dot(v1, v2) / math.sqrt(dot(v1, v1) * dot(v2, v2)) \n",
            " \n",
            "assert cosine_similarity([1., 1, 1], [2., 2, 2]) == 1, \"same direction\" \n",
            "assert cosine_similarity([-1., -1], [2., 2]) == -1,    \"opposite direction\" \n",
            "assert cosine_similarity([1., 0], [0., 1]) == 0,       \"orthogonal\"Let’ s learn some word vectors to see how this works.\n",
            "T o start with, we’ll need a toy dataset. The commonly used word vectors\n",
            "are typically derived from training on millions or even billions of words. As\n",
            "our toy library can’ t cope with that much data, we’ll create an artificial\n",
            "dataset with some structure to it:\n",
            "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"\"] \n",
            "nouns = [\"bed\", \"car\", \"boat\", \"cat\"] \n",
            "verbs = [\"is\", \"was\", \"seems\"] \n",
            "adverbs = [\"very\", \"quite\", \"extremely\", \"\"] \n",
            "adjectives = [\"slow\", \"fast\", \"soft\", \"hard\"] \n",
            " \n",
            "def make_sentence() -> str: \n",
            "    return \" \".join([ \n",
            "        \"The\", \n",
            "        random.choice(colors), \n",
            "        random.choice(nouns), \n",
            "        random.choice(verbs), \n",
            "        random.choice(adverbs), \n",
            "        random.choice(adjectives), \n",
            "        \".\" \n",
            "    ]) \n",
            " \n",
            "NUM_SENTENCES = 50 \n",
            " \n",
            "random.seed(0) \n",
            "sentences = [make_sentence() for _ in range(NUM_SENTENCES)]\n",
            "This will generate lots of sentences with similar structure but dif ferent\n",
            "words; for example, “The green boat seems quite slow .” Given this setup,\n",
            "the colors will mostly appear in “similar” contexts, as will the nouns, and so\n",
            "on. So if we do a good job of assigning word vectors, the colors should get\n",
            "similar vectors, and so on.\n",
            "N O T E\n",
            "In practical usage, you’d probably have a corpus of millions of sentences, in which case\n",
            "you’d get “enough” context from the sentences as they are. Here, with only 50\n",
            "sentences, we have to make them somewhat artificial.As mentioned earlier , we’ll  want to one-hot-encode our words, which\n",
            "means we’ll need to convert them to IDs. W e’ll introduce a Vocabulary\n",
            "class to keep track of this mapping:\n",
            "from scratch.deep_learning import Tensor \n",
            " \n",
            "class Vocabulary: \n",
            "    def __init__(self, words: List[str] = None) -> None: \n",
            "        self.w2i: Dict[str, int] = {}  # mapping word -> word_id \n",
            "        self.i2w: Dict[int, str] = {}  # mapping word_id -> word \n",
            " \n",
            "        for word in (words or []):     # If words were provided, \n",
            "            self.add(word)             # add them. \n",
            " \n",
            "    @property \n",
            "    def size(self) -> int: \n",
            "        \"\"\"how many words are in the vocabulary\"\"\" \n",
            "        return len(self.w2i) \n",
            " \n",
            "    def add(self, word: str) -> None: \n",
            "        if word not in self.w2i:        # If the word is new to us: \n",
            "            word_id = len(self.w2i)     # Find the next id. \n",
            "            self.w2i[word] = word_id    # Add to the word -> word_id map. \n",
            "            self.i2w[word_id] = word    # Add to the word_id -> word map. \n",
            " \n",
            "    def get_id(self, word: str) -> int: \n",
            "        \"\"\"return the id of the word (or None)\"\"\" \n",
            "        return self.w2i.get(word) \n",
            " \n",
            "    def get_word(self, word_id: int) -> str: \n",
            "        \"\"\"return the word with the given id (or None)\"\"\" \n",
            "        return self.i2w.get(word_id) \n",
            " \n",
            "    def one_hot_encode(self, word: str) -> Tensor: \n",
            "        word_id = self.get_id(word) \n",
            "        assert word_id is not None, f\"unknown word {word}\" \n",
            " \n",
            "        return [1.0 if i == word_id else 0.0 for i in range(self.size)]\n",
            "These are all things we could do manually , but it’ s handy to have it in a\n",
            "class. W e should probably test it:\n",
            "vocab = Vocabulary([\"a\", \"b\", \"c\"]) \n",
            "assert vocab.size == 3,              \"there are 3 words in the vocab\"assert vocab.get_id(\"b\") == 1,       \"b should have word_id 1\" \n",
            "assert vocab.one_hot_encode(\"b\") == [0, 1, 0] \n",
            "assert vocab.get_id(\"z\") is None,    \"z is not in the vocab\" \n",
            "assert vocab.get_word(2) == \"c\",     \"word_id 2 should be c\" \n",
            "vocab.add(\"z\") \n",
            "assert vocab.size == 4,              \"now there are 4 words in the vocab\" \n",
            "assert vocab.get_id(\"z\") == 3,       \"now z should have id 3\" \n",
            "assert vocab.one_hot_encode(\"z\") == [0, 0, 0, 1]\n",
            "W e should also write simple helper functions to save and load a vocabulary ,\n",
            "just as we have for our deep learning models:\n",
            "import json \n",
            " \n",
            "def save_vocab(vocab: Vocabulary, filename: str) -> None: \n",
            "    with open(filename, 'w') as f: \n",
            "        json.dump(vocab.w2i, f)       # Only need to save w2i \n",
            " \n",
            "def load_vocab(filename: str) -> Vocabulary: \n",
            "    vocab = Vocabulary() \n",
            "    with open(filename) as f: \n",
            "        # Load w2i and generate i2w from it \n",
            "        vocab.w2i = json.load(f) \n",
            "        vocab.i2w = {id: word for word, id in vocab.w2i.items()} \n",
            "    return vocab\n",
            "W e’ll  be using a word vector model called skip-gram  that takes as input a\n",
            "word and generates probabilities for what words are likely to be seen near\n",
            "it. W e will feed it training pairs (word, nearby_word)  and try to minimize\n",
            "the SoftmaxCrossEntropy  loss.\n",
            "N O T E\n",
            "Another  common model, continuous bag-of-wor ds  (CBOW), takes the nearby words as\n",
            "the inputs and tries to predict the original word.\n",
            "Let’ s  design our neural network. At its heart will be an embedding  layer that\n",
            "takes as input a word ID and returns a word vector . Under the covers we\n",
            "can just use a lookup table for this.W e’ll then pass the word vector to a Linear  layer with the same number of\n",
            "outputs as we have words in our vocabulary . As before, we’ll use softmax\n",
            "to convert these outputs to probabilities over nearby words. As we use\n",
            "gradient descent to train the model, we will be updating the vectors in the\n",
            "lookup table. Once we’ve finished training, that lookup table gives us our\n",
            "word vectors.\n",
            "Let’ s create that embedding layer . In practice we might want to embed\n",
            "things other than words, so we’ll construct a more general Embedding  layer .\n",
            "(Later we’ll write a TextEmbedding  subclass that’ s specifically for word\n",
            "vectors.)\n",
            "In its constructor we’ll provide the number and dimension of our\n",
            "embedding vectors, so it can create the embeddings (which will be standard\n",
            "random normals, initially):\n",
            "from typing import Iterable \n",
            "from scratch.deep_learning import Layer, Tensor, random_tensor, zeros_like \n",
            " \n",
            "class Embedding(Layer): \n",
            "    def __init__(self, num_embeddings: int, embedding_dim: int) -> None: \n",
            "        self.num_embeddings = num_embeddings \n",
            "        self.embedding_dim = embedding_dim \n",
            " \n",
            "        # One vector of size embedding_dim for each desired embedding \n",
            "        self.embeddings = random_tensor(num_embeddings, embedding_dim) \n",
            "        self.grad = zeros_like(self.embeddings) \n",
            " \n",
            "        # Save last input id \n",
            "        self.last_input_id = None\n",
            "In our case we’ll only be embedding one word at a time. However , in other\n",
            "models we might want to embed a sequence of words and get back a\n",
            "sequence of word vectors. (For example, if we wanted to train the CBOW\n",
            "model described earlier .) So an alternative design would take sequences of\n",
            "word IDs. W e’ll stick with one at a time, to make things simpler .\n",
            "    def forward(self, input_id: int) -> Tensor: \n",
            "        \"\"\"Just select the embedding vector corresponding to the input id\"\"\" \n",
            "        self.input_id = input_id    # remember for use in backpropagationreturn self.embeddings[input_id]\n",
            "For the backward pass we’ll get a gradient corresponding to the chosen\n",
            "embedding vector , and we’ll need to construct the corresponding gradient\n",
            "for self.embeddings , which is zero for every embedding other than the\n",
            "chosen one:\n",
            "    def backward(self, gradient: Tensor) -> None: \n",
            "        # Zero out the gradient corresponding to the last input. \n",
            "        # This is way cheaper than creating a new all-zero tensor each time. \n",
            "        if self.last_input_id is not None: \n",
            "            zero_row = [0 for _ in range(self.embedding_dim)] \n",
            "            self.grad[self.last_input_id] = zero_row \n",
            " \n",
            "        self.last_input_id = self.input_id \n",
            "        self.grad[self.input_id] = gradient\n",
            "Because we have parameters and gradients, we need to override those\n",
            "methods:\n",
            "    def params(self) -> Iterable[Tensor]: \n",
            "        return [self.embeddings] \n",
            " \n",
            "    def grads(self) -> Iterable[Tensor]: \n",
            "        return [self.grad]\n",
            "As mentioned earlier , we’ll want a subclass specifically for word vectors. In\n",
            "that case our number of embeddings is determined by our vocabulary , so\n",
            "let’ s just pass that in instead:\n",
            "class TextEmbedding(Embedding): \n",
            "    def __init__(self, vocab: Vocabulary, embedding_dim: int) -> None: \n",
            "        # Call the superclass constructor \n",
            "        super().__init__(vocab.size, embedding_dim) \n",
            " \n",
            "        # And hang onto the vocab \n",
            "        self.vocab = vocabThe other built-in methods will all work as is, but we’ll add a couple more\n",
            "methods specific to working with text. For example, we’d like to be able to\n",
            "retrieve the vector for a given word. (This is not part of the Layer  interface,\n",
            "but we are always free to add extra methods to specific layers as we like.)\n",
            "    def __getitem__(self, word: str) -> Tensor: \n",
            "        word_id = self.vocab.get_id(word) \n",
            "        if word_id is not None: \n",
            "            return self.embeddings[word_id] \n",
            "        else: \n",
            "            return None\n",
            "This dunder method will allow us to retrieve word vectors using indexing:\n",
            "word_vector = embedding[\"black\"]\n",
            "And we’d also like the embedding layer to tell us the closest words to a\n",
            "given word:\n",
            "    def closest(self, word: str, n: int = 5) -> List[Tuple[float, str]]: \n",
            "        \"\"\"Returns the n closest words based on cosine similarity\"\"\" \n",
            "        vector = self[word] \n",
            " \n",
            "        # Compute pairs (similarity, other_word), and sort most similar first \n",
            "        scores = [(cosine_similarity(vector, self.embeddings[i]), other_word) \n",
            "                  for other_word, i in self.vocab.w2i.items()] \n",
            "        scores.sort(reverse=True) \n",
            " \n",
            "        return scores[:n]\n",
            "Our embedding layer just outputs vectors, which we can feed into a Linear\n",
            "layer .\n",
            "Now we’re ready to assemble our training data. For each input word, we’ll\n",
            "choose as tar get words the two words to its left and the two words to its\n",
            "right.\n",
            "Let’ s start by lowercasing the sentences and splitting them into words:import re \n",
            " \n",
            "# This is not a great regex, but it works on our data. \n",
            "tokenized_sentences = [re.findall(\"[a-z]+|[.]\", sentence.lower()) \n",
            "                       for sentence in sentences]\n",
            "at which point we can construct a vocabulary:\n",
            "# Create a vocabulary (that is, a mapping word -> word_id) based on our text. \n",
            "vocab = Vocabulary(word \n",
            "                   for sentence_words in tokenized_sentences \n",
            "                   for word in sentence_words)\n",
            "And now we can create training data:\n",
            "from scratch.deep_learning import Tensor, one_hot_encode \n",
            " \n",
            "inputs: List[int] = [] \n",
            "targets: List[Tensor] = [] \n",
            " \n",
            "for sentence in tokenized_sentences: \n",
            "    for i, word in enumerate(sentence):          # For each word \n",
            "        for j in [i - 2, i - 1, i + 1, i + 2]:   # take the nearby locations \n",
            "            if 0 <= j < len(sentence):           # that aren't out of bounds \n",
            "                nearby_word = sentence[j]        # and get those words. \n",
            " \n",
            "                # Add an input that's the original word_id \n",
            "                inputs.append(vocab.get_id(word)) \n",
            " \n",
            "                # Add a target that's the one-hot-encoded nearby word \n",
            "                targets.append(vocab.one_hot_encode(nearby_word))\n",
            "W ith the machinery we’ve built up, it’ s now easy to create our model:\n",
            "from scratch.deep_learning import Sequential, Linear \n",
            " \n",
            "random.seed(0) \n",
            "EMBEDDING_DIM = 5  # seems like a good size \n",
            " \n",
            "# Define the embedding layer separately, so we can reference it. \n",
            "embedding = TextEmbedding(vocab=vocab, embedding_dim=EMBEDDING_DIM) \n",
            " \n",
            "model = Sequential([ \n",
            "    # Given a word (as a vector of word_ids), look up its embedding.embedding, \n",
            "    # And use a linear layer to compute scores for \"nearby words.\" \n",
            "    Linear(input_dim=EMBEDDING_DIM, output_dim=vocab.size) \n",
            "])\n",
            "Using the machinery from Chapter 19 , it’ s easy to train our model:\n",
            "from scratch.deep_learning import SoftmaxCrossEntropy, Momentum, \n",
            "GradientDescent \n",
            " \n",
            "loss = SoftmaxCrossEntropy() \n",
            "optimizer = GradientDescent(learning_rate=0.01) \n",
            " \n",
            "for epoch in range(100): \n",
            "    epoch_loss = 0.0 \n",
            "    for input, target in zip(inputs, targets): \n",
            "        predicted = model.forward(input) \n",
            "        epoch_loss += loss.loss(predicted, target) \n",
            "        gradient = loss.gradient(predicted, target) \n",
            "        model.backward(gradient) \n",
            "        optimizer.step(model) \n",
            "    print(epoch, epoch_loss)            # Print the loss \n",
            "    print(embedding.closest(\"black\"))   # and also a few nearest words \n",
            "    print(embedding.closest(\"slow\"))    # so we can see what's being \n",
            "    print(embedding.closest(\"car\"))     # learned.\n",
            "As you watch this train, you can see the colors getting closer to each other ,\n",
            "the adjectives getting closer to each other , and the nouns getting closer to\n",
            "each other .\n",
            "Once the model is trained, it’ s fun to explore the most similar words:\n",
            "pairs = [(cosine_similarity(embedding[w1], embedding[w2]), w1, w2) \n",
            "         for w1 in vocab.w2i \n",
            "         for w2 in vocab.w2i \n",
            "         if w1 < w2] \n",
            "pairs.sort(reverse=True) \n",
            "print(pairs[:5])\n",
            "which (for me) results in:\n",
            "[(0.9980283554864815, 'boat', 'car'), \n",
            " (0.9975147744587706, 'bed', 'cat'),(0.9953153441218054, 'seems', 'was'), \n",
            " (0.9927107440377975, 'extremely', 'quite'), \n",
            " (0.9836183658415987, 'bed', 'car')]\n",
            "(Obviously bed  and cat  are not really similar , but in our training sentences\n",
            "they appear to be, and that’ s what the model is capturing.)\n",
            "W e can also extract the first two principal components and plot them:\n",
            "from scratch.working_with_data import pca, transform \n",
            "import matplotlib.pyplot as plt \n",
            " \n",
            "# Extract the first two principal components and transform the word vectors \n",
            "components = pca(embedding.embeddings, 2) \n",
            "transformed = transform(embedding.embeddings, components) \n",
            " \n",
            "# Scatter the points (and make them white so they're \"invisible\") \n",
            "fig, ax = plt.subplots() \n",
            "ax.scatter(*zip(*transformed), marker='.', color='w') \n",
            " \n",
            "# Add annotations for each word at its transformed location \n",
            "for word, idx in vocab.w2i.items(): \n",
            "    ax.annotate(word, transformed[idx]) \n",
            " \n",
            "# And hide the axes \n",
            "ax.get_xaxis().set_visible(False) \n",
            "ax.get_yaxis().set_visible(False) \n",
            " \n",
            "plt.show()\n",
            "which shows that similar words are indeed clustering together ( Figure 21-\n",
            "3 ):Figur e 21-3. W or d vectors\n",
            "If you’re interested, it’ s not hard to train CBOW word vectors. Y ou’ll have\n",
            "to do a little work. First, you’ll need to modify the Embedding  layer so that\n",
            "it takes as input a list  of IDs and outputs a list  of embedding vectors. Then\n",
            "you’ll have to create a new layer ( Sum ?) that takes a list of vectors and\n",
            "returns their sum.\n",
            "Each word represents a training example where the input is the word IDs\n",
            "for the surrounding words, and the tar get is the one-hot encoding of the\n",
            "word itself.\n",
            "The modified Embedding  layer turns the surrounding words into a list of\n",
            "vectors, the new Sum  layer collapses the list of vectors down to a single\n",
            "vector , and then a Linear  layer can produce scores that can be softmax ed\n",
            "to get a distribution representing “most likely words, given this context.”I found the CBOW model harder to train than the skip-gram one, but I\n",
            "encourage you to try it out.\n",
            "R e c u r r e n t  N e u r a l  N e t w o r k s\n",
            "The  word vectors we developed in the previous section are often used as the\n",
            "inputs to neural networks. One challenge to doing this is that sentences have\n",
            "varying lengths: you could think of a 3-word sentence as a [3,\n",
            "embedding_dim]  tensor and a 10-word sentence as a [10,\n",
            "embedding_dim]  tensor . In order to, say , pass them to a Linear  layer , we\n",
            "need to do something about that first variable-length dimension.\n",
            "One option  is to use a Sum  layer (or a variant that takes the average);\n",
            "however , the or der  of the words in a sentence is usually important to its\n",
            "meaning. T o take a common example, “dog bites man” and “man bites dog”\n",
            "are two very dif ferent stories!\n",
            "Another way of handling this is using r ecurr ent neural networks  (RNNs),\n",
            "which have a hidden state  they maintain between inputs. In the simplest\n",
            "case, each input is combined with the current hidden state to produce an\n",
            "output, which is then used as the new hidden state. This allows such\n",
            "networks to “remember” (in a sense) the inputs they’ve seen, and to build\n",
            "up to a final output that depends on all the inputs and their order .\n",
            "W e’ll create pretty much the simplest possible RNN layer , which will\n",
            "accept a single input (corresponding to, e.g., a single word in a sentence, or\n",
            "a single character in a word), and which will maintain its hidden state\n",
            "between calls.\n",
            "Recall that our Linear  layer had some weights, w , and a bias, b . It took a\n",
            "vector input  and produced a dif ferent vector as output  using the logic:\n",
            "output[o] = dot(w[o], input) + b[o]\n",
            "Here we’ll want to incorporate our hidden state, so we’ll have two  sets of\n",
            "weights—one to apply to the input  and one to apply to the previoushidden  state:\n",
            "output[o] = dot(w[o], input) + dot(u[o], hidden) + b[o]\n",
            "Next, we’ll use the output  vector as the new value of hidden . This isn’ t a\n",
            "huge change, but it will allow our networks to do wonderful things.\n",
            "from scratch.deep_learning import tensor_apply, tanh \n",
            " \n",
            "class SimpleRnn(Layer): \n",
            "    \"\"\"Just about the simplest possible recurrent layer.\"\"\" \n",
            "    def __init__(self, input_dim: int, hidden_dim: int) -> None: \n",
            "        self.input_dim = input_dim \n",
            "        self.hidden_dim = hidden_dim \n",
            " \n",
            "        self.w = random_tensor(hidden_dim, input_dim, init='xavier') \n",
            "        self.u = random_tensor(hidden_dim, hidden_dim, init='xavier') \n",
            "        self.b = random_tensor(hidden_dim) \n",
            " \n",
            "        self.reset_hidden_state() \n",
            " \n",
            "    def reset_hidden_state(self) -> None: \n",
            "        self.hidden = [0 for _ in range(self.hidden_dim)]\n",
            "Y ou can see that we start out the hidden state as a vector of 0s, and we\n",
            "provide a function that people using the network can call to reset the hidden\n",
            "state.\n",
            "Given this setup, the forward  function is reasonably straightforward (at\n",
            "least, it is if you remember and understand how our Linear  layer worked):\n",
            "    def forward(self, input: Tensor) -> Tensor: \n",
            "        self.input = input              # Save both input and previous \n",
            "        self.prev_hidden = self.hidden  # hidden state to use in backprop. \n",
            " \n",
            "        a = [(dot(self.w[h], input) +           # weights @ input \n",
            "              dot(self.u[h], self.hidden) +     # weights @ hidden \n",
            "              self.b[h])                        # bias \n",
            "             for h in range(self.hidden_dim)] \n",
            " \n",
            "        self.hidden = tensor_apply(tanh, a)  # Apply tanh activation \n",
            "        return self.hidden                   # and return the result.The backward  pass is similar to the one in our Linear  layer , except that it\n",
            "needs to compute an additional set of gradients for the u  weights:\n",
            "    def backward(self, gradient: Tensor): \n",
            "        # Backpropagate through the tanh \n",
            "        a_grad = [gradient[h] * (1 - self.hidden[h] ** 2) \n",
            "                  for h in range(self.hidden_dim)] \n",
            " \n",
            "        # b has the same gradient as a \n",
            "        self.b_grad = a_grad \n",
            " \n",
            "        # Each w[h][i] is multiplied by input[i] and added to a[h], \n",
            "        # so each w_grad[h][i] = a_grad[h] * input[i] \n",
            "        self.w_grad = [[a_grad[h] * self.input[i] \n",
            "                        for i in range(self.input_dim)] \n",
            "                       for h in range(self.hidden_dim)] \n",
            " \n",
            "        # Each u[h][h2] is multiplied by hidden[h2] and added to a[h], \n",
            "        # so each u_grad[h][h2] = a_grad[h] * prev_hidden[h2] \n",
            "        self.u_grad = [[a_grad[h] * self.prev_hidden[h2] \n",
            "                        for h2 in range(self.hidden_dim)] \n",
            "                       for h in range(self.hidden_dim)] \n",
            " \n",
            "        # Each input[i] is multiplied by every w[h][i] and added to a[h], \n",
            "        # so each input_grad[i] = sum(a_grad[h] * w[h][i] for h in ...) \n",
            "        return [sum(a_grad[h] * self.w[h][i] for h in range(self.hidden_dim)) \n",
            "                for i in range(self.input_dim)]\n",
            "And finally we need to override the params  and grads  methods:\n",
            "    def params(self) -> Iterable[Tensor]: \n",
            "        return [self.w, self.u, self.b] \n",
            " \n",
            "    def grads(self) -> Iterable[Tensor]: \n",
            "        return [self.w_grad, self.u_grad, self.b_grad]\n",
            "W A R N I N G\n",
            "This “simple” RNN is so simple that you probably shouldn’ t use it in practice.Our SimpleRnn  has a couple of undesirable features. One is that its entire\n",
            "hidden state is used to update the input every time you call it. The other is\n",
            "that the entire hidden state is overwritten every time you call it. Both of\n",
            "these make it dif ficult to train; in particular , they make it dif ficult for the\n",
            "model to learn long-range dependencies.\n",
            "For  this reason, almost no one uses this kind of simple RNN. Instead, they\n",
            "use more complicated variants like the LSTM (“long short-term memory”)\n",
            "or the  GRU (“gated recurrent unit”), which have many more parameters and\n",
            "use parameterized “gates” that allow only some of the state to be updated\n",
            "(and only some of the state to be used) at each timestep.\n",
            "There is nothing particularly difficult  about these variants; however , they\n",
            "involve a great deal more code, which would not be (in my opinion)\n",
            "correspondingly more edifying to read. The code for this chapter on GitHub\n",
            "includes an LSTM implementation. I encourage you to check it out, but it’ s\n",
            "somewhat tedious and so we won’ t discuss it further here.\n",
            "One other quirk of our implementation is that it takes only one “step” at a\n",
            "time and requires us to manually reset the hidden state. A more practical\n",
            "RNN implementation might accept sequences of inputs, set its hidden state\n",
            "to 0s at the beginning of each sequence, and produce sequences of outputs.\n",
            "Ours could certainly be modified to behave this way; again, this would\n",
            "require more code and complexity for little gain in understanding.\n",
            "E x a m p l e :  U s i n g  a  C h a r a c t e r - L e v e l  R N N\n",
            "The  newly hired VP of Branding did not come up with the name\n",
            "DataSciencester  himself, and (accordingly) he suspects that a better name\n",
            "might lead to more success for the company . He asks you to use data\n",
            "science to suggest candidates for replacement.\n",
            "One “cute” application of RNNs involves using characters  (rather than\n",
            "words) as their inputs, training them to learn the subtle language patterns in\n",
            "some dataset, and then using them to generate fictional instances from that\n",
            "dataset.For example, you could train an RNN on the names of alternative bands,\n",
            "use the trained model to generate new names for fake alternative bands, and\n",
            "then hand-select the funniest ones and share them on T witter . Hilarity!\n",
            "Having seen this trick enough times to no longer consider it clever , you\n",
            "decide to give it a shot.\n",
            "After some digging, you find that the startup accelerator Y Combinator has\n",
            "published a list of its top 100 (actually 101) most successful startups , which\n",
            "seems like a good starting point. Checking the page, you find that the\n",
            "company names all live inside <b class=\"h4\">  tags, which means it’ s easy\n",
            "to use your web scraping skills to retrieve them:\n",
            "from bs4 import BeautifulSoup \n",
            "import requests \n",
            " \n",
            "url = \"https://www.ycombinator.com/topcompanies/\" \n",
            "soup = BeautifulSoup(requests.get(url).text, 'html5lib') \n",
            " \n",
            "# We get the companies twice, so use a set comprehension to deduplicate. \n",
            "companies = list({b.text \n",
            "                  for b in soup(\"b\") \n",
            "                  if \"h4\" in b.get(\"class\", ())}) \n",
            "assert len(companies) == 101\n",
            "As always, the page may change (or vanish), in which case this code won’ t\n",
            "work. If so, you can use your newly learned data science skills to fix it or\n",
            "just get the list from the book’ s GitHub site.\n",
            "So what is our plan? W e’ll train a model to predict the next character of a\n",
            "name, given the current character and  a hidden state representing all the\n",
            "characters we’ve seen so far .\n",
            "As usual, we’ll actually predict a probability distribution over characters\n",
            "and train our model to minimize the SoftmaxCrossEntropy  loss.\n",
            "Once our model is trained, we can use it to generate some probabilities,\n",
            "randomly sample a character according to those probabilities, and then feed\n",
            "that character as its next input. This will allow us to generate  company\n",
            "names using the learned weights.T o start with, we should build a Vocabulary  from the characters in the\n",
            "names:\n",
            "vocab = Vocabulary([c for company in companies for c in company])\n",
            "In addition, we’ll use special tokens to signify the start and end of a\n",
            "company name. This allows the model to learn which characters should\n",
            "begin  a company name and also to learn when a company name is finished .\n",
            "W e’ll just use the regex characters for start and end, which (luckily) don’ t\n",
            "appear in our list of companies:\n",
            "START = \"^\" \n",
            "STOP = \"$\" \n",
            " \n",
            "# We need to add them to the vocabulary too. \n",
            "vocab.add(START) \n",
            "vocab.add(STOP)\n",
            "For our model, we’ll one-hot-encode each character , pass it through two\n",
            "SimpleRnn s, and then use a Linear  layer to generate the scores for each\n",
            "possible next character:\n",
            "HIDDEN_DIM = 32  # You should experiment with different sizes! \n",
            " \n",
            "rnn1 =  SimpleRnn(input_dim=vocab.size, hidden_dim=HIDDEN_DIM) \n",
            "rnn2 =  SimpleRnn(input_dim=HIDDEN_DIM, hidden_dim=HIDDEN_DIM) \n",
            "linear = Linear(input_dim=HIDDEN_DIM, output_dim=vocab.size) \n",
            " \n",
            "model = Sequential([ \n",
            "    rnn1, \n",
            "    rnn2, \n",
            "    linear \n",
            "])\n",
            "Imagine for the moment that we’ve trained this model. Let’ s write the\n",
            "function that uses it to generate new company names, using the\n",
            "sample_from  function from “T opic Modeling” :from scratch.deep_learning import softmax \n",
            " \n",
            "def generate(seed: str = START, max_len: int = 50) -> str: \n",
            "    rnn1.reset_hidden_state()  # Reset both hidden states \n",
            "    rnn2.reset_hidden_state() \n",
            "    output = [seed]            # Start the output with the specified seed \n",
            " \n",
            "    # Keep going until we produce the STOP character or reach the max length \n",
            "    while output[-1] != STOP and len(output) < max_len: \n",
            "        # Use the last character as the input \n",
            "        input = vocab.one_hot_encode(output[-1]) \n",
            " \n",
            "        # Generate scores using the model \n",
            "        predicted = model.forward(input) \n",
            " \n",
            "        # Convert them to probabilities and draw a random char_id \n",
            "        probabilities = softmax(predicted) \n",
            "        next_char_id = sample_from(probabilities) \n",
            " \n",
            "        # Add the corresponding char to our output \n",
            "        output.append(vocab.get_word(next_char_id)) \n",
            " \n",
            "    # Get rid of START and END characters and return the word \n",
            "    return ''.join(output[1:-1])\n",
            "At long last, we’re ready to train our character -level RNN. It will take a\n",
            "while!\n",
            "loss = SoftmaxCrossEntropy() \n",
            "optimizer = Momentum(learning_rate=0.01, momentum=0.9) \n",
            " \n",
            "for epoch in range(300): \n",
            "    random.shuffle(companies)  # Train in a different order each epoch. \n",
            "    epoch_loss = 0             # Track the loss. \n",
            "    for company in tqdm.tqdm(companies): \n",
            "        rnn1.reset_hidden_state()  # Reset both hidden states. \n",
            "        rnn2.reset_hidden_state() \n",
            "        company = START + company + STOP   # Add START and STOP characters. \n",
            " \n",
            "        # The rest is just our usual training loop, except that the inputs \n",
            "        # and target are the one-hot-encoded previous and next characters. \n",
            "        for prev, next in zip(company, company[1:]): \n",
            "            input = vocab.one_hot_encode(prev) \n",
            "            target = vocab.one_hot_encode(next) \n",
            "            predicted = model.forward(input) \n",
            "            epoch_loss += loss.loss(predicted, target)gradient = loss.gradient(predicted, target) \n",
            "            model.backward(gradient) \n",
            "            optimizer.step(model) \n",
            " \n",
            "    # Each epoch, print the loss and also generate a name. \n",
            "    print(epoch, epoch_loss, generate()) \n",
            " \n",
            "    # Turn down the learning rate for the last 100 epochs. \n",
            "    # There's no principled reason for this, but it seems to work. \n",
            "    if epoch == 200: \n",
            "        optimizer.lr *= 0.1\n",
            "After training, the model generates some actual names from the list (which\n",
            "isn’ t surprising, since the model has a fair amount of capacity and not a lot\n",
            "of training data), as well as names that are only slightly dif ferent from\n",
            "training names (Scripe, Loinbare, Pozium), names that seem genuinely\n",
            "creative (Benuus, Cletpo, Equite, V ivest), and names that are garbage-y but\n",
            "still sort of word-like (SFitreasy , Sint ocanelp, GliyOx, Doorboronelhav).\n",
            "Unfortunately , like most character -level-RNN outputs, these are only mildly\n",
            "clever , and the VP of Branding ends up unable to use them.\n",
            "If I up the hidden dimension to 64, I get a lot more names verbatim from the\n",
            "list; if I drop it to 8, I get mostly garbage. The vocabulary and final weights\n",
            "for all these model sizes are available on the book’ s GitHub site , and you\n",
            "can use load_weights  and load_vocab  to use them yourself.\n",
            "As mentioned previously , the GitHub code for this chapter also contains an\n",
            "implementation for an LSTM, which you should feel free to swap in as a\n",
            "replacement for the SimpleRnn s in our company name model.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "NL TK  is  a popular library of NLP tools for Python. It has its own\n",
            "entire book , which is available to read online.\n",
            "gensim  is a  Python library for topic modeling, which is a better bet\n",
            "than our from-scratch model.spaCy  is  a library for “Industrial Strength Natural Language\n",
            "Processing in Python” and is also quite popular .\n",
            "Andrej Karpathy has a famous blog post, “The Unreasonable\n",
            "Ef fectiveness of Recurrent Neural Networks” , that’ s very much\n",
            "worth reading.\n",
            "My  day job involves building AllenNLP , a Python library for\n",
            "doing NLP research. (At least, as of the time this book went to\n",
            "press, it did.) The library is quite beyond the scope of this book,\n",
            "but you might still find it interesting, and it has a cool interactive\n",
            "demo of many state-of-the-art NLP models.Chapter 22. Network Analysis\n",
            "Y our connections to all the things ar ound you literally define who you\n",
            "ar e.\n",
            "— Aaron O’Connell\n",
            "Many  interesting data problems can be fruitfully thought of in terms of\n",
            "networks , consisting of nodes  of some type and the edges  that join them.\n",
            "For instance, your Facebook friends form the nodes of a network whose\n",
            "edges are friendship relations. A less obvious example is the W orld W ide\n",
            "W eb itself, with each web page a node and each hyperlink from one page to\n",
            "another an edge.\n",
            "Facebook friendship is mutual—if I am Facebook friends with you, then\n",
            "necessarily you are friends with me. In this case, we say that the  edges are\n",
            "undir ected . Hyperlinks are not—my website links to whitehouse.gov , but\n",
            "(for reasons inexplicable to me) whitehouse.gov  refuses to link to my\n",
            "website. W e  call these types of edges dir ected . W e’ll look at both kinds of\n",
            "networks.\n",
            "B e t w e e n n e s s  C e n t r a l i t y\n",
            "In Chapter 1 , we  computed the key connectors in the DataSciencester\n",
            "network by counting the number of friends each user had. Now we have\n",
            "enough machinery to take a look at other approaches. W e will use the same\n",
            "network, but now we’ll use NamedTuple s for the data.\n",
            "Recall that the network ( Figure 22-1 ) comprised users:\n",
            "from typing import NamedTuple \n",
            " \n",
            "class User(NamedTuple): \n",
            "    id: int \n",
            "    name: strusers = [User(0, \"Hero\"), User(1, \"Dunn\"), User(2, \"Sue\"), User(3, \"Chi\"), \n",
            "         User(4, \"Thor\"), User(5, \"Clive\"), User(6, \"Hicks\"), \n",
            "         User(7, \"Devin\"), User(8, \"Kate\"), User(9, \"Klein\")]\n",
            "and friendships:\n",
            "friend_pairs = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4), \n",
            "                (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n",
            "Figur e 22-1. The DataSciencester network\n",
            "The friendships will be easier to work with as a dict :\n",
            "from typing import Dict, List \n",
            " \n",
            "# type alias for keeping track of Friendships \n",
            "Friendships = Dict[int, List[int]] \n",
            " \n",
            "friendships: Friendships = {user.id: [] for user in users} \n",
            " \n",
            "for i, j in friend_pairs: \n",
            "    friendships[i].append(j) \n",
            "    friendships[j].append(i) \n",
            " \n",
            "assert friendships[4] == [3, 5] \n",
            "assert friendships[8] == [6, 7, 9]\n",
            "When we left of f we were dissatisfied with our notion of degr ee centrality ,\n",
            "which didn’ t really agree with our intuition about who the key connectors of\n",
            "the network were.An alternative metric is betweenness centrality , which identifies people\n",
            "who frequently are on the shortest paths between pairs of other people. In\n",
            "particular , the betweenness centrality of node i  is computed by adding up,\n",
            "for every other pair of nodes j  and k , the proportion of shortest paths\n",
            "between node j  and node k  that pass through i .\n",
            "That is, to figure out Thor ’ s betweenness centrality , we’ll need to compute\n",
            "all the shortest paths between all pairs of people who aren’ t Thor . And then\n",
            "we’ll need to count how many of those shortest paths pass through Thor .\n",
            "For instance, the only shortest path between Chi ( id  3) and Clive ( id  5)\n",
            "passes through Thor , while neither of the two shortest paths between Hero\n",
            "(id  0) and Chi ( id  3) does.\n",
            "So, as a first step, we’ll need to figure out the shortest paths between all\n",
            "pairs of people. There are some pretty sophisticated algorithms for doing so\n",
            "ef ficiently , but (as is almost always the case) we will use a less ef ficient,\n",
            "easier -to-understand algorithm.\n",
            "This  algorithm (an implementation of breadth-first search) is one of the\n",
            "more complicated ones in the book, so let’ s talk through it carefully:\n",
            "1 . Our goal is a function that takes a from_user  and finds all  shortest\n",
            "paths to every other user .\n",
            "2 . W e’ll represent a path as a list  of user IDs. Since every path starts\n",
            "at from_user , we won’ t include her ID in the list. This means that\n",
            "the length of the list representing the path will be the length of the\n",
            "path itself.\n",
            "3 . W e’ll maintain a dictionary called shortest_paths_to  where the\n",
            "keys are user IDs and the values are lists of paths that end at the\n",
            "user with the specified ID. If there is a unique shortest path, the list\n",
            "will just contain that one path. If there are multiple shortest paths,\n",
            "the list will contain all of them.\n",
            "4 . W e’ll also maintain a queue called frontier  that contains the\n",
            "users we want to explore in the order we want to explore them.W e’ll store them as pairs (prev_user, user)  so that we know\n",
            "how we got to each one. W e initialize the queue with all the\n",
            "neighbors of from_user . (W e haven’ t talked about queues, which\n",
            "are data structures optimized for “add to the end” and “remove\n",
            "from the front” operations. In Python, they are implemented as\n",
            "collections.deque , which is actually a double-ended queue.)\n",
            "5 . As we explore the graph, whenever we find new neighbors that we\n",
            "don’ t already know the shortest paths to, we add them to the end of\n",
            "the queue to explore later , with the current user as prev_user .\n",
            "6 . When we take a user of f the queue, and we’ve never encountered\n",
            "that user before, we’ve definitely found one or more shortest paths\n",
            "to him—each of the shortest paths to prev_user  with one extra\n",
            "step added.\n",
            "7 . When we take a user of f the queue and we have  encountered that\n",
            "user before, then either we’ve found another shortest path (in\n",
            "which case we should add it) or we’ve found a longer path (in\n",
            "which case we shouldn’ t).\n",
            "8 . When no more users are left on the queue, we’ve explored the\n",
            "whole graph (or , at least, the parts of it that are reachable from the\n",
            "starting user) and we’re done.\n",
            "W e can put this all together into a (lar ge) function:\n",
            "from collections import deque \n",
            " \n",
            "Path = List[int] \n",
            " \n",
            "def shortest_paths_from(from_user_id: int, \n",
            "                        friendships: Friendships) -> Dict[int, List[Path]]: \n",
            "    # A dictionary from user_id to *all* shortest paths to that user. \n",
            "    shortest_paths_to: Dict[int, List[Path]] = {from_user_id: [[]]} \n",
            " \n",
            "    # A queue of (previous user, next user) that we need to check. \n",
            "    # Starts out with all pairs (from_user, friend_of_from_user). \n",
            "    frontier = deque((from_user_id, friend_id) \n",
            "                     for friend_id in friendships[from_user_id])# Keep going until we empty the queue. \n",
            "    while frontier: \n",
            "        # Remove the pair that's next in the queue. \n",
            "        prev_user_id, user_id = frontier.popleft() \n",
            " \n",
            "        # Because of the way we're adding to the queue, \n",
            "        # necessarily we already know some shortest paths to prev_user. \n",
            "        paths_to_prev_user = shortest_paths_to[prev_user_id] \n",
            "        new_paths_to_user = [path + [user_id] for path in paths_to_prev_user] \n",
            " \n",
            "        # It's possible we already know a shortest path to user_id. \n",
            "        old_paths_to_user = shortest_paths_to.get(user_id, []) \n",
            " \n",
            "        # What's the shortest path to here that we've seen so far? \n",
            "        if old_paths_to_user: \n",
            "            min_path_length = len(old_paths_to_user[0]) \n",
            "        else: \n",
            "            min_path_length = float('inf') \n",
            " \n",
            "        # Only keep paths that aren't too long and are actually new. \n",
            "        new_paths_to_user = [path \n",
            "                             for path in new_paths_to_user \n",
            "                             if len(path) <= min_path_length \n",
            "                             and path not in old_paths_to_user] \n",
            " \n",
            "        shortest_paths_to[user_id] = old_paths_to_user + new_paths_to_user \n",
            " \n",
            "        # Add never-seen neighbors to the frontier. \n",
            "        frontier.extend((user_id, friend_id) \n",
            "                        for friend_id in friendships[user_id] \n",
            "                        if friend_id not in shortest_paths_to) \n",
            " \n",
            "    return shortest_paths_to\n",
            "Now let’ s compute all the shortest paths:\n",
            "# For each from_user, for each to_user, a list of shortest paths. \n",
            "shortest_paths = {user.id: shortest_paths_from(user.id, friendships) \n",
            "                  for user in users}\n",
            "And we’re finally ready to compute betweenness centrality . For every pair\n",
            "of nodes i  and j , we know the n  shortest paths from i  to j . Then, for each of\n",
            "those paths, we just add 1/n to the centrality of each node on that path:betweenness_centrality = {user.id: 0.0 for user in users} \n",
            " \n",
            "for source in users: \n",
            "    for target_id, paths in shortest_paths[source.id].items(): \n",
            "        if source.id < target_id:      # don't double count \n",
            "            num_paths = len(paths)     # how many shortest paths? \n",
            "            contrib = 1 / num_paths    # contribution to centrality \n",
            "            for path in paths: \n",
            "                for between_id in path: \n",
            "                    if between_id not in [source.id, target_id]: \n",
            "                        betweenness_centrality[between_id] += contrib\n",
            "As shown in Figure 22-2 , users 0 and 9 have centrality 0 (as neither is on\n",
            "any shortest path between other users), whereas 3, 4, and 5 all have high\n",
            "centralities (as all three lie on many shortest paths).\n",
            "Figur e 22-2. The DataSciencester network sized by betweenness centrality\n",
            "N O T E\n",
            "Generally the centrality numbers aren’ t that meaningful themselves. What we care about\n",
            "is how the numbers for each node compare to the numbers for other nodes.\n",
            "Another  measure we can look at is closeness centrality . First, for each user\n",
            "we compute her farness , which is the sum of the lengths of her shortest\n",
            "paths to each other user . Since we’ve already computed the shortest paths\n",
            "between each pair of nodes, it’ s easy to add their lengths. (If there are\n",
            "multiple shortest paths, they all have the same length, so we can just look at\n",
            "the first one.)def farness(user_id: int) -> float: \n",
            "    \"\"\"the sum of the lengths of the shortest paths to each other user\"\"\" \n",
            "    return sum(len(paths[0]) \n",
            "               for paths in shortest_paths[user_id].values())\n",
            "after which it’ s very little work to compute closeness centrality ( Figure 22-\n",
            "3 ):\n",
            "closeness_centrality = {user.id: 1 / farness(user.id) for user in users}\n",
            "Figur e 22-3. The DataSciencester network sized by closeness centrality\n",
            "There is much less variation here—even the very central nodes are still\n",
            "pretty far from the nodes out on the periphery .\n",
            "As we saw , computing shortest paths is kind of a pain. For this reason,\n",
            "betweenness and closeness centrality aren’ t often used on lar ge networks.\n",
            "The less intuitive (but generally easier to compute) eigenvector centrality  is\n",
            "more frequently used.\n",
            "E i g e n v e c t o r  C e n t r a l i t y\n",
            "In  order to talk about eigenvector centrality , we have to talk about\n",
            "eigenvectors, and in order to talk about eigenvectors, we have to talk about\n",
            "matrix multiplication.\n",
            "Matrix MultiplicationIf A  is  an n×m  matrix and B  is an m×k  matrix (notice that the second\n",
            "dimension of A  is same as the first dimension of B ), then their product AB  is\n",
            "the n×k  matrix whose ( i , j )th entry is:\n",
            "Ai1B1j+Ai2B2j+⋯+AimBmj\n",
            "which is just the dot product of the i th row of A  (thought of as a vector)\n",
            "with the j th column of B  (also thought of as a vector).\n",
            "W e can implement this using the make_matrix  function from Chapter 4 :\n",
            "from scratch.linear_algebra import Matrix, make_matrix, shape \n",
            " \n",
            "def matrix_times_matrix(m1: Matrix, m2: Matrix) -> Matrix: \n",
            "    nr1, nc1 = shape(m1) \n",
            "    nr2, nc2 = shape(m2) \n",
            "    assert nc1 == nr2, \"must have (# of columns in m1) == (# of rows in m2)\" \n",
            " \n",
            "    def entry_fn(i: int, j: int) -> float: \n",
            "        \"\"\"dot product of i-th row of m1 with j-th column of m2\"\"\" \n",
            "        return sum(m1[i][k] * m2[k][j] for k in range(nc1)) \n",
            " \n",
            "    return make_matrix(nr1, nc2, entry_fn)\n",
            "If we think of an m -dimensional vector as an (m, 1)  matrix, we can\n",
            "multiply it by an (n, m)  matrix to get an (n, 1)  matrix, which we can then\n",
            "think of as an n -dimensional vector .\n",
            "This means another way to think about an (n, m)  matrix is as a linear\n",
            "mapping that transforms m -dimensional vectors into n -dimensional vectors:\n",
            "from scratch.linear_algebra import Vector, dot \n",
            " \n",
            "def matrix_times_vector(m: Matrix, v: Vector) -> Vector: \n",
            "    nr, nc = shape(m) \n",
            "    n = len(v) \n",
            "    assert nc == n, \"must have (# of cols in m) == (# of elements in v)\" \n",
            " \n",
            "    return [dot(row, v) for row in m]  # output has length nrWhen A  is a squar e  matrix, this operation maps n -dimensional vectors to\n",
            "other n -dimensional vectors. It’ s possible that, for some matrix A  and vector\n",
            "v , when A  operates on v  we get back a scalar multiple of v —that is, that the\n",
            "result is a vector that points in the same direction as v . When this happens\n",
            "(and when, in addition, v  is not a vector of all zeros), we call v  an\n",
            "eigenvector  of A . And we call the multiplier an eigenvalue .\n",
            "One possible way to find an eigenvector of A  is by picking a starting vector\n",
            "v , applying matrix_times_vector , rescaling the result to have magnitude\n",
            "1, and repeating until the process conver ges:\n",
            "from typing import Tuple \n",
            "import random \n",
            "from scratch.linear_algebra import magnitude, distance \n",
            " \n",
            "def find_eigenvector(m: Matrix, \n",
            "                     tolerance: float = 0.00001) -> Tuple[Vector, float]: \n",
            "    guess = [random.random() for _ in m] \n",
            " \n",
            "    while True: \n",
            "        result = matrix_times_vector(m, guess)    # transform guess \n",
            "        norm = magnitude(result)                  # compute norm \n",
            "        next_guess = [x / norm for x in result]   # rescale \n",
            " \n",
            "        if distance(guess, next_guess) < tolerance: \n",
            "            # convergence so return (eigenvector, eigenvalue) \n",
            "            return next_guess, norm \n",
            " \n",
            "        guess = next_guess\n",
            "By construction, the returned guess  is a vector such that, when you apply\n",
            "matrix_times_vector  to it and rescale it to have length 1, you get back a\n",
            "vector very close to itself—which means it’ s an eigenvector .\n",
            "Not all matrices of real numbers have eigenvectors and eigenvalues. For\n",
            "example, the matrix:\n",
            "rotate = [[ 0, 1], \n",
            "          [-1, 0]]rotates vectors 90 degrees clockwise, which means that the only vector it\n",
            "maps to a scalar multiple of itself is a vector of zeros. If you tried\n",
            "find_eigenvector(rotate)  it would run forever . Even matrices that have\n",
            "eigenvectors can sometimes get stuck in cycles. Consider the matrix:\n",
            "flip = [[0, 1], \n",
            "        [1, 0]]\n",
            "This matrix maps any vector [x, y]  to [y, x] . This means that, for\n",
            "example, [1, 1]  is an eigenvector with eigenvalue 1. However , if you start\n",
            "with a random vector with unequal coordinates, find_eigenvector  will\n",
            "just repeatedly swap the coordinates forever . (Not-from-scratch libraries\n",
            "like NumPy use dif ferent methods that would work in this case.)\n",
            "Nonetheless, when find_eigenvector  does return a result, that result is\n",
            "indeed an eigenvector .\n",
            "Centrality\n",
            "How  does this help us understand the DataSciencester network? T o start,\n",
            "we’ll need to represent the connections in our network as an\n",
            "adjacency_matrix , whose ( i , j )th entry is either 1 (if user i  and user j  are\n",
            "friends) or 0 (if they’re not):\n",
            "def entry_fn(i: int, j: int): \n",
            "    return 1 if (i, j) in friend_pairs or (j, i) in friend_pairs else 0 \n",
            " \n",
            "n = len(users) \n",
            "adjacency_matrix = make_matrix(n, n, entry_fn)\n",
            "The eigenvector centrality for each user is then the entry corresponding to\n",
            "that user in the eigenvector returned by find_eigenvector  ( Figure 22-4 ).Figur e 22-4. The DataSciencester network sized by eigenvector centrality\n",
            "N O T E\n",
            "For technical reasons that are way beyond the scope of this book, any nonzero\n",
            "adjacency matrix necessarily has an eigenvector , all of whose values are nonnegative.\n",
            "And fortunately for us, for this adjacency_matrix  our find_eigenvector  function\n",
            "finds it.\n",
            "eigenvector_centralities, _ = find_eigenvector(adjacency_matrix)\n",
            "Users with high eigenvector centrality should be those who have a lot of\n",
            "connections, and connections to people who themselves have high\n",
            "centrality .\n",
            "Here users 1 and 2 are the most central, as they both have three connections\n",
            "to people who are themselves highly central. As we move away from them,\n",
            "people’ s centralities steadily drop of f.\n",
            "On a network this small, eigenvector centrality behaves somewhat\n",
            "erratically . If you try adding or subtracting links, you’ll find that small\n",
            "changes in the network can dramatically change the centrality numbers. In a\n",
            "much lar ger network, this would not particularly be the case.\n",
            "W e still haven’ t motivated why an eigenvector might lead to a reasonable\n",
            "notion of centrality . Being an eigenvector means that if you compute:\n",
            "matrix_times_vector(adjacency_matrix, eigenvector_centralities)the result is a scalar multiple of eigenvector_centralities .\n",
            "If you look at how matrix multiplication works, matrix_times_vector\n",
            "produces a vector whose i th element is:\n",
            "dot(adjacency_matrix[i], eigenvector_centralities)\n",
            "which is precisely the sum of the eigenvector centralities of the users\n",
            "connected to user i .\n",
            "In other words, eigenvector centralities are numbers, one per user , such that\n",
            "each user ’ s value is a constant multiple of the sum of his neighbors’ values.\n",
            "In this case centrality means being connected to people who themselves are\n",
            "central. The more centrality you are directly connected to, the more central\n",
            "you are. This is of course a circular definition—eigenvectors are the way of\n",
            "breaking out of the circularity .\n",
            "Another way of understanding this is by thinking about what\n",
            "find_eigenvector  is doing here. It starts by assigning each node a random\n",
            "centrality . It then repeats the following two steps until the process\n",
            "conver ges:\n",
            "1 . Give each node a new centrality score that equals the sum of its\n",
            "neighbors’ (old) centrality scores.\n",
            "2 . Rescale the vector of centralities to have magnitude 1.\n",
            "Although the mathematics behind it may seem somewhat opaque at first,\n",
            "the calculation itself is relatively straightforward (unlike, say , betweenness\n",
            "centrality) and is pretty easy to perform on even very lar ge graphs. (At\n",
            "least, if you use a real linear algebra library it’ s easy to perform on lar ge\n",
            "graphs. If you used our matrices-as-lists implementation you’d struggle.)\n",
            "D i r e c t e d  G r a p h s  a n d  P a g e R a n k\n",
            "DataSciencester  isn’ t getting much traction, so the VP of Revenue considers\n",
            "pivoting from a friendship model to an endorsement model. It turns out thatno one particularly cares which data scientists are friends  with one another ,\n",
            "but tech recruiters care very much which data scientists are r espected  by\n",
            "other data scientists.\n",
            "In this new model, we’ll track endorsements (source, target)  that no\n",
            "longer represent a reciprocal relationship, but rather that source  endorses\n",
            "target  as an awesome data scientist ( Figure 22-5 ).\n",
            "Figur e 22-5. The DataSciencester network of endorsements\n",
            "W e’ll need to account for this asymmetry:\n",
            "endorsements = [(0, 1), (1, 0), (0, 2), (2, 0), (1, 2), \n",
            "                (2, 1), (1, 3), (2, 3), (3, 4), (5, 4), \n",
            "                (5, 6), (7, 5), (6, 8), (8, 7), (8, 9)]\n",
            "after which we can easily find the most_endorsed  data scientists and sell\n",
            "that information to recruiters:\n",
            "from collections import Counter \n",
            " \n",
            "endorsement_counts = Counter(target for source, target in endorsements)\n",
            "However , “number of endorsements” is an easy metric to game. All you\n",
            "need to do is create phony accounts and have them endorse you. Or arrange\n",
            "with your friends to endorse each other . (As users 0, 1, and 2 seem to have\n",
            "done.)\n",
            "A better metric would take into account who  endorses you. Endorsements\n",
            "from people who have a lot of endorsements should somehow count morethan endorsements from people with few endorsements. This is the essence\n",
            "of the PageRank algorithm, used by Google to rank websites based on\n",
            "which other websites link to them, which other websites link to those, and\n",
            "so on.\n",
            "(If this sort of reminds you of the idea behind eigenvector centrality , it\n",
            "should.)\n",
            "A simplified version looks like this:\n",
            "1 . There is a total of 1.0 (or 100%) PageRank in the network.\n",
            "2 . Initially this PageRank is equally distributed among nodes.\n",
            "3 . At each step, a lar ge fraction of each node’ s PageRank is\n",
            "distributed evenly among its outgoing links.\n",
            "4 . At each step, the remainder of each node’ s PageRank is distributed\n",
            "evenly among all nodes.\n",
            "import tqdm \n",
            " \n",
            "def page_rank(users: List[User], \n",
            "              endorsements: List[Tuple[int, int]], \n",
            "              damping: float = 0.85, \n",
            "              num_iters: int = 100) -> Dict[int, float]: \n",
            "    # Compute how many people each person endorses \n",
            "    outgoing_counts = Counter(target for source, target in endorsements) \n",
            " \n",
            "    # Initially distribute PageRank evenly \n",
            "    num_users = len(users) \n",
            "    pr = {user.id : 1 / num_users for user in users} \n",
            " \n",
            "    # Small fraction of PageRank that each node gets each iteration \n",
            "    base_pr = (1 - damping) / num_users \n",
            " \n",
            "    for iter in tqdm.trange(num_iters): \n",
            "        next_pr = {user.id : base_pr for user in users}  # start with base_pr \n",
            " \n",
            "        for source, target in endorsements: \n",
            "            # Add damped fraction of source pr to target \n",
            "            next_pr[target] += damping * pr[source] / outgoing_counts[source] \n",
            " \n",
            "        pr = next_prreturn pr\n",
            "If we compute page ranks:\n",
            "pr = page_rank(users, endorsements) \n",
            " \n",
            "# Thor (user_id 4) has higher page rank than anyone else \n",
            "assert pr[4] > max(page_rank \n",
            "                   for user_id, page_rank in pr.items() \n",
            "                   if user_id != 4)\n",
            "PageRank ( Figure 22-6 ) identifies user 4 (Thor) as the highest-ranked data\n",
            "scientist.\n",
            "Figur e 22-6. The DataSciencester network sized by PageRank\n",
            "Even though Thor has fewer endorsements (two) than users 0, 1, and 2, his\n",
            "endorsements carry with them rank from their endorsements. Additionally ,\n",
            "both of his endorsers endorsed only him, which means that he doesn’ t have\n",
            "to divide their rank with anyone else.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "There  are many other notions of centrality  besides the ones we\n",
            "used (although the ones we used are pretty much the most popular\n",
            "ones).\n",
            "NetworkX  is a  Python library for network analysis. It has functions\n",
            "for computing centralities and for visualizing graphs.Gephi  is  a love-it/hate-it GUI-based network visualization tool.Chapter 23. Recommender\n",
            "Systems\n",
            "O natur e, natur e, why art thou so dishonest, as ever to send men with\n",
            "these false r ecommendations into the world!\n",
            "— Henry Fielding\n",
            "Another common data problem is producing r ecommendations  of some sort.\n",
            "Netflix recommends movies you might want to watch. Amazon\n",
            "recommends products you might want to buy . T witter recommends users\n",
            "you might want to follow . In this chapter , we’ll look at several ways to use\n",
            "data to make recommendations.\n",
            "In  particular , we’ll look at the dataset of users_interests  that we’ve used\n",
            "before:\n",
            "users_interests = [ \n",
            "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"], \n",
            "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"], \n",
            "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"], \n",
            "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"], \n",
            "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"], \n",
            "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"], \n",
            "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"], \n",
            "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"], \n",
            "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial \n",
            "intelligence\"], \n",
            "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"], \n",
            "    [\"statistics\", \"R\", \"statsmodels\"], \n",
            "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"], \n",
            "    [\"pandas\", \"R\", \"Python\"], \n",
            "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"], \n",
            "    [\"libsvm\", \"regression\", \"support vector machines\"] \n",
            "]\n",
            "And we’ll think about the problem of recommending new interests to a user\n",
            "based on her currently specified interests.M a n u a l  C u r a t i o n\n",
            "Before  the internet, when you needed book recommendations you would go\n",
            "to the library , where a librarian was available to suggest books that were\n",
            "relevant to your interests or similar to books you liked.\n",
            "Given DataSciencester ’ s limited number of users and interests, it would be\n",
            "easy for you to spend an afternoon manually recommending interests for\n",
            "each user . But this method doesn’ t scale particularly well, and it’ s limited\n",
            "by your personal knowledge and imagination. (Not that I’m suggesting that\n",
            "your personal knowledge and imagination are limited.) So let’ s think about\n",
            "what we can do with data .\n",
            "R e c o m m e n d i n g  W h a t ’ s  P o p u l a r\n",
            "One  easy approach is to simply recommend what’ s popular:\n",
            "from collections import Counter \n",
            " \n",
            "popular_interests = Counter(interest \n",
            "                            for user_interests in users_interests \n",
            "                            for interest in user_interests)\n",
            "which looks like:\n",
            "[('Python', 4), \n",
            " ('R', 4), \n",
            " ('Java', 3), \n",
            " ('regression', 3), \n",
            " ('statistics', 3), \n",
            " ('probability', 3), \n",
            " # ... \n",
            "]\n",
            "Having computed this, we can just suggest to a user the most popular\n",
            "interests that he’ s not already interested in:\n",
            "from typing import List, Tupledef most_popular_new_interests( \n",
            "        user_interests: List[str], \n",
            "        max_results: int = 5) -> List[Tuple[str, int]]: \n",
            "    suggestions = [(interest, frequency) \n",
            "                   for interest, frequency in popular_interests.most_common() \n",
            "                   if interest not in user_interests] \n",
            "    return suggestions[:max_results]\n",
            "So, if you are user 1, with interests:\n",
            "[\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"]\n",
            "then we’d recommend you:\n",
            "[('Python', 4), ('R', 4), ('Java', 3), ('regression', 3), ('statistics', 3)]\n",
            "If you are user 3, who’ s already interested in many of those things, you’d\n",
            "instead get:\n",
            "[('Java', 3), ('HBase', 3), ('Big Data', 3), \n",
            " ('neural networks', 2), ('Hadoop', 2)]\n",
            "Of course, “lots of people are interested in Python, so maybe you should be\n",
            "too” is not the most compelling sales pitch. If someone is brand new to our\n",
            "site and we don’ t know anything about them, that’ s possibly the best we can\n",
            "do. Let’ s see how we can do better by basing each user ’ s recommendations\n",
            "on her existing interests.\n",
            "U s e r - B a s e d  C o l l a b o r a t i v e  F i l t e r i n g\n",
            "One  way of taking a user ’ s interests into account is to look for users who\n",
            "are somehow similar  to her , and then suggest the things that those users are\n",
            "interested in.\n",
            "In order to do that, we’ll need a way to measure how similar two users are.\n",
            "Here we’ll use cosine similarity , which we used in Chapter 21  to measure\n",
            "how similar two word vectors were.W e’ll apply this to vectors of 0s and 1s, each vector v  representing one\n",
            "user ’ s interests. v[i]  will be 1 if the user specified the i th interest, and 0\n",
            "otherwise. Accordingly , “similar users” will mean “users whose interest\n",
            "vectors most nearly point in the same direction.” Users with identical\n",
            "interests will have similarity 1. Users with no identical interests will have\n",
            "similarity 0. Otherwise, the similarity will fall in between, with numbers\n",
            "closer to 1 indicating “very similar” and numbers closer to 0 indicating “not\n",
            "very similar .”\n",
            "A good place to start is collecting the known interests and (implicitly)\n",
            "assigning indices to them. W e can do this by using a set comprehension to\n",
            "find the unique interests, and then sorting them into a list. The first interest\n",
            "in the resulting list will be interest 0, and so on:\n",
            "unique_interests = sorted({interest \n",
            "                           for user_interests in users_interests \n",
            "                           for interest in user_interests})\n",
            "This gives us a list that starts:\n",
            "assert unique_interests[:6] == [ \n",
            "    'Big Data', \n",
            "    'C++', \n",
            "    'Cassandra', \n",
            "    'HBase', \n",
            "    'Hadoop', \n",
            "    'Haskell', \n",
            "    # ... \n",
            "]\n",
            "Next we want to produce an “interest” vector of 0s and 1s for each user . W e\n",
            "just need to iterate over the unique_interests  list, substituting a 1 if the\n",
            "user has each interest, and a 0 if not:\n",
            "def make_user_interest_vector(user_interests: List[str]) -> List[int]: \n",
            "    \"\"\" \n",
            "    Given a list of interests, produce a vector whose ith element is 1 \n",
            "    if unique_interests[i] is in the list, 0 otherwise \n",
            "    \"\"\"return [1 if interest in user_interests else 0 \n",
            "            for interest in unique_interests]\n",
            "And now we can make a list of user interest vectors:\n",
            "user_interest_vectors = [make_user_interest_vector(user_interests) \n",
            "                         for user_interests in users_interests]\n",
            "Now user_interest_vectors[i][j]  equals 1 if user i  specified interest\n",
            "j , and 0 otherwise.\n",
            "Because we have a small dataset, it’ s no problem to compute the pairwise\n",
            "similarities between all of our users:\n",
            "from scratch.nlp import cosine_similarity \n",
            " \n",
            "user_similarities = [[cosine_similarity(interest_vector_i, interest_vector_j) \n",
            "                      for interest_vector_j in user_interest_vectors] \n",
            "                     for interest_vector_i in user_interest_vectors]\n",
            "after which user_similarities[i][j]  gives us the similarity between\n",
            "users i  and j :\n",
            "# Users 0 and 9 share interests in Hadoop, Java, and Big Data \n",
            "assert 0.56 < user_similarities[0][9] < 0.58, \"several shared interests\" \n",
            " \n",
            "# Users 0 and 8 share only one interest: Big Data \n",
            "assert 0.18 < user_similarities[0][8] < 0.20, \"only one shared interest\"\n",
            "In particular , user_similarities[i]  is the vector of user i ’ s similarities\n",
            "to every other user . W e can use this to write a function that finds the most\n",
            "similar users to a given user . W e’ll make sure not to include the user herself,\n",
            "nor any users with zero similarity . And we’ll sort the results from most\n",
            "similar to least similar:\n",
            "def most_similar_users_to(user_id: int) -> List[Tuple[int, float]]: \n",
            "    pairs = [(other_user_id, similarity)                      # Find other \n",
            "             for other_user_id, similarity in                 # users with \n",
            "                enumerate(user_similarities[user_id])         # nonzero \n",
            "             if user_id != other_user_id and similarity > 0]  # similarity.return sorted(pairs,                                      # Sort them \n",
            "                  key=lambda pair: pair[-1],                  # most similar \n",
            "                  reverse=True)                               # first.\n",
            "For instance, if we call most_similar_users_to(0)  we get:\n",
            "[(9, 0.5669467095138409), \n",
            " (1, 0.3380617018914066), \n",
            " (8, 0.1889822365046136), \n",
            " (13, 0.1690308509457033), \n",
            " (5, 0.1543033499620919)]\n",
            "How do we use this to suggest new interests to a user? For each interest, we\n",
            "can just add up the user similarities of the other users interested in it:\n",
            "from collections import defaultdict \n",
            " \n",
            "def user_based_suggestions(user_id: int, \n",
            "                           include_current_interests: bool = False): \n",
            "    # Sum up the similarities \n",
            "    suggestions: Dict[str, float] = defaultdict(float) \n",
            "    for other_user_id, similarity in most_similar_users_to(user_id): \n",
            "        for interest in users_interests[other_user_id]: \n",
            "            suggestions[interest] += similarity \n",
            " \n",
            "    # Convert them to a sorted list \n",
            "    suggestions = sorted(suggestions.items(), \n",
            "                         key=lambda pair: pair[-1],  # weight \n",
            "                         reverse=True) \n",
            " \n",
            "    # And (maybe) exclude already interests \n",
            "    if include_current_interests: \n",
            "        return suggestions \n",
            "    else: \n",
            "        return [(suggestion, weight) \n",
            "                for suggestion, weight in suggestions \n",
            "                if suggestion not in users_interests[user_id]]\n",
            "If we call user_based_suggestions(0) , the first several suggested\n",
            "interests are:[('MapReduce', 0.5669467095138409), \n",
            " ('MongoDB', 0.50709255283711), \n",
            " ('Postgres', 0.50709255283711), \n",
            " ('NoSQL', 0.3380617018914066), \n",
            " ('neural networks', 0.1889822365046136), \n",
            " ('deep learning', 0.1889822365046136), \n",
            " ('artificial intelligence', 0.1889822365046136), \n",
            " #... \n",
            "]\n",
            "These seem like pretty decent suggestions for someone whose stated\n",
            "interests are “Big Data” and database-related. (The weights aren’ t\n",
            "intrinsically meaningful; we just use them for ordering.)\n",
            "This approach doesn’ t work as well when the number of items gets very\n",
            "lar ge. Recall the curse of dimensionality from Chapter 12 —in lar ge-\n",
            "dimensional vector spaces most vectors are very far apart (and also point in\n",
            "very dif ferent directions). That is, when there are a lar ge number of interests\n",
            "the “most similar users” to a given user might not be similar at all.\n",
            "Imagine a site like Amazon.com, from which I’ve bought thousands of\n",
            "items over the last couple of decades. Y ou could attempt to identify similar\n",
            "users to me based on buying patterns, but most likely in all the world there’ s\n",
            "no one whose purchase history looks even remotely like mine. Whoever my\n",
            "“most similar” shopper is, he’ s probably not similar to me at all, and his\n",
            "purchases would almost certainly make for lousy recommendations.\n",
            "I t e m - B a s e d  C o l l a b o r a t i v e  F i l t e r i n g\n",
            "An  alternative approach is to compute similarities between interests\n",
            "directly . W e can then generate suggestions for each user by aggregating\n",
            "interests that are similar to her current interests.\n",
            "T o start with, we’ll want to transpose  our user -interest matrix so that rows\n",
            "correspond to interests and columns correspond to users:\n",
            "interest_user_matrix = [[user_interest_vector[j] \n",
            "                         for user_interest_vector in user_interest_vectors] \n",
            "                        for j, _ in enumerate(unique_interests)]What does this look like? Row j  of interest_user_matrix  is column j  of\n",
            "user_interest_matrix . That is, it has 1 for each user with that interest\n",
            "and 0 for each user without that interest.\n",
            "For example, unique_interests[0]  is Big Data, and so\n",
            "interest_user_matrix[0]  is:\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
            "because users 0, 8, and 9 indicated interest in Big Data.\n",
            "W e can now use cosine similarity again. If precisely the same users are\n",
            "interested in two topics, their similarity will be 1. If no two users are\n",
            "interested in both topics, their similarity will be 0:\n",
            "interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j) \n",
            "                          for user_vector_j in interest_user_matrix] \n",
            "                         for user_vector_i in interest_user_matrix]\n",
            "For example, we can find the interests most similar to Big Data (interest 0)\n",
            "using:\n",
            "def most_similar_interests_to(interest_id: int): \n",
            "    similarities = interest_similarities[interest_id] \n",
            "    pairs = [(unique_interests[other_interest_id], similarity) \n",
            "             for other_interest_id, similarity in enumerate(similarities) \n",
            "             if interest_id != other_interest_id and similarity > 0] \n",
            "    return sorted(pairs, \n",
            "                  key=lambda pair: pair[-1], \n",
            "                  reverse=True)\n",
            "which suggests the following similar interests:\n",
            "[('Hadoop', 0.8164965809277261), \n",
            " ('Java', 0.6666666666666666), \n",
            " ('MapReduce', 0.5773502691896258), \n",
            " ('Spark', 0.5773502691896258), \n",
            " ('Storm', 0.5773502691896258), \n",
            " ('Cassandra', 0.4082482904638631), \n",
            " ('artificial intelligence', 0.4082482904638631), \n",
            " ('deep learning', 0.4082482904638631),('neural networks', 0.4082482904638631), \n",
            " ('HBase', 0.3333333333333333)]\n",
            "Now we can create recommendations for a user by summing up the\n",
            "similarities of the interests similar to his:\n",
            "def item_based_suggestions(user_id: int, \n",
            "                           include_current_interests: bool = False): \n",
            "    # Add up the similar interests \n",
            "    suggestions = defaultdict(float) \n",
            "    user_interest_vector = user_interest_vectors[user_id] \n",
            "    for interest_id, is_interested in enumerate(user_interest_vector): \n",
            "        if is_interested == 1: \n",
            "            similar_interests = most_similar_interests_to(interest_id) \n",
            "            for interest, similarity in similar_interests: \n",
            "                suggestions[interest] += similarity \n",
            " \n",
            "    # Sort them by weight \n",
            "    suggestions = sorted(suggestions.items(), \n",
            "                         key=lambda pair: pair[-1], \n",
            "                         reverse=True) \n",
            " \n",
            "    if include_current_interests: \n",
            "        return suggestions \n",
            "    else: \n",
            "        return [(suggestion, weight) \n",
            "                for suggestion, weight in suggestions \n",
            "                if suggestion not in users_interests[user_id]]\n",
            "For user 0, this generates the following (seemingly reasonable)\n",
            "recommendations:\n",
            "[('MapReduce', 1.861807319565799), \n",
            " ('Postgres', 1.3164965809277263), \n",
            " ('MongoDB', 1.3164965809277263), \n",
            " ('NoSQL', 1.2844570503761732), \n",
            " ('programming languages', 0.5773502691896258), \n",
            " ('MySQL', 0.5773502691896258), \n",
            " ('Haskell', 0.5773502691896258), \n",
            " ('databases', 0.5773502691896258), \n",
            " ('neural networks', 0.4082482904638631), \n",
            " ('deep learning', 0.4082482904638631), \n",
            " ('C++', 0.4082482904638631), \n",
            " ('artificial intelligence', 0.4082482904638631),('Python', 0.2886751345948129), \n",
            " ('R', 0.2886751345948129)]\n",
            "M a t r i x  F a c t o r i z a t i o n\n",
            "As  we’ve seen, we can represent our users’ preferences as a [num_users,\n",
            "num_items]  matrix of 0s and 1s, where the 1s represent liked items and the\n",
            "0s unliked items.\n",
            "Sometimes you might actually have numeric ratings ; for example, when\n",
            "you write an Amazon review you assign the item a score ranging from 1 to\n",
            "5 stars. Y ou could still represent these by numbers in a [num_users,\n",
            "num_items]  matrix (ignoring for now the problem of what to do about\n",
            "unrated items).\n",
            "In this section we’ll assume we have such ratings data and try to learn a\n",
            "model that can predict the rating for a given user and item.\n",
            "One way of approaching the problem is to assume that every user has some\n",
            "latent “type,” which can be represented as a vector of numbers, and that\n",
            "each item similarly has some latent “type.”\n",
            "If the user types are represented as a [num_users, dim]  matrix, and the\n",
            "transpose of the item types is represented as a [dim, num_items]  matrix,\n",
            "their product is a [num_users, num_items]  matrix. Accordingly , one way\n",
            "of building such a model is by “factoring” the preferences matrix into the\n",
            "product of a user matrix and an item matrix.\n",
            "(Possibly this idea of latent types reminds you of the word embeddings we\n",
            "developed in Chapter 21 . Hold on to that idea.)\n",
            "Rather than working with our made-up 10-user dataset, we’ll work with  the\n",
            "MovieLens 100k dataset, which contains ratings from 0 to 5 for many\n",
            "movies from many users. Each user has only rated a small subset of the\n",
            "movies. W e’ll use this to try to build a system that can predict the rating for\n",
            "any given (user , movie) pair . W e’ll train it to predict well on the movieseach user has rated; hopefully then it will generalize to movies the user\n",
            "hasn’ t rated.\n",
            "T o start with, let’ s acquire the dataset. Y ou can download it from\n",
            "http://files.gr ouplens.or g/datasets/movielens/ml-100k.zip .\n",
            "Unzip it and extract the files; we’ll only use two of them:\n",
            "# This points to the current directory, modify if your files are elsewhere. \n",
            "MOVIES = \"u.item\"   # pipe-delimited: movie_id|title|... \n",
            "RATINGS = \"u.data\"  # tab-delimited: user_id, movie_id, rating, timestamp\n",
            "As is often the case, we’ll introduce a NamedTuple  to make things easier to\n",
            "work with:\n",
            "from typing import NamedTuple \n",
            " \n",
            "class Rating(NamedTuple): \n",
            "    user_id: str \n",
            "    movie_id: str \n",
            "    rating: float\n",
            "N O T E\n",
            "The movie ID and user IDs are actually integers, but they’re not consecutive, which\n",
            "means if we worked with them as integers we’d end up with a lot of wasted dimensions\n",
            "(unless we renumbered everything). So to keep it simpler we’ll just treat them as strings.\n",
            "Now let’ s read in the data and explore it. The movies file is pipe-delimited\n",
            "and has many columns. W e only care about the first two, which are the ID\n",
            "and the title:\n",
            "import csv \n",
            "# We specify this encoding to avoid a UnicodeDecodeError. \n",
            "# See: https://stackoverflow.com/a/53136168/1076346. \n",
            "with open(MOVIES, encoding=\"iso-8859-1\") as f: \n",
            "    reader = csv.reader(f, delimiter=\"|\") \n",
            "    movies = {movie_id: title for movie_id, title, *_ in reader}The ratings file is tab-delimited and contains four columns for user_id ,\n",
            "movie_id , rating  (1 to 5), and timestamp . W e’ll ignore the timestamp, as\n",
            "we don’ t need it:\n",
            "# Create a list of [Rating] \n",
            "with open(RATINGS, encoding=\"iso-8859-1\") as f: \n",
            "    reader = csv.reader(f, delimiter=\"\\t\") \n",
            "    ratings = [Rating(user_id, movie_id, float(rating)) \n",
            "               for user_id, movie_id, rating, _ in reader] \n",
            " \n",
            "# 1682 movies rated by 943 users \n",
            "assert len(movies) == 1682 \n",
            "assert len(list({rating.user_id for rating in ratings})) == 943\n",
            "There’ s a lot of interesting exploratory analysis you can do on this data; for\n",
            "instance, you might be interested in the average ratings for Star W ars\n",
            "movies (the dataset is from 1998, which means it predates The Phantom\n",
            "Menace  by a year):\n",
            "import re \n",
            " \n",
            "# Data structure for accumulating ratings by movie_id \n",
            "star_wars_ratings = {movie_id: [] \n",
            "                     for movie_id, title in movies.items() \n",
            "                     if re.search(\"Star Wars|Empire Strikes|Jedi\", title)} \n",
            " \n",
            "# Iterate over ratings, accumulating the Star Wars ones \n",
            "for rating in ratings: \n",
            "    if rating.movie_id in star_wars_ratings: \n",
            "        star_wars_ratings[rating.movie_id].append(rating.rating) \n",
            " \n",
            "# Compute the average rating for each movie \n",
            "avg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id) \n",
            "               for movie_id, title_ratings in star_wars_ratings.items()] \n",
            " \n",
            "# And then print them in order \n",
            "for avg_rating, movie_id in sorted(avg_ratings, reverse=True): \n",
            "    print(f\"{avg_rating:.2f} {movies[movie_id]}\")\n",
            "They’re all pretty highly rated:4.36 Star Wars (1977) \n",
            "4.20 Empire Strikes Back, The (1980) \n",
            "4.01 Return of the Jedi (1983)\n",
            "So let’ s try to come up with a model to predict these ratings. As a first step,\n",
            "let’ s split the ratings data into train, validation, and test sets:\n",
            "import random \n",
            "random.seed(0) \n",
            "random.shuffle(ratings) \n",
            " \n",
            "split1 = int(len(ratings) * 0.7) \n",
            "split2 = int(len(ratings) * 0.85) \n",
            " \n",
            "train = ratings[:split1]              # 70% of the data \n",
            "validation = ratings[split1:split2]   # 15% of the data \n",
            "test = ratings[split2:]               # 15% of the data\n",
            "It’ s always good to have a simple baseline model and make sure that ours\n",
            "does better than that. Here a simple baseline model might be “predict the\n",
            "average rating.” W e’ll be using mean squared error as our metric, so let’ s\n",
            "see how the baseline does on our test set:\n",
            "avg_rating = sum(rating.rating for rating in train) / len(train) \n",
            "baseline_error = sum((rating.rating - avg_rating) ** 2 \n",
            "                     for rating in test) / len(test) \n",
            " \n",
            "# This is what we hope to do better than \n",
            "assert 1.26 < baseline_error < 1.27\n",
            "Given our embeddings, the predicted ratings are given by the matrix\n",
            "product of the user embeddings and the movie embeddings. For a given\n",
            "user and movie, that value is just the dot product of the corresponding\n",
            "embeddings.\n",
            "So let’ s start by creating the embeddings. W e’ll represent them as dict s\n",
            "where the keys are IDs and the values are vectors, which will allow us to\n",
            "easily retrieve the embedding for a given ID:from scratch.deep_learning import random_tensor \n",
            " \n",
            "EMBEDDING_DIM = 2 \n",
            " \n",
            "# Find unique ids \n",
            "user_ids = {rating.user_id for rating in ratings} \n",
            "movie_ids = {rating.movie_id for rating in ratings} \n",
            " \n",
            "# Then create a random vector per id \n",
            "user_vectors = {user_id: random_tensor(EMBEDDING_DIM) \n",
            "                for user_id in user_ids} \n",
            "movie_vectors = {movie_id: random_tensor(EMBEDDING_DIM) \n",
            "                 for movie_id in movie_ids}\n",
            "By now we should be pretty expert at writing training loops:\n",
            "from typing import List \n",
            "import tqdm \n",
            "from scratch.linear_algebra import dot \n",
            " \n",
            "def loop(dataset: List[Rating], \n",
            "         learning_rate: float = None) -> None: \n",
            "    with tqdm.tqdm(dataset) as t: \n",
            "        loss = 0.0 \n",
            "        for i, rating in enumerate(t): \n",
            "            movie_vector = movie_vectors[rating.movie_id] \n",
            "            user_vector = user_vectors[rating.user_id] \n",
            "            predicted = dot(user_vector, movie_vector) \n",
            "            error = predicted - rating.rating \n",
            "            loss += error ** 2 \n",
            " \n",
            "            if learning_rate is not None: \n",
            "                #     predicted = m_0 * u_0 + ... + m_k * u_k \n",
            "                # So each u_j enters output with coefficent m_j \n",
            "                # and each m_j enters output with coefficient u_j \n",
            "                user_gradient = [error * m_j for m_j in movie_vector] \n",
            "                movie_gradient = [error * u_j for u_j in user_vector] \n",
            " \n",
            "                # Take gradient steps \n",
            "                for j in range(EMBEDDING_DIM): \n",
            "                    user_vector[j] -= learning_rate * user_gradient[j] \n",
            "                    movie_vector[j] -= learning_rate * movie_gradient[j] \n",
            " \n",
            "            t.set_description(f\"avg loss: {loss / (i + 1)}\")And now we can train our model (that is, find the optimal embeddings). For\n",
            "me it worked best if I decreased the learning rate a little each epoch:\n",
            "learning_rate = 0.05 \n",
            "for epoch in range(20): \n",
            "    learning_rate *= 0.9 \n",
            "    print(epoch, learning_rate) \n",
            "    loop(train, learning_rate=learning_rate) \n",
            "    loop(validation) \n",
            "loop(test)\n",
            "This model is pretty apt to overfit the training set. I got the best results with\n",
            "EMBEDDING_DIM=2 , which got me an average loss on the test set of about\n",
            "0.89.\n",
            "N O T E\n",
            "If you wanted higher -dimensional embeddings, you could try regularization like we\n",
            "used in “Regularization” . In particular , at each gradient update you could shrink the\n",
            "weights toward 0. I was not able to get any better results that way .\n",
            "Now , inspect the learned vectors. There’ s no reason to expect the two\n",
            "components to be particularly meaningful, so we’ll use principal component\n",
            "analysis:\n",
            "from scratch.working_with_data import pca, transform \n",
            " \n",
            "original_vectors = [vector for vector in movie_vectors.values()] \n",
            "components = pca(original_vectors, 2)\n",
            "Let’ s transform our vectors to represent the principal components and join\n",
            "in the movie IDs and average ratings:\n",
            "ratings_by_movie = defaultdict(list) \n",
            "for rating in ratings: \n",
            "    ratings_by_movie[rating.movie_id].append(rating.rating) \n",
            " \n",
            "vectors = [(movie_id, \n",
            "     sum(ratings_by_movie[movie_id]) / len(ratings_by_movie[movie_id]), \n",
            "     movies[movie_id], \n",
            "     vector) \n",
            "    for movie_id, vector in zip(movie_vectors.keys(), \n",
            "                                transform(original_vectors, components)) \n",
            "] \n",
            " \n",
            "# Print top 25 and bottom 25 by first principal component \n",
            "print(sorted(vectors, key=lambda v: v[-1][0])[:25]) \n",
            "print(sorted(vectors, key=lambda v: v[-1][0])[-25:])\n",
            "The top 25 are all highly rated, while the bottom 25 are mostly low-rated\n",
            "(or unrated in the training data), which suggests that the first principal\n",
            "component is mostly capturing “how good is this movie?”\n",
            "It’ s hard for me to make much sense of the second component; and, indeed\n",
            "the two-dimensional embeddings performed only slightly better than the\n",
            "one-dimensional embeddings, suggesting that whatever the second\n",
            "component captured is possibly very subtle. (Presumably one of the lar ger\n",
            "MovieLens datasets would have more interesting things going on.)\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "Surprise  is a Python  library for “building and analyzing\n",
            "recommender systems” that seems reasonably popular and up-to-\n",
            "date.\n",
            "The Netflix Prize  was a  somewhat famous competition to build a\n",
            "better system to recommend movies to Netflix users.Chapter 24. Databases and SQL\n",
            "Memory is man’ s gr eatest friend and worst enemy .\n",
            "— Gilbert Parker\n",
            "The data you need will often live in databases , systems designed for\n",
            "ef ficiently storing and querying data. The  bulk of these are r elational\n",
            "databases, such as PostgreSQL, MySQL, and SQL Server , which store data\n",
            "in tables  and are typically queried using Structured Query Language (SQL),\n",
            "a declarative language for manipulating data.\n",
            "SQL is a pretty essential part of the data scientist’ s toolkit. In this chapter ,\n",
            "we’ll create NotQuiteABase, a Python implementation of something that’ s\n",
            "not quite a database. W e’ll also cover the basics of SQL while showing how\n",
            "they work in our not-quite database, which is the most “from scratch” way I\n",
            "could think of to help you understand what they’re doing. My hope is that\n",
            "solving problems in NotQuiteABase will give you a good sense of how you\n",
            "might solve the same problems using SQL.\n",
            "C R E A T E  T A B L E  a n d  I N S E R T\n",
            "A  relational database is a collection of tables, and of relationships among\n",
            "them. A table is simply a collection of rows, not unlike some of the matrices\n",
            "we’ve been working with. However , a table also has associated with it a\n",
            "fixed schema  consisting of column names and column types.\n",
            "For example, imagine a users  dataset containing for each user her\n",
            "user_id , name , and num_friends :\n",
            "users = [[0, \"Hero\", 0], \n",
            "         [1, \"Dunn\", 2], \n",
            "         [2, \"Sue\", 3], \n",
            "         [3, \"Chi\", 3]]In SQL, we might create this table with:\n",
            "CREATE TABLE users ( \n",
            "    user_id INT NOT NULL, \n",
            "    name VARCHAR(200), \n",
            "    num_friends INT);\n",
            "Notice that we specified that the user_id  and num_friends  must be\n",
            "integers (and that user_id  isn’ t allowed to be NULL , which indicates a\n",
            "missing value and is sort of like our None ) and that the name should be a\n",
            "string of length 200 or less. W e’ll use Python types in a similar way .\n",
            "N O T E\n",
            "SQL is almost completely case and indentation insensitive. The capitalization and\n",
            "indentation style here is my preferred style. If you start learning SQL, you will surely\n",
            "encounter other examples styled dif ferently .\n",
            "Y ou can insert the rows with INSERT  statements:\n",
            "INSERT INTO users (user_id, name, num_friends) VALUES (0, 'Hero', 0);\n",
            "Notice also that SQL statements need to end with semicolons, and that SQL\n",
            "requires single quotes for its strings.\n",
            "In NotQuiteABase, you’ll create a Table  by specifying a similar schema.\n",
            "Then to insert a row , you’ll use the table’ s insert  method, which takes a\n",
            "list  of row values that need to be in the same order as the table’ s column\n",
            "names.\n",
            "Behind the scenes, we’ll store each row as a dict  from column names to\n",
            "values. A real database would never use such a space-wasting\n",
            "representation, but doing so will make NotQuiteABase much easier to work\n",
            "with.W e’ll implement the NotQuiteABase Table  as a giant class, which we’ll\n",
            "implement one method at a time. Let’ s start by getting out of the way some\n",
            "imports and type aliases:\n",
            "from typing import Tuple, Sequence, List, Any, Callable, Dict, Iterator \n",
            "from collections import defaultdict \n",
            " \n",
            "# A few type aliases we'll use later \n",
            "Row = Dict[str, Any]                        # A database row \n",
            "WhereClause = Callable[[Row], bool]         # Predicate for a single row \n",
            "HavingClause = Callable[[List[Row]], bool]  # Predicate over multiple rows\n",
            "Let’ s start with the constructor . T o create a NotQuiteABase table, we’ll\n",
            "need to pass in a list of column names, and a list of column types, just as\n",
            "you would if you were creating a table in a SQL database:\n",
            "class Table: \n",
            "    def __init__(self, columns: List[str], types: List[type]) -> None: \n",
            "        assert len(columns) == len(types), \"# of columns must == # of types\" \n",
            " \n",
            "        self.columns = columns         # Names of columns \n",
            "        self.types = types             # Data types of columns \n",
            "        self.rows: List[Row] = []      # (no data yet)\n",
            "W e’ll add a helper method to get the type of a column:\n",
            "    def col2type(self, col: str) -> type: \n",
            "        idx = self.columns.index(col)      # Find the index of the column, \n",
            "        return self.types[idx]             # and return its type.\n",
            "And we’ll add an insert  method that checks that the values you’re\n",
            "inserting are valid. In particular , you have to provide the correct number of\n",
            "values, and each has to be the correct type (or None ):\n",
            "    def insert(self, values: list) -> None: \n",
            "        # Check for right # of values \n",
            "        if len(values) != len(self.types): \n",
            "            raise ValueError(f\"You need to provide {len(self.types)} values\") \n",
            " \n",
            "        # Check for right types of values \n",
            "        for value, typ3 in zip(values, self.types):if not isinstance(value, typ3) and value is not None: \n",
            "                raise TypeError(f\"Expected type {typ3} but got {value}\") \n",
            " \n",
            "        # Add the corresponding dict as a \"row\" \n",
            "        self.rows.append(dict(zip(self.columns, values)))\n",
            "In an actual SQL database you’d explicitly specify whether any given\n",
            "column was allowed to contain null ( None ) values; to make our lives\n",
            "simpler we’ll just say that any column can.\n",
            "W e’ll also introduce a few dunder methods that allow us to treat a table like\n",
            "a List[Row] , which we’ll mostly use for testing our code:\n",
            "    def __getitem__(self, idx: int) -> Row: \n",
            "        return self.rows[idx] \n",
            " \n",
            "    def __iter__(self) -> Iterator[Row]: \n",
            "        return iter(self.rows) \n",
            " \n",
            "    def __len__(self) -> int: \n",
            "        return len(self.rows)\n",
            "And we’ll add a method to pretty-print our table:\n",
            "    def __repr__(self): \n",
            "        \"\"\"Pretty representation of the table: columns then rows\"\"\" \n",
            "        rows = \"\\n\".join(str(row) for row in self.rows) \n",
            " \n",
            "        return f\"{self.columns}\\n{rows}\"\n",
            "Now we can create our Users  table:\n",
            "# Constructor requires column names and types \n",
            "users = Table(['user_id', 'name', 'num_friends'], [int, str, int]) \n",
            "users.insert([0, \"Hero\", 0]) \n",
            "users.insert([1, \"Dunn\", 2]) \n",
            "users.insert([2, \"Sue\", 3]) \n",
            "users.insert([3, \"Chi\", 3]) \n",
            "users.insert([4, \"Thor\", 3]) \n",
            "users.insert([5, \"Clive\", 2]) \n",
            "users.insert([6, \"Hicks\", 3]) \n",
            "users.insert([7, \"Devin\", 2]) \n",
            "users.insert([8, \"Kate\", 2])users.insert([9, \"Klein\", 3]) \n",
            "users.insert([10, \"Jen\", 1])\n",
            "If you now print(users) , you’ll see:\n",
            "['user_id', 'name', 'num_friends'] \n",
            "{'user_id': 0, 'name': 'Hero', 'num_friends': 0} \n",
            "{'user_id': 1, 'name': 'Dunn', 'num_friends': 2} \n",
            "{'user_id': 2, 'name': 'Sue', 'num_friends': 3} \n",
            "...\n",
            "The list-like API makes it easy to write tests:\n",
            "assert len(users) == 11 \n",
            "assert users[1]['name'] == 'Dunn'\n",
            "W e’ve got a lot more functionality to add.\n",
            "U P D A T E\n",
            "Sometimes  you need to update the data that’ s already in the database. For\n",
            "instance, if Dunn acquires another friend, you might need to do this:\n",
            "UPDATE users \n",
            "SET num_friends = 3 \n",
            "WHERE user_id = 1;\n",
            "The key features are:\n",
            "What table to update\n",
            "Which rows to update\n",
            "Which fields to update\n",
            "What their new values should be\n",
            "W e’ll add a similar update  method to NotQuiteABase. Its first ar gument\n",
            "will be a dict  whose keys are the columns to update and whose values arethe new values for those fields. Its second (optional) ar gument should be a\n",
            "predicate  that returns True  for rows that should be updated, and False\n",
            "otherwise:\n",
            "    def update(self, \n",
            "               updates: Dict[str, Any], \n",
            "               predicate: WhereClause = lambda row: True): \n",
            "        # First make sure the updates have valid names and types \n",
            "        for column, new_value in updates.items(): \n",
            "            if column not in self.columns: \n",
            "                raise ValueError(f\"invalid column: {column}\") \n",
            " \n",
            "            typ3 = self.col2type(column) \n",
            "            if not isinstance(new_value, typ3) and new_value is not None: \n",
            "                raise TypeError(f\"expected type {typ3}, but got {new_value}\") \n",
            " \n",
            "        # Now update \n",
            "        for row in self.rows: \n",
            "            if predicate(row): \n",
            "                for column, new_value in updates.items(): \n",
            "                    row[column] = new_value\n",
            "after which we can simply do this:\n",
            "assert users[1]['num_friends'] == 2             # Original value \n",
            " \n",
            "users.update({'num_friends' : 3},               # Set num_friends = 3 \n",
            "             lambda row: row['user_id'] == 1)   # in rows where user_id == 1 \n",
            " \n",
            "assert users[1]['num_friends'] == 3             # Updated value\n",
            "D E L E T E\n",
            "There  are two ways to delete rows from a table in SQL. The dangerous way\n",
            "deletes every row from a table:\n",
            "DELETE FROM users;\n",
            "The less dangerous way adds a WHERE  clause and deletes only rows that\n",
            "match a certain condition:DELETE FROM users WHERE user_id = 1;\n",
            "It’ s easy to add this functionality to our Table :\n",
            "    def delete(self, predicate: WhereClause = lambda row: True) -> None: \n",
            "        \"\"\"Delete all rows matching predicate\"\"\" \n",
            "        self.rows = [row for row in self.rows if not predicate(row)]\n",
            "If you supply a predicate  function (i.e., a WHERE  clause), this deletes only\n",
            "the rows that satisfy it. If you don’ t supply one, the default predicate\n",
            "always returns True , and you will delete every row .\n",
            "For example:\n",
            "# We're not actually going to run these \n",
            "users.delete(lambda row: row[\"user_id\"] == 1)  # Deletes rows with user_id == \n",
            "1 \n",
            "users.delete()                                 # Deletes every row\n",
            "S E L E C T\n",
            "T ypically  you don’ t inspect SQL tables directly . Instead you query them\n",
            "with a SELECT  statement:\n",
            "SELECT * FROM users;                            -- get the entire contents \n",
            "SELECT * FROM users LIMIT 2;                    -- get the first two rows \n",
            "SELECT user_id FROM users;                      -- only get specific columns \n",
            "SELECT user_id FROM users WHERE name = 'Dunn';  -- only get specific rows\n",
            "Y ou can also use SELECT  statements to calculate fields:\n",
            "SELECT LENGTH(name) AS name_length FROM users;\n",
            "W e’ll give our Table  class a select  method that returns a new Table . The\n",
            "method accepts two optional ar guments:\n",
            "keep_columns  specifies the names of the columns you want to\n",
            "keep in the result. If you don’ t supply it, the result contains all thecolumns.\n",
            "additional_columns  is a dictionary whose keys are new column\n",
            "names and whose values are functions specifying how to compute\n",
            "the values of the new columns. W e’ll peek at the type annotations\n",
            "of those functions to figure out the types of the new columns, so\n",
            "the functions will need to have annotated return types.\n",
            "If you were to supply neither of them, you’d simply get back a copy of the\n",
            "table:\n",
            "    def select(self, \n",
            "               keep_columns: List[str] = None, \n",
            "               additional_columns: Dict[str, Callable] = None) -> 'Table': \n",
            " \n",
            "        if keep_columns is None:         # If no columns specified, \n",
            "            keep_columns = self.columns  # return all columns \n",
            " \n",
            "        if additional_columns is None: \n",
            "            additional_columns = {} \n",
            " \n",
            "        # New column names and types \n",
            "        new_columns = keep_columns + list(additional_columns.keys()) \n",
            "        keep_types = [self.col2type(col) for col in keep_columns] \n",
            " \n",
            "        # This is how to get the return type from a type annotation. \n",
            "        # It will crash if `calculation` doesn't have a return type. \n",
            "        add_types = [calculation.__annotations__['return'] \n",
            "                     for calculation in additional_columns.values()] \n",
            " \n",
            "        # Create a new table for results \n",
            "        new_table = Table(new_columns, keep_types + add_types) \n",
            " \n",
            "        for row in self.rows: \n",
            "            new_row = [row[column] for column in keep_columns] \n",
            "            for column_name, calculation in additional_columns.items(): \n",
            "                new_row.append(calculation(row)) \n",
            "            new_table.insert(new_row) \n",
            " \n",
            "        return new_tableN O T E\n",
            "Remember way back in Chapter 2  when we said that type annotations don’ t actually do\n",
            "anything? W ell, here’ s the counterexample. But look at the convoluted procedure we\n",
            "have to go through to get at them.\n",
            "Our select  returns a new Table , while the typical SQL  SELECT  just\n",
            "produces some sort of transient result set (unless you explicitly insert the\n",
            "results into a table).\n",
            "W e’ll also need where  and limit  methods. Both are pretty simple:\n",
            "    def where(self, predicate: WhereClause = lambda row: True) -> 'Table': \n",
            "        \"\"\"Return only the rows that satisfy the supplied predicate\"\"\" \n",
            "        where_table = Table(self.columns, self.types) \n",
            "        for row in self.rows: \n",
            "            if predicate(row): \n",
            "                values = [row[column] for column in self.columns] \n",
            "                where_table.insert(values) \n",
            "        return where_table \n",
            " \n",
            "    def limit(self, num_rows: int) -> 'Table': \n",
            "        \"\"\"Return only the first `num_rows` rows\"\"\" \n",
            "        limit_table = Table(self.columns, self.types) \n",
            "        for i, row in enumerate(self.rows): \n",
            "            if i >= num_rows: \n",
            "                break \n",
            "            values = [row[column] for column in self.columns] \n",
            "            limit_table.insert(values) \n",
            "        return limit_table\n",
            "after which we can easily construct NotQuiteABase equivalents to the\n",
            "preceding SQL statements:\n",
            "# SELECT * FROM users; \n",
            "all_users = users.select() \n",
            "assert len(all_users) == 11 \n",
            " \n",
            "# SELECT * FROM users LIMIT 2; \n",
            "two_users = users.limit(2) \n",
            "assert len(two_users) == 2# SELECT user_id FROM users; \n",
            "just_ids = users.select(keep_columns=[\"user_id\"]) \n",
            "assert just_ids.columns == ['user_id'] \n",
            " \n",
            "# SELECT user_id FROM users WHERE name = 'Dunn'; \n",
            "dunn_ids = ( \n",
            "    users \n",
            "    .where(lambda row: row[\"name\"] == \"Dunn\") \n",
            "    .select(keep_columns=[\"user_id\"]) \n",
            ") \n",
            "assert len(dunn_ids) == 1 \n",
            "assert dunn_ids[0] == {\"user_id\": 1} \n",
            " \n",
            "# SELECT LENGTH(name) AS name_length FROM users; \n",
            "def name_length(row) -> int: return len(row[\"name\"]) \n",
            " \n",
            "name_lengths = users.select(keep_columns=[], \n",
            "                            additional_columns = {\"name_length\": name_length}) \n",
            "assert name_lengths[0]['name_length'] == len(\"Hero\")\n",
            "Notice that for the multiline “fluent” queries we have to wrap the whole\n",
            "query in parentheses.\n",
            "G R O U P  B Y\n",
            "Another  common SQL operation is GROUP BY , which groups together rows\n",
            "with identical values in specified columns and produces aggregate values\n",
            "like MIN  and MAX  and COUNT  and SUM .\n",
            "For example, you might want to find the number of users and the smallest\n",
            "user_id  for each possible name length:\n",
            "SELECT LENGTH(name) as name_length, \n",
            " MIN(user_id) AS min_user_id, \n",
            " COUNT(*) AS num_users \n",
            "FROM users \n",
            "GROUP BY LENGTH(name);\n",
            "Every field we SELECT  needs to be either in the GROUP BY  clause (which\n",
            "name_length  is) or an aggregate computation (which min_user_id  and\n",
            "num_users  are).SQL also supports a HAVING  clause that behaves similarly to a WHERE\n",
            "clause, except that its filter is applied to the aggregates (whereas a WHERE\n",
            "would filter out rows before aggregation even took place).\n",
            "Y ou might want to know the average number of friends for users whose\n",
            "names start with specific letters but see only the results for letters whose\n",
            "corresponding average is greater than 1. (Y es, some of these examples are\n",
            "contrived.)\n",
            "SELECT SUBSTR(name, 1, 1) AS first_letter, \n",
            " AVG(num_friends) AS avg_num_friends \n",
            "FROM users \n",
            "GROUP BY SUBSTR(name, 1, 1) \n",
            "HAVING AVG(num_friends) > 1;\n",
            "N O T E\n",
            "Functions for working with strings vary across SQL implementations; some databases\n",
            "might instead use SUBSTRING  or something else.\n",
            "Y ou can also compute overall aggregates. In that case, you leave of f the\n",
            "GROUP BY :\n",
            "SELECT SUM(user_id) as user_id_sum \n",
            "FROM users \n",
            "WHERE user_id > 1;\n",
            "T o add this functionality to NotQuiteABase Table s, we’ll add a group_by\n",
            "method. It takes the names of the columns you want to group by , a\n",
            "dictionary of the aggregation functions you want to run over each group,\n",
            "and an optional predicate called having  that operates on multiple rows.\n",
            "Then it does the following steps:\n",
            "1 . Creates a defaultdict  to map tuple s (of the group-by values) to\n",
            "rows (containing the group-by values). Recall that you can’ t use\n",
            "lists as dict  keys; you have to use tuples.2 . Iterates over the rows of the table, populating the defaultdict .\n",
            "3 . Creates a new table with the correct output columns.\n",
            "4 . Iterates over the defaultdict  and populates the output table,\n",
            "applying the having  filter , if any .\n",
            "    def group_by(self, \n",
            "                 group_by_columns: List[str], \n",
            "                 aggregates: Dict[str, Callable], \n",
            "                 having: HavingClause = lambda group: True) -> 'Table': \n",
            " \n",
            "        grouped_rows = defaultdict(list) \n",
            " \n",
            "        # Populate groups \n",
            "        for row in self.rows: \n",
            "            key = tuple(row[column] for column in group_by_columns) \n",
            "            grouped_rows[key].append(row) \n",
            " \n",
            "        # Result table consists of group_by columns and aggregates \n",
            "        new_columns = group_by_columns + list(aggregates.keys()) \n",
            "        group_by_types = [self.col2type(col) for col in group_by_columns] \n",
            "        aggregate_types = [agg.__annotations__['return'] \n",
            "                           for agg in aggregates.values()] \n",
            "        result_table = Table(new_columns, group_by_types + aggregate_types) \n",
            " \n",
            "        for key, rows in grouped_rows.items(): \n",
            "            if having(rows): \n",
            "                new_row = list(key) \n",
            "                for aggregate_name, aggregate_fn in aggregates.items(): \n",
            "                    new_row.append(aggregate_fn(rows)) \n",
            "                result_table.insert(new_row) \n",
            " \n",
            "        return result_table\n",
            "N O T E\n",
            "An actual database would almost certainly do this in a more ef ficient manner .)\n",
            "Again, let’ s see how we would do the equivalent of the preceding SQL\n",
            "statements. The name_length  metrics are:def min_user_id(rows) -> int: \n",
            "    return min(row[\"user_id\"] for row in rows) \n",
            " \n",
            "def length(rows) -> int: \n",
            "    return len(rows) \n",
            " \n",
            "stats_by_length = ( \n",
            "    users \n",
            "    .select(additional_columns={\"name_length\" : name_length}) \n",
            "    .group_by(group_by_columns=[\"name_length\"], \n",
            "              aggregates={\"min_user_id\" : min_user_id, \n",
            "                          \"num_users\" : length}) \n",
            ")\n",
            "The first_letter  metrics:\n",
            "def first_letter_of_name(row: Row) -> str: \n",
            "    return row[\"name\"][0] if row[\"name\"] else \"\" \n",
            " \n",
            "def average_num_friends(rows: List[Row]) -> float: \n",
            "    return sum(row[\"num_friends\"] for row in rows) / len(rows) \n",
            " \n",
            "def enough_friends(rows: List[Row]) -> bool: \n",
            "    return average_num_friends(rows) > 1 \n",
            " \n",
            "avg_friends_by_letter = ( \n",
            "    users \n",
            "    .select(additional_columns={'first_letter' : first_letter_of_name}) \n",
            "    .group_by(group_by_columns=['first_letter'], \n",
            "              aggregates={\"avg_num_friends\" : average_num_friends}, \n",
            "              having=enough_friends) \n",
            ")\n",
            "and the user_id_sum  is:\n",
            "def sum_user_ids(rows: List[Row]) -> int: \n",
            "    return sum(row[\"user_id\"] for row in rows) \n",
            " \n",
            "user_id_sum = ( \n",
            "    users \n",
            "    .where(lambda row: row[\"user_id\"] > 1) \n",
            "    .group_by(group_by_columns=[], \n",
            "              aggregates={ \"user_id_sum\" : sum_user_ids }) \n",
            ")O R D E R  B Y\n",
            "Frequently , you’ll  want to sort your results. For example, you might want to\n",
            "know the (alphabetically) first two names of your users:\n",
            "SELECT * FROM users \n",
            "ORDER BY name \n",
            "LIMIT 2;\n",
            "This is easy to implement by giving our Table  an order_by  method that\n",
            "takes an order  function:\n",
            "    def order_by(self, order: Callable[[Row], Any]) -> 'Table': \n",
            "        new_table = self.select()       # make a copy \n",
            "        new_table.rows.sort(key=order) \n",
            "        return new_table\n",
            "which we can then use as follows:\n",
            "friendliest_letters = ( \n",
            "    avg_friends_by_letter \n",
            "    .order_by(lambda row: -row[\"avg_num_friends\"]) \n",
            "    .limit(4) \n",
            ")\n",
            "The SQL ORDER BY  lets you specify ASC  (ascending) or DESC  (descending)\n",
            "for each sort field; here we’d have to bake that into our order  function.\n",
            "J O I N\n",
            "Relational database  tables are often normalized , which means that they’re\n",
            "or ganized to minimize redundancy . For example, when we work with our\n",
            "users’ interests in Python, we can just give each user a list  containing his\n",
            "interests.\n",
            "SQL tables can’ t typically contain lists, so the typical solution is to create a\n",
            "second table called user_interests  containing the one-to-manyrelationship between user_id s and interest s. In SQL you might do:\n",
            "CREATE TABLE user_interests ( \n",
            "    user_id INT NOT NULL, \n",
            "    interest VARCHAR(100) NOT NULL \n",
            ");\n",
            "whereas in NotQuiteABase you’d create the table:\n",
            "user_interests = Table(['user_id', 'interest'], [int, str]) \n",
            "user_interests.insert([0, \"SQL\"]) \n",
            "user_interests.insert([0, \"NoSQL\"]) \n",
            "user_interests.insert([2, \"SQL\"]) \n",
            "user_interests.insert([2, \"MySQL\"])\n",
            "N O T E\n",
            "There’ s still plenty of redundancy—the interest “SQL” is stored in two dif ferent places.\n",
            "In a real database you might store user_id  and interest_id  in the user_interests\n",
            "table and then create a third table, interests , mapping interest_id  to interest  so\n",
            "you could store the interest names only once each. Here that would just make our\n",
            "examples more complicated than they need to be.\n",
            "When our data lives across dif ferent tables, how do we analyze it? By\n",
            "JOIN ing the tables together . A JOIN  combines rows in the left table with\n",
            "corresponding rows in the right table, where the meaning of\n",
            "“corresponding” is based on how we specify the join.\n",
            "For example, to find the users interested in SQL you’d query:\n",
            "SELECT users.name \n",
            "FROM users \n",
            "JOIN user_interests \n",
            "ON users.user_id = user_interests.user_id \n",
            "WHERE user_interests.interest = 'SQL'\n",
            "The JOIN  says that, for each row in users , we should look at the user_id\n",
            "and associate that row with every row in user_interests  containing thesame user_id .\n",
            "Notice we had to specify which tables to JOIN  and also which columns to\n",
            "join ON . This is an INNER JOIN , which returns the combinations of rows\n",
            "(and only the combinations of rows) that match according to the specified\n",
            "join criteria.\n",
            "There is also a LEFT JOIN , which—in addition to the combinations of\n",
            "matching rows—returns a row for each left-table row with no matching\n",
            "rows (in which case, the fields that would have come from the right table\n",
            "are all NULL ).\n",
            "Using a LEFT JOIN , it’ s easy to count the number of interests each user has:\n",
            "SELECT users.id, COUNT(user_interests.interest) AS num_interests \n",
            "FROM users \n",
            "LEFT JOIN user_interests \n",
            "ON users.user_id = user_interests.user_id\n",
            "The LEFT JOIN  ensures that users with no interests will still have rows in\n",
            "the joined dataset (with NULL  values for the fields coming from\n",
            "user_interests ), and COUNT  counts only values that are non- NULL .\n",
            "The NotQuiteABase join  implementation will be more restrictive—it\n",
            "simply joins two tables on whatever columns they have in common. Even\n",
            "so, it’ s not trivial to write:\n",
            "    def join(self, other_table: 'Table', left_join: bool = False) -> 'Table': \n",
            " \n",
            "        join_on_columns = [c for c in self.columns           # columns in \n",
            "                           if c in other_table.columns]      # both tables \n",
            " \n",
            "        additional_columns = [c for c in other_table.columns # columns only \n",
            "                              if c not in join_on_columns]   # in right table \n",
            " \n",
            "        # all columns from left table + additional_columns from right table \n",
            "        new_columns = self.columns + additional_columns \n",
            "        new_types = self.types + [other_table.col2type(col) \n",
            "                                  for col in additional_columns] \n",
            " \n",
            "        join_table = Table(new_columns, new_types)for row in self.rows: \n",
            "            def is_join(other_row): \n",
            "                return all(other_row[c] == row[c] for c in join_on_columns) \n",
            " \n",
            "            other_rows = other_table.where(is_join).rows \n",
            " \n",
            "            # Each other row that matches this one produces a result row. \n",
            "            for other_row in other_rows: \n",
            "                join_table.insert([row[c] for c in self.columns] + \n",
            "                                  [other_row[c] for c in additional_columns]) \n",
            " \n",
            "            # If no rows match and it's a left join, output with Nones. \n",
            "            if left_join and not other_rows: \n",
            "                join_table.insert([row[c] for c in self.columns] + \n",
            "                                  [None for c in additional_columns]) \n",
            " \n",
            "        return join_table\n",
            "So, we could find users interested in SQL with:\n",
            "sql_users = ( \n",
            "    users \n",
            "    .join(user_interests) \n",
            "    .where(lambda row: row[\"interest\"] == \"SQL\") \n",
            "    .select(keep_columns=[\"name\"]) \n",
            ")\n",
            "And we could get the interest counts with:\n",
            "def count_interests(rows: List[Row]) -> int: \n",
            "    \"\"\"counts how many rows have non-None interests\"\"\" \n",
            "    return len([row for row in rows if row[\"interest\"] is not None]) \n",
            " \n",
            "user_interest_counts = ( \n",
            "    users \n",
            "    .join(user_interests, left_join=True) \n",
            "    .group_by(group_by_columns=[\"user_id\"], \n",
            "              aggregates={\"num_interests\" : count_interests }) \n",
            ")\n",
            "In SQL, there is also a RIGHT JOIN , which keeps rows from the right table\n",
            "that have no matches, and a FULL OUTER JOIN , which keeps rows fromboth tables that have no matches. W e won’ t implement either of those.\n",
            "S u b q u e r i e s\n",
            "In  SQL, you can SELECT  from (and JOIN ) the results of queries as if they\n",
            "were tables. So, if you wanted to find the smallest user_id  of anyone\n",
            "interested in SQL, you could use a subquery . (Of course, you could do the\n",
            "same calculation using a JOIN , but that wouldn’ t illustrate subqueries.)\n",
            "SELECT MIN(user_id) AS min_user_id FROM \n",
            "(SELECT user_id FROM user_interests WHERE interest = 'SQL') sql_interests;\n",
            "Given the way we’ve designed NotQuiteABase, we get this for free. (Our\n",
            "query results are actual tables.)\n",
            "likes_sql_user_ids = ( \n",
            "    user_interests \n",
            "    .where(lambda row: row[\"interest\"] == \"SQL\") \n",
            "    .select(keep_columns=['user_id']) \n",
            ") \n",
            " \n",
            "likes_sql_user_ids.group_by(group_by_columns=[], \n",
            "                            aggregates={ \"min_user_id\" : min_user_id })\n",
            "I n d e x e s\n",
            "T o  find rows containing a specific value (say , where name  is “Hero”),\n",
            "NotQuiteABase has to inspect every row in the table. If the table has a lot\n",
            "of rows, this can take a very long time.\n",
            "Similarly , our join  algorithm is extremely inef ficient. For each row in the\n",
            "left table, it inspects every row in the right table to see if it’ s a match. W ith\n",
            "two lar ge tables this could take approximately forever .\n",
            "Also, you’d often like to apply constraints to some of your columns. For\n",
            "example, in your users  table you probably don’ t want to allow two\n",
            "dif ferent users to have the same user_id .Indexes solve all these problems. If the user_interests  table had an index\n",
            "on user_id , a smart join  algorithm could find matches directly rather than\n",
            "scanning the whole table. If the users  table had a “unique” index on\n",
            "user_id , you’d get an error if you tried to insert a duplicate.\n",
            "Each table in a database can have one or more indexes, which allow you to\n",
            "quickly look up rows by key columns, ef ficiently join tables together , and\n",
            "enforce unique constraints on columns or combinations of columns.\n",
            "Designing and using indexes well is something of a black art (which varies\n",
            "somewhat depending on the specific database), but if you end up doing a lot\n",
            "of database work it’ s worth learning about.\n",
            "Q u e r y  O p t i m i z a t i o n\n",
            "Recall  the query to find all users who are interested in SQL:\n",
            "SELECT users.name \n",
            "FROM users \n",
            "JOIN user_interests \n",
            "ON users.user_id = user_interests.user_id \n",
            "WHERE user_interests.interest = 'SQL'\n",
            "In NotQuiteABase there are (at least) two dif ferent ways to write this query .\n",
            "Y ou could filter the user_interests  table before performing the join:\n",
            "( \n",
            "    user_interests \n",
            "    .where(lambda row: row[\"interest\"] == \"SQL\") \n",
            "    .join(users) \n",
            "    .select([\"name\"]) \n",
            ")\n",
            "Or you could filter the results of the join:\n",
            "( \n",
            "    user_interests \n",
            "    .join(users) \n",
            "    .where(lambda row: row[\"interest\"] == \"SQL\").select([\"name\"]) \n",
            ")\n",
            "Y ou’ll end up with the same results either way , but filter -before-join is\n",
            "almost certainly more ef ficient, since in that case join  has many fewer\n",
            "rows to operate on.\n",
            "In SQL, you generally wouldn’ t worry about this. Y ou “declare” the results\n",
            "you want and leave it up to the query engine to execute them (and use\n",
            "indexes ef ficiently).\n",
            "N o S Q L\n",
            "A  recent trend in databases is toward nonrelational “NoSQL” databases,\n",
            "which don’ t represent data in tables. For instance, MongoDB is a popular\n",
            "schemaless database whose elements are arbitrarily complex JSON\n",
            "documents rather than rows.\n",
            "There are column databases that store data in columns instead of rows\n",
            "(good when data has many columns but queries need few of them),\n",
            "key/value stores that are optimized for retrieving single (complex) values\n",
            "by their keys, databases for storing and traversing graphs, databases that are\n",
            "optimized to run across multiple datacenters, databases that are designed to\n",
            "run in memory , databases for storing time-series data, and hundreds more.\n",
            "T omorrow’ s flavor of the day might not even exist now , so I can’ t do much\n",
            "more than let you know that NoSQL  is a thing. So now you know . It’ s a\n",
            "thing.\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "If you’d like  to download a relational database to play with,\n",
            "SQLite  is fast and tiny , while MySQL  and PostgreSQL  are lar ger\n",
            "and featureful. All are free and have lots of documentation.If you  want to explore NoSQL, MongoDB  is very simple to get\n",
            "started with, which can be both a blessing and somewhat of a\n",
            "curse. It also has pretty good documentation.\n",
            "The W ikipedia article on NoSQL  almost certainly now contains\n",
            "links to databases that didn’ t even exist when this book was\n",
            "written.Chapter 25. MapReduce\n",
            "The futur e has alr eady arrived. It’ s just not evenly distributed yet.\n",
            "— W illiam Gibson\n",
            "MapReduce  is a programming model for performing parallel processing on\n",
            "lar ge datasets. Although it is a powerful technique, its basics are relatively\n",
            "simple.\n",
            "Imagine we have a collection of items we’d like to process somehow . For\n",
            "instance, the items might be website logs, the texts of various books, image\n",
            "files, or anything else. A  basic version of the MapReduce algorithm consists\n",
            "of the following steps:\n",
            "1 . Use a mapper  function to turn each item into zero or more\n",
            "key/value pairs. (Often this is called the map  function, but there is\n",
            "already a Python function called map  and we don’ t need to confuse\n",
            "the two.)\n",
            "2 . Collect together all the pairs with identical keys.\n",
            "3 . Use a reducer  function on each collection of grouped values to\n",
            "produce output values for the corresponding key .\n",
            "N O T E\n",
            "MapReduce is sort of passé, so much so that I considered removing this chapter from\n",
            "the second edition. But I decided it’ s still an interesting topic, so I ended up leaving it in\n",
            "(obviously).\n",
            "This is all sort of abstract, so let’ s look at a specific example. There are few\n",
            "absolute rules of data science, but one of them is that your first MapReduce\n",
            "example has to involve counting words.E x a m p l e :  W o r d  C o u n t\n",
            "DataSciencester  has grown to millions of users! This is great for your job\n",
            "security , but it makes routine analyses slightly more dif ficult.\n",
            "For example, your VP of Content wants to know what sorts of things people\n",
            "are talking about in their status updates. As a first attempt, you decide to\n",
            "count the words that appear , so that you can prepare a report on the most\n",
            "frequent ones.\n",
            "When you had a few hundred users, this was simple to do:\n",
            "from typing import List \n",
            "from collections import Counter \n",
            " \n",
            "def tokenize(document: str) -> List[str]: \n",
            "    \"\"\"Just split on whitespace\"\"\" \n",
            "    return document.split() \n",
            " \n",
            "def word_count_old(documents: List[str]): \n",
            "    \"\"\"Word count not using MapReduce\"\"\" \n",
            "    return Counter(word \n",
            "        for document in documents \n",
            "        for word in tokenize(document))\n",
            "W ith millions of users the set of documents  (status updates) is suddenly too\n",
            "big to fit on your computer . If you can just fit this into the MapReduce\n",
            "model, you can use some “big data” infrastructure that your engineers have\n",
            "implemented.\n",
            "First, we need a function that turns a document into a sequence of key/value\n",
            "pairs. W e’ll want our output to be grouped by word, which means that the\n",
            "keys should be words. And for each word, we’ll just emit the value 1  to\n",
            "indicate that this pair corresponds to one occurrence of the word:\n",
            "from typing import Iterator, Tuple \n",
            " \n",
            "def wc_mapper(document: str) -> Iterator[Tuple[str, int]]: \n",
            "    \"\"\"For each word in the document, emit (word, 1)\"\"\" \n",
            "    for word in tokenize(document): \n",
            "        yield (word, 1)Skipping the “plumbing” step 2 for the moment, imagine that for some\n",
            "word we’ve collected a list of the corresponding counts we emitted. T o\n",
            "produce the overall count for that word, then, we just need:\n",
            "from typing import Iterable \n",
            " \n",
            "def wc_reducer(word: str, \n",
            "               counts: Iterable[int]) -> Iterator[Tuple[str, int]]: \n",
            "    \"\"\"Sum up the counts for a word\"\"\" \n",
            "    yield (word, sum(counts))\n",
            "Returning to step 2, we now need to collect the results from wc_mapper  and\n",
            "feed them to wc_reducer . Let’ s think about how we would do this on just\n",
            "one computer:\n",
            "from collections import defaultdict \n",
            " \n",
            "def word_count(documents: List[str]) -> List[Tuple[str, int]]: \n",
            "    \"\"\"Count the words in the input documents using MapReduce\"\"\" \n",
            " \n",
            "    collector = defaultdict(list)  # To store grouped values \n",
            " \n",
            "    for document in documents: \n",
            "        for word, count in wc_mapper(document): \n",
            "            collector[word].append(count) \n",
            " \n",
            "    return [output \n",
            "            for word, counts in collector.items() \n",
            "            for output in wc_reducer(word, counts)]\n",
            "Imagine that we have three documents [\"data science\", \"big data\",\n",
            "\"science fiction\"] .\n",
            "Then wc_mapper  applied to the first document yields the two pairs\n",
            "(\"data\", 1)  and (\"science\", 1) . After we’ve gone through all three\n",
            "documents, the collector  contains:\n",
            "{\"data\" : [1, 1], \n",
            " \"science\" : [1, 1], \n",
            " \"big\" : [1], \n",
            " \"fiction\" : [1]}Then wc_reducer  produces the counts for each word:\n",
            "[(\"data\", 2), (\"science\", 2), (\"big\", 1), (\"fiction\", 1)]\n",
            "W h y  M a p R e d u c e ?\n",
            "As  mentioned earlier , the primary benefit of MapReduce is that it allows us\n",
            "to distribute computations by moving the processing to the data. Imagine\n",
            "we want to word-count across billions of documents.\n",
            "Our original (non-MapReduce) approach requires the machine doing the\n",
            "processing to have access to every document. This means that the\n",
            "documents all need to either live on that machine or else be transferred to it\n",
            "during processing. More important, it means that the machine can process\n",
            "only one document at a time.\n",
            "N O T E\n",
            "Possibly it can process up to a few at a time if it has multiple cores and if the code is\n",
            "rewritten to take advantage of them. But even so, all the documents still have to get to\n",
            "that machine.\n",
            "Imagine now that our billions of documents are scattered across 100\n",
            "machines. W ith the right infrastructure (and glossing over some of the\n",
            "details), we can do the following:\n",
            "Have each machine run the mapper on its documents, producing\n",
            "lots of key/value pairs.\n",
            "Distribute those key/value pairs to a number of “reducing”\n",
            "machines, making sure that the pairs corresponding to any given\n",
            "key all end up on the same machine.\n",
            "Have each reducing machine group the pairs by key and then run\n",
            "the reducer on each set of values.Return each (key , output) pair .\n",
            "What is amazing about this is that it scales horizontally . If we double the\n",
            "number of machines, then (ignoring certain fixed costs of running a\n",
            "MapReduce system) our computation should run approximately twice as\n",
            "fast. Each mapper machine will only need to do half as much work, and\n",
            "(assuming there are enough distinct keys to further distribute the reducer\n",
            "work) the same is true for the reducer machines.\n",
            "M a p R e d u c e  M o r e  G e n e r a l l y\n",
            "If  you think about it for a minute, all of the word count–specific code in the\n",
            "previous example is contained in the wc_mapper  and wc_reducer\n",
            "functions. This means that with a couple of changes we have a much more\n",
            "general framework (that still runs on a single machine).\n",
            "W e could use generic types to fully type-annotate our map_reduce  function,\n",
            "but it would end up being kind of a mess pedagogically , so in this chapter\n",
            "we’ll be much more casual about our type annotations:\n",
            "from typing import Callable, Iterable, Any, Tuple \n",
            " \n",
            "# A key/value pair is just a 2-tuple \n",
            "KV = Tuple[Any, Any] \n",
            " \n",
            "# A Mapper is a function that returns an Iterable of key/value pairs \n",
            "Mapper = Callable[..., Iterable[KV]] \n",
            " \n",
            "# A Reducer is a function that takes a key and an iterable of values \n",
            "# and returns a key/value pair \n",
            "Reducer = Callable[[Any, Iterable], KV]\n",
            "Now we can write a general map_reduce  function:\n",
            "def map_reduce(inputs: Iterable, \n",
            "               mapper: Mapper, \n",
            "               reducer: Reducer) -> List[KV]: \n",
            "    \"\"\"Run MapReduce on the inputs using mapper and reducer\"\"\" \n",
            "    collector = defaultdict(list)for input in inputs: \n",
            "        for key, value in mapper(input): \n",
            "            collector[key].append(value) \n",
            " \n",
            "    return [output \n",
            "            for key, values in collector.items() \n",
            "            for output in reducer(key, values)]\n",
            "Then we can count words simply by using:\n",
            "word_counts = map_reduce(documents, wc_mapper, wc_reducer)\n",
            "This gives us the flexibility to solve a wide variety of problems.\n",
            "Before we proceed, notice that wc_reducer  is just summing the values\n",
            "corresponding to each key . This kind of aggregation is common enough that\n",
            "it’ s worth abstracting it out:\n",
            "def values_reducer(values_fn: Callable) -> Reducer: \n",
            "    \"\"\"Return a reducer that just applies values_fn to its values\"\"\" \n",
            "    def reduce(key, values: Iterable) -> KV: \n",
            "        return (key, values_fn(values)) \n",
            " \n",
            "    return reduce\n",
            "After which we can easily create:\n",
            "sum_reducer = values_reducer(sum) \n",
            "max_reducer = values_reducer(max) \n",
            "min_reducer = values_reducer(min) \n",
            "count_distinct_reducer = values_reducer(lambda values: len(set(values))) \n",
            " \n",
            "assert sum_reducer(\"key\", [1, 2, 3, 3]) == (\"key\", 9) \n",
            "assert min_reducer(\"key\", [1, 2, 3, 3]) == (\"key\", 1) \n",
            "assert max_reducer(\"key\", [1, 2, 3, 3]) == (\"key\", 3) \n",
            "assert count_distinct_reducer(\"key\", [1, 2, 3, 3]) == (\"key\", 3)\n",
            "and so on.E x a m p l e :  A n a l y z i n g  S t a t u s  U p d a t e s\n",
            "The  content VP was impressed with the word counts and asks what else you\n",
            "can learn from people’ s status updates. Y ou manage to extract a dataset of\n",
            "status updates that look like:\n",
            "status_updates = [ \n",
            "    {\"id\": 2, \n",
            "     \"username\" : \"joelgrus\", \n",
            "     \"text\" : \"Should I write a second edition of my data science book?\", \n",
            "     \"created_at\" : datetime.datetime(2018, 2, 21, 11, 47, 0), \n",
            "     \"liked_by\" : [\"data_guy\", \"data_gal\", \"mike\"] }, \n",
            "     # ... \n",
            "]\n",
            "Let’ s say we need to figure out which day of the week people talk the most\n",
            "about data science. In order to find this, we’ll just count how many data\n",
            "science updates there are on each day of the week. This means we’ll need to\n",
            "group by the day of week, so that’ s our key . And if we emit a value of 1  for\n",
            "each update that contains “data science,” we can simply get the total\n",
            "number using sum :\n",
            "def data_science_day_mapper(status_update: dict) -> Iterable: \n",
            "    \"\"\"Yields (day_of_week, 1) if status_update contains \"data science\" \"\"\" \n",
            "    if \"data science\" in status_update[\"text\"].lower(): \n",
            "        day_of_week = status_update[\"created_at\"].weekday() \n",
            "        yield (day_of_week, 1) \n",
            " \n",
            "data_science_days = map_reduce(status_updates, \n",
            "                               data_science_day_mapper, \n",
            "                               sum_reducer)\n",
            "As a slightly more complicated example, imagine we need to find out for\n",
            "each user the most common word that she puts in her status updates. There\n",
            "are three possible approaches that spring to mind for the mapper :\n",
            "Put the username in the key; put the words and counts in the\n",
            "values.Put the word in the key; put the usernames and counts in the\n",
            "values.\n",
            "Put the username and word in the key; put the counts in the values.\n",
            "If you think about it a bit more, we definitely want to group by username ,\n",
            "because we want to consider each person’ s words separately . And we don’ t\n",
            "want to group by word , since our reducer will need to see all the words for\n",
            "each person to find out which is the most popular . This means that the first\n",
            "option is the right choice:\n",
            "def words_per_user_mapper(status_update: dict): \n",
            "    user = status_update[\"username\"] \n",
            "    for word in tokenize(status_update[\"text\"]): \n",
            "        yield (user, (word, 1)) \n",
            " \n",
            "def most_popular_word_reducer(user: str, \n",
            "                              words_and_counts: Iterable[KV]): \n",
            "    \"\"\" \n",
            "    Given a sequence of (word, count) pairs, \n",
            "    return the word with the highest total count \n",
            "    \"\"\" \n",
            "    word_counts = Counter() \n",
            "    for word, count in words_and_counts: \n",
            "        word_counts[word] += count \n",
            " \n",
            "    word, count = word_counts.most_common(1)[0] \n",
            " \n",
            "    yield (user, (word, count)) \n",
            " \n",
            "user_words = map_reduce(status_updates, \n",
            "                        words_per_user_mapper, \n",
            "                        most_popular_word_reducer)\n",
            "Or we could find out the number of distinct status-likers for each user:\n",
            "def liker_mapper(status_update: dict): \n",
            "    user = status_update[\"username\"] \n",
            "    for liker in status_update[\"liked_by\"]: \n",
            "        yield (user, liker) \n",
            " \n",
            "distinct_likers_per_user = map_reduce(status_updates,liker_mapper, \n",
            "                                      count_distinct_reducer)\n",
            "E x a m p l e :  M a t r i x  M u l t i p l i c a t i o n\n",
            "Recall  from “Matrix Multiplication”  that given an [n, m]  matrix A  and an\n",
            "[m, k]  matrix B , we can multiply them to form an [n, k]  matrix C , where\n",
            "the element of C  in row i  and column j  is given by:\n",
            "C[i][j] = sum(A[i][x] * B[x][j] for x in range(m))\n",
            "This works if we represent our matrices as lists of lists, as we’ve been\n",
            "doing.\n",
            "But lar ge matrices are sometimes sparse , which means that most of their\n",
            "elements equal 0. For lar ge sparse matrices, a list of lists can be a very\n",
            "wasteful representation. A more compact representation stores only the\n",
            "locations with nonzero values:\n",
            "from typing import NamedTuple \n",
            " \n",
            "class Entry(NamedTuple): \n",
            "    name: str \n",
            "    i: int \n",
            "    j: int \n",
            "    value: float\n",
            "For example, a 1 billion × 1 billion matrix has 1 quintillion  entries, which\n",
            "would not be easy to store on a computer . But if there are only a few\n",
            "nonzero entries in each row , this alternative representation is many orders of\n",
            "magnitude smaller .\n",
            "Given this sort of representation, it turns out that we can use MapReduce to\n",
            "perform matrix multiplication in a distributed manner .\n",
            "T o motivate our algorithm, notice that each element A[i][j]  is only used to\n",
            "compute the elements of C  in row i , and each element B[i][j]  is only used\n",
            "to compute the elements of C  in column j . Our goal will be for each outputof our reducer  to be a single entry of C , which means we’ll need our\n",
            "mapper to emit keys identifying a single entry of C . This suggests the\n",
            "following:\n",
            "def matrix_multiply_mapper(num_rows_a: int, num_cols_b: int) -> Mapper: \n",
            "    # C[x][y] = A[x][0] * B[0][y] + ... + A[x][m] * B[m][y] \n",
            "    # \n",
            "    # so an element A[i][j] goes into every C[i][y] with coef B[j][y] \n",
            "    # and an element B[i][j] goes into every C[x][j] with coef A[x][i] \n",
            "    def mapper(entry: Entry): \n",
            "        if entry.name == \"A\": \n",
            "            for y in range(num_cols_b): \n",
            "                key = (entry.i, y)              # which element of C \n",
            "                value = (entry.j, entry.value)  # which entry in the sum \n",
            "                yield (key, value) \n",
            "        else: \n",
            "            for x in range(num_rows_a): \n",
            "                key = (x, entry.j)              # which element of C \n",
            "                value = (entry.i, entry.value)  # which entry in the sum \n",
            "                yield (key, value) \n",
            " \n",
            "    return mapper\n",
            "And then:\n",
            "def matrix_multiply_reducer(key: Tuple[int, int], \n",
            "                            indexed_values: Iterable[Tuple[int, int]]): \n",
            "    results_by_index = defaultdict(list) \n",
            " \n",
            "    for index, value in indexed_values: \n",
            "        results_by_index[index].append(value) \n",
            " \n",
            "    # Multiply the values for positions with two values \n",
            "    # (one from A, and one from B) and sum them up. \n",
            "    sumproduct = sum(values[0] * values[1] \n",
            "                     for values in results_by_index.values() \n",
            "                     if len(values) == 2) \n",
            " \n",
            "    if sumproduct != 0.0: \n",
            "        yield (key, sumproduct)\n",
            "For example, if you had these two matrices:A = [[3, 2, 0], \n",
            "     [0, 0, 0]] \n",
            " \n",
            "B = [[4, -1, 0], \n",
            "     [10, 0, 0], \n",
            "     [0, 0, 0]]\n",
            "you could rewrite them as tuples:\n",
            "entries = [Entry(\"A\", 0, 0, 3), Entry(\"A\", 0, 1,  2), Entry(\"B\", 0, 0, 4), \n",
            "           Entry(\"B\", 0, 1, -1), Entry(\"B\", 1, 0, 10)] \n",
            " \n",
            " \n",
            "mapper = matrix_multiply_mapper(num_rows_a=2, num_cols_b=3) \n",
            "reducer = matrix_multiply_reducer \n",
            " \n",
            "# Product should be [[32, -3, 0], [0, 0, 0]]. \n",
            "# So it should have two entries. \n",
            "assert (set(map_reduce(entries, mapper, reducer)) == \n",
            "        {((0, 1), -3), ((0, 0), 32)})\n",
            "This isn’ t terribly interesting on such small matrices, but if you had millions\n",
            "of rows and millions of columns, it could help you a lot.\n",
            "A n  A s i d e :  C o m b i n e r s\n",
            "One thing you have probably noticed is that many of our mappers seem to\n",
            "include a bunch of extra information. For example, when counting words,\n",
            "rather than emitting (word, 1)  and summing over the values, we could\n",
            "have emitted (word, None)  and just taken the length.\n",
            "One reason we didn’ t do this is that, in the distributed setting, we\n",
            "sometimes want to use combiners  to reduce the amount of data that has to\n",
            "be transferred around from machine to machine. If one of our mapper\n",
            "machines sees the word data  500 times, we can tell it to combine the 500\n",
            "instances of (\"data\", 1)  into a single (\"data\", 500)  before handing of f\n",
            "to the reducing machine. This results in a lot less data getting moved\n",
            "around, which can make our algorithm substantially faster still.Because of the way we wrote our reducer , it would handle this combined\n",
            "data correctly . (If we’d written it using len , it would not have.)\n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "Like I said, MapReduce feels a lot less popular now than it did\n",
            "when I wrote the first edition. It’ s probably not worth investing a\n",
            "ton of your time.\n",
            "That said, the most widely used MapReduce system is Hadoop .\n",
            "There are various commercial and noncommercial distributions\n",
            "and a huge ecosystem of Hadoop-related tools.\n",
            "Amazon.com of fers an Elastic MapReduce  service that’ s probably\n",
            "easier than setting up your own cluster .\n",
            "Hadoop jobs are typically high-latency , which makes them a poor\n",
            "choice for “real-time” analytics. A popular choice for these\n",
            "workloads is Spark , which can be MapReduce-y .Chapter 26. Data Ethics\n",
            "Grub first, then ethics.\n",
            "— Bertolt Brecht\n",
            "W h a t  I s  D a t a  E t h i c s ?\n",
            "W ith  the use of data comes the misuse of data. This has pretty much always\n",
            "been the case, but recently this idea has been reified as “data ethics” and has\n",
            "featured somewhat prominently in the news.\n",
            "For instance, in the 2016 election, a company called Cambridge Analytica\n",
            "improperly accessed Facebook data  and used that for political ad tar geting.\n",
            "In 2018, an autonomous car being tested by Uber struck and killed a\n",
            "pedestrian  (there was a “safety driver” in the car , but apparently she was not\n",
            "paying attention at the time).\n",
            "Algorithms are used to predict the risk that criminals will reof fend  and to\n",
            "sentence them accordingly . Is this more or less fair than allowing judges to\n",
            "determine the same?\n",
            "Some airlines assign families separate seats , forcing them to pay extra to sit\n",
            "together . Should a data scientist have stepped in to prevent this? (Many data\n",
            "scientists in the linked thread seem to believe so.)\n",
            "“Data ethics” purports to provide answers to these questions, or at least a\n",
            "framework for wrestling with them. I’m not so arrogant as to tell you how\n",
            "to think about these things (and “these things” are changing quickly), so in\n",
            "this chapter we’ll just take a quick tour of some of the most relevant issues\n",
            "and (hopefully) inspire you to think about them further . (Alas, I am not a\n",
            "good enough philosopher to do ethics fr om scratch .)N o ,  R e a l l y ,  W h a t  I s  D a t a  E t h i c s ?\n",
            "W ell, let’ s start  with “what is ethics?” If  you take the average of every\n",
            "definition you can find, you end up with something like ethics  is a\n",
            "framework for thinking about “right” and “wrong” behavior . Data  ethics,\n",
            "then, is a framework for thinking about right and wrong behavior involving\n",
            "data.\n",
            "Some people talk as if “data ethics” is (perhaps implicitly) a set of\n",
            "commandments about what you may and may not do. Some of them are\n",
            "hard at work creating manifestos, others crafting mandatory pledges to\n",
            "which they hope to make you swear . Still others are campaigning for data\n",
            "ethics to be made a mandatory part of the data science curriculum—hence\n",
            "this chapter , as a means of hedging my bets in case they succeed.\n",
            "N O T E\n",
            "Curiously , there is not much data suggesting that ethics courses lead to ethical behavior ,\n",
            "in which case perhaps this campaign is itself data-unethical!\n",
            "Other people (for example, yours truly) think that reasonable people will\n",
            "frequently disagree over subtle matters of right and wrong, and that the\n",
            "important part of data ethics is committing to consider  the ethical\n",
            "consequences of your behaviors. This requires understanding  the sorts of\n",
            "things that many “data ethics” advocates don’ t approve of, but it doesn’ t\n",
            "necessarily require agreeing with their disapproval.\n",
            "S h o u l d  I  C a r e  A b o u t  D a t a  E t h i c s ?\n",
            "Y ou  should care about ethics whatever your job. If your job involves data,\n",
            "you are free to characterize your caring as “data ethics,” but you should\n",
            "care just as much about ethics in the nondata parts of your job.\n",
            "Perhaps what’ s dif ferent about technology jobs is that technology scales ,\n",
            "and that decisions made by individuals working on technology problems(whether data-related or not) have potentially wide-reaching ef fects.\n",
            "A tiny change to a news discovery algorithm could be the dif ference\n",
            "between millions of people reading an article and no one reading it.\n",
            "A single flawed algorithm for granting parole that’ s used all over the\n",
            "country systematically af fects millions of people, whereas a flawed-in-its-\n",
            "own-way parole board af fects only the people who come before it.\n",
            "So yes, in general, you should care about what ef fects your work has on the\n",
            "world. And the broader the ef fects of your work, the more you need to\n",
            "worry about these things.\n",
            "Unfortunately , some of the discourse around data ethics involves people\n",
            "trying to force their ethical conclusions on you. Whether you should care\n",
            "about the same things they  care about is really up to you.\n",
            "B u i l d i n g  B a d  D a t a  P r o d u c t s\n",
            "Some “data ethics” issues  are the result of building bad pr oducts .\n",
            "For example, Microsoft released a chat bot named T ay  that parroted back\n",
            "things tweeted to it, which the internet quickly discovered enabled them to\n",
            "get T ay to tweet all sorts of of fensive things. It seems unlikely that anyone\n",
            "at Microsoft debated the ethicality of releasing a “racist” bot; most likely\n",
            "they simply built a bot and failed to think through how it could be abused.\n",
            "This is perhaps a low bar , but let’ s agree that you should think about how\n",
            "the things you build could be abused.\n",
            "Another  example is that Google Photos at one point used an image\n",
            "recognition algorithm that would sometimes classify pictures of black\n",
            "people as “gorillas” . Again, it is extremely unlikely that anyone at Google\n",
            "explicitly decided  to ship this feature (let alone grappled with the “ethics” of\n",
            "it). Here it seems likely the problem is some combination of bad training\n",
            "data, model inaccuracy , and the gross of fensiveness of the mistake (if the\n",
            "model had occasionally categorized mailboxes as fire trucks, probably no\n",
            "one would have cared).In this case the solution is less obvious: how can you ensure that your\n",
            "trained model won’ t make predictions that are in some way of fensive? Of\n",
            "course you should train (and test) your model on a diverse range of inputs,\n",
            "but can you ever be sure that there isn’ t some  input somewhere out there\n",
            "that will make your model behave in a way that embarrasses you? This is a\n",
            "hard problem. (Google seems to have “solved” it by simply refusing to ever\n",
            "predict “gorilla.”)\n",
            " \n",
            "T r a d i n g  O f f  A c c u r a c y  a n d  F a i r n e s s\n",
            "Imagine  you are building a model that predicts how likely people are to take\n",
            "some action. Y ou do a pretty good job ( T able 26-1 ).\n",
            "T able 26-1. A pr etty good job\n",
            "Prediction People Actions %\n",
            "Unlikely 125 25 20%\n",
            "Likely 125 75 60%\n",
            "Of the people you predict are unlikely to take the action, only 20% of them\n",
            "do. Of the people you predict are likely to take the action, 60% of them do.\n",
            "Seems not terrible.\n",
            "Now imagine that the people can be split into two groups: A and B. Some\n",
            "of your colleagues are concerned that your model is unfair  to one of the\n",
            "groups. Although the model does not take group membership into account,\n",
            "it does consider various other factors that correlate in complicated ways\n",
            "with group membership.\n",
            "Indeed, when you break down the predictions by group, you discover\n",
            "surprising statistics ( T able 26-2 ).T able 26-2. Surprising statistics\n",
            "Group Prediction People Actions %\n",
            "A Unlikely 100 20 20%\n",
            "A Likely 25 15 60%\n",
            "B Unlikely 25 5 20%\n",
            "B Likely 100 60 60%\n",
            "Is your model unfair? The data scientists on your team make a variety of\n",
            "ar guments:\n",
            "Ar gument 1\n",
            "Y our model classifies 80% of group A as “unlikely” but 80% of group B\n",
            "as “likely .” This data scientist complains that the model is treating the\n",
            "two groups unfairly in the sense that it is generating vastly dif ferent\n",
            "predictions across the two groups.\n",
            "Ar gument 2\n",
            "Regardless of group membership, if we predict “unlikely” you have a\n",
            "20% chance of action, and if we predict “likely” you have a 60% chance\n",
            "of action. This data scientist insists that the model is “accurate” in the\n",
            "sense that its predictions seem to mean  the same things no matter which\n",
            "group you belong to.\n",
            "Ar gument 3\n",
            "40/125 = 32% of group B were falsely labeled “likely ,” whereas only\n",
            "10/125 = 8% of group A were falsely labeled “likely .” This data\n",
            "scientist (who considers a “likely” prediction to be a bad thing) insists\n",
            "that the model unfairly stigmatizes group B.\n",
            "Ar gument 420/125 = 16% of group A were falsely labeled “unlikely ,” whereas only\n",
            "5/125 = 4% of group B were falsely labeled “unlikely .” This data\n",
            "scientist (who considers an “unlikely” prediction to be a bad thing)\n",
            "insists that the model unfairly stigmatizes group A.\n",
            "Which of these data scientists is correct? Are any of them correct? Perhaps\n",
            "it depends on the context.\n",
            "Possibly you feel one way if the two groups are “men” and “women” and\n",
            "another way if the two groups are “R users” and “Python users.” Or\n",
            "possibly not if it turns out that Python users skew male and R users skew\n",
            "female?\n",
            "Possibly you feel one way if the model is for predicting whether a\n",
            "DataSciencester user will apply  for a job through the DataSciencester job\n",
            "board and another way if the model is predicting whether a user will pass\n",
            "such an interview .\n",
            "Possibly your opinion depends on the model itself, what features it takes\n",
            "into account, and what data it was trained on.\n",
            "In any event, my point is to impress upon you that there can be a tradeof f\n",
            "between “accuracy” and “fairness” (depending, of course, on how you\n",
            "define them) and that these tradeof fs don’ t always have obvious “right”\n",
            "solutions.\n",
            "C o l l a b o r a t i o n\n",
            "A  repressive (by your standards) country’ s government of ficials have\n",
            "finally decided to allow citizens to join DataSciencester . However , they\n",
            "insist that the users from their country not be allowed to discuss deep\n",
            "learning. Furthermore, they want you to report to them the names of any\n",
            "users who even try  to seek out information on deep learning.\n",
            "Are this country’ s data scientists better of f with access to the topic-limited\n",
            "(and surveilled) DataSciencester that you’d be allowed to of fer? Or are the\n",
            "proposed restrictions so awful that they’d be better of f with no access at all?I n t e r p r e t a b i l i t y\n",
            "The  DataSciencester HR department asks you to develop a model\n",
            "predicting which employees are most at risk of leaving the company , so that\n",
            "it can intervene and try to make them happier . (Attrition rate is an important\n",
            "component of the “10 Happiest W orkplaces” magazine feature that your\n",
            "CEO aspires to appear in.)\n",
            " \n",
            "Y ou’ve collected an assortment of historical data and are considering three\n",
            "models:\n",
            "A decision tree\n",
            "A neural network\n",
            "A high-priced “retention expert”\n",
            "One of your data scientists insists that you should just use whichever model\n",
            "performs best.\n",
            "A second insists that you not use the neural network model, as only the\n",
            "other two can explain their predictions, and that only explanation of the\n",
            "predictions can help HR institute widespread changes (as opposed to one-\n",
            "of f interventions).\n",
            "A third says that while the “expert” can of fer an  explanation for her\n",
            "predictions, there’ s no reason to take her at her word that it describes the\n",
            "r eal  reasons she predicted the way she did.\n",
            "As with our other examples, there is no absolute best choice here. In some\n",
            "circumstances (possibly for legal reasons or if your predictions are\n",
            "somehow life-changing) you might prefer a model that performs worse but\n",
            "whose predictions can be explained. In others, you might just want the\n",
            "model that predicts best. In still others, perhaps there is no interpretable\n",
            "model that performs well.R e c o m m e n d a t i o n s\n",
            "As  we discussed in Chapter 23 , a common data science application involves\n",
            "recommending things to people. When someone watches a Y ouT ube video,\n",
            "Y ouT ube recommends videos they should watch next.\n",
            "Y ouT ube makes money through advertising and (presumably) wants to\n",
            "recommend videos that you are more likely to watch, so that they can show\n",
            "you more advertisements. However , it turns out that people like to watch\n",
            "videos about conspiracy theories, which tend to feature in the\n",
            "recommendations.\n",
            "N O T E\n",
            "At the time I wrote this chapter , if you searched Y ouT ube for “saturn” the third result\n",
            "was “Something Is Happening On Saturn… Are THEY Hiding It?” which maybe gives\n",
            "you a sense of the kinds of videos I’m talking about.\n",
            "Does Y ouT ube have an obligation not to recommend conspiracy videos?\n",
            "Even if that’ s what lots of people seem to want to watch?\n",
            "A dif ferent example is that if you go to google.com (or bing.com) and start\n",
            "typing a search, the search engine will of fer suggestions to autocomplete\n",
            "your search. These suggestions are based (at least in part) on other people’ s\n",
            "searches; in particular , if other people are searching for unsavory things this\n",
            "may be reflected in your suggestions.\n",
            "Should a search engine try to af firmatively filter out suggestions it doesn’ t\n",
            "like? Google (for whatever reason) seems intent on not suggesting things\n",
            "related to people’ s religion. For example, if you type “mitt romney m” into\n",
            "Bing, the first suggestion is “mitt romney mormon” (which is what I would\n",
            "have expected), whereas Google refuses to provide that suggestion.\n",
            "Indeed, Google explicitly filters out autosuggestions that it considers\n",
            "“of fensive or disparaging” . (How it decides what’ s of fensive or disparaging\n",
            "is left vague.) And yet sometimes the truth is of fensive. Is protecting peoplefrom those suggestions the ethical thing to do? Or is it an unethical thing to\n",
            "do? Or is it not a question of ethics at all?\n",
            "B i a s e d  D a t a\n",
            "In “W ord V ectors”  we  used a corpus of documents to learn vector\n",
            "embeddings for words. These vectors were  designed to exhibit\n",
            "distributional similarity . That is, words that appear in similar contexts\n",
            "should have similar vectors. In particular , any biases that exist in the\n",
            "training data will be reflected in the word vectors themselves.\n",
            "For example, if our documents are all about how R users are moral\n",
            "reprobates and how Python users are paragons of virtue, most likely the\n",
            "model will learn such associations for “Python” and “R.”\n",
            "More commonly , word vectors are based on some combination of Google\n",
            "News articles, W ikipedia, books, and crawled web pages. This means that\n",
            "they’ll learn whatever distributional patterns are present in those sources.\n",
            "For example, if the majority of news articles about software engineers are\n",
            "about male  software engineers, then the learned vector for “software” might\n",
            "lie closer to vectors for other “male” words than to the vectors for “female”\n",
            "words.\n",
            "At that point any downstream applications you build using these vectors\n",
            "might also exhibit this closeness. Depending on the application, this may or\n",
            "may not be a problem for you. In that case there are various techniques that\n",
            "you can try to “remove” specific biases, although you’ll probably never get\n",
            "all of them. But it’ s something you should be aware of.\n",
            "Similarly , as  in the “photos” example in “Building Bad Data Products” , if\n",
            "you train a model on nonrepresentative data, there’ s a strong possibility it\n",
            "will perform poorly in the real world, possibly in ways that are of fensive or\n",
            "embarrassing.\n",
            "Along dif ferent lines, it’ s also possible that your algorithms might codify\n",
            "actual biases that exist out in the world. For example, your parole modelmay do a perfect job of predicting which released criminals get rearrested,\n",
            "but if those rearrests are themselves the result of biased real-world\n",
            "processes, then your model might be perpetuating that bias.\n",
            "D a t a  P r o t e c t i o n\n",
            "Y ou  know a lot about the DataSciencester users. Y ou know what\n",
            "technologies they like, who their data scientist friends are, where they work,\n",
            "how much they earn, how much time they spend on the site, which job\n",
            "postings they click on, and so forth.\n",
            "The VP of Monetization wants to sell this data to advertisers, who are eager\n",
            "to market their various “big data” solutions to your users. The Chief\n",
            "Scientist wants to share this data with academic researchers, who are keen\n",
            "to publish papers about who becomes a data scientist. The VP of\n",
            "Electioneering has plans to provide this data to political campaigns, most of\n",
            "whom are eager to recruit their own data science or ganizations. And the VP\n",
            "of Government Af fairs would like to use this data to answer questions from\n",
            "law enforcement.\n",
            "Thanks to a forward-thinking VP of Contracts, your users agreed to terms\n",
            "of service that guarantee you the right to do pretty much whatever you want\n",
            "with their data.\n",
            "However (as you have now come to expect), various of the data scientists\n",
            "on your team raise various objections to these various uses. One thinks it’ s\n",
            "wrong to hand the data over to advertisers; another worries that academics\n",
            "can’ t be trusted to safeguard the data responsibly . A third thinks that the\n",
            "company should stay out of politics, while the last insists that police can’ t\n",
            "be trusted and that collaborating with law enforcement will harm innocent\n",
            "people.\n",
            "Do any of these data scientists have a point?\n",
            "I n  S u m m a r yThese are a lot of things to worry about! And there are countless more we\n",
            "haven’ t mentioned, and still more that will come up in the future but that\n",
            "would never occur to us today .\n",
            "   \n",
            "F o r  F u r t h e r  E x p l o r a t i o n\n",
            "There  is no shortage of people professing important thoughts about\n",
            "data ethics. Searching on T witter (or your favorite news site) is\n",
            "probably the best way to find out about the most current data ethics\n",
            "controversy .\n",
            "If you want something slightly more practical, Mike Loukides,\n",
            "Hilary Mason, and DJ Patil have written a short ebook, Ethics and\n",
            "Data Science , on putting data ethics into practice, which I am\n",
            "honor -bound to recommend on account of Mike being the person\n",
            "who agreed to publish Data Science fr om Scratch  way back in\n",
            "2014. (Exercise: is this ethical of me?)Chapter 27. Go Forth and Do\n",
            "Data Science\n",
            "And now , once again, I bid my hideous pr ogeny go forth and pr osper .\n",
            "— Mary Shelley\n",
            "Where do you go from here? Assuming I haven’ t scared you of f of data\n",
            "science, there are a number of things you should learn next.\n",
            "I P y t h o n\n",
            "I  mentioned IPython  earlier in the book. It provides a shell with far more\n",
            "functionality than the standard Python shell, and it adds “magic functions”\n",
            "that allow you to (among other things) easily copy and paste code (which is\n",
            "normally complicated by the combination of blank lines and whitespace\n",
            "formatting) and run scripts from within the shell.\n",
            "Mastering IPython will make your life far easier . (Even learning just a little\n",
            "bit of IPython will make your life a lot easier .)\n",
            "N O T E\n",
            "In  the first edition, I also recommended that you learn about the IPython (now Jupyter)\n",
            "Notebook, a computational environment that allows you to combine text, live Python\n",
            "code, and visualizations.\n",
            "I’ve since become a notebook skeptic , as I find that they confuse beginners and\n",
            "encourage bad coding practices. (I have many other reasons too.) Y ou will surely\n",
            "receive plenty of encouragement to use them from people who aren’ t me, so just\n",
            "remember that I’m the dissenting voice.\n",
            "M a t h e m a t i c sThroughout this book, we dabbled in linear algebra ( Chapter 4 ), statistics\n",
            "( Chapter 5 ), probability ( Chapter 6 ), and various aspects of machine\n",
            "learning.\n",
            "T o be a good data scientist, you should know much more about these topics,\n",
            "and I encourage you to give each of them a more in-depth study , using the\n",
            "textbooks recommended at the ends of the chapters, your own preferred\n",
            "textbooks, online courses, or even real-life courses.\n",
            "N o t  f r o m  S c r a t c h\n",
            "Implementing  things “from scratch” is great for understanding how they\n",
            "work. But it’ s generally not great for performance (unless you’re\n",
            "implementing them specifically with performance in mind), ease of use,\n",
            "rapid prototyping, or error handling.\n",
            "In practice, you’ll want to use well-designed libraries that solidly\n",
            "implement the fundamentals. My original proposal for this book involved a\n",
            "second “now let’ s learn the libraries” half that O’Reilly , thankfully , vetoed.\n",
            "Since the first edition came out, Jake V anderPlas has written the Python\n",
            "Data Science Handbook  (O’Reilly), which is a good introduction to the\n",
            "relevant libraries and would be a good book for you to read next.\n",
            "NumPy\n",
            "NumPy  (for “Numeric Python”) provides  facilities for doing “real”\n",
            "scientific computing. It features arrays that perform better than our list -\n",
            "vectors, matrices that perform better than our list -of-list -matrices, and\n",
            "lots of numeric functions for working with them.\n",
            "NumPy is a building block for many other libraries, which makes it\n",
            "especially valuable to know .\n",
            "pandaspandas  provides  additional data structures for working with datasets in\n",
            "Python. Its primary abstraction is the DataFrame , which is conceptually\n",
            "similar to the NotQuiteABase Table  class we constructed in Chapter 24 ,\n",
            "but with much more functionality and better performance.\n",
            "If you’re going to use Python to munge, slice, group, and manipulate\n",
            "datasets, pandas is an invaluable tool.\n",
            "scikit-learn\n",
            "scikit-learn  is  probably the most popular library for doing machine learning\n",
            "in Python. It contains all the models we’ve implemented and many more\n",
            "that we haven’ t. On a real problem, you’d never build a decision tree from\n",
            "scratch; you’d let scikit-learn do the heavy lifting. On a real problem, you’d\n",
            "never write an optimization algorithm by hand; you’d count on scikit-learn\n",
            "to already be using a really good one.\n",
            "Its documentation contains many , many examples  of what it can do (and,\n",
            "more generally , what machine learning can do).\n",
            "V isualization\n",
            "The  matplotlib charts we’ve been creating have been clean and functional\n",
            "but not particularly stylish (and not at all interactive). If you want to get\n",
            "deeper into data visualization, you have several options.\n",
            "The first is to further explore matplotlib, only a handful of whose features\n",
            "we’ve actually covered. Its website contains many examples  of its\n",
            "functionality and a gallery  of some of the more interesting ones. If you want\n",
            "to create static visualizations (say , for printing in a book), this is probably\n",
            "your best next step.\n",
            "Y ou  should also check out seaborn , which is a library that (among other\n",
            "things) makes matplotlib more attractive.\n",
            "If you’d  like to create interactive  visualizations that you can share on the\n",
            "web, the  obvious choice is probably D3.js , a JavaScript library for creating\n",
            "“data-driven documents” (those are the three Ds). Even if you don’ t knowmuch JavaScript, it’ s often possible to crib examples from the D3 gallery\n",
            "and tweak them to work with your data. (Good data scientists copy from the\n",
            "D3 gallery; great data scientists steal  from the D3 gallery .)\n",
            "Even if you have no interest in D3, just browsing the gallery is itself a\n",
            "pretty incredible education in data visualization.\n",
            "Bokeh  is a  project that brings D3-style functionality into Python.\n",
            "R\n",
            "Although  you can totally get away with not learning R , a lot of data\n",
            "scientists and data science projects use it, so it’ s worth getting at least\n",
            "familiar with it.\n",
            "In part, this is so that you can understand people’ s R-based blog posts and\n",
            "examples and code; in part, this is to help you better appreciate the\n",
            "(comparatively) clean elegance of Python; and in part, this is to help you be\n",
            "a more informed participant in the never -ending “R versus Python”\n",
            "flamewars.\n",
            "Deep Learning\n",
            "Y ou can be a data scientist without doing deep learning, but you can’ t be a\n",
            "tr endy  data scientist without doing deep learning.\n",
            "The  two most popular deep learning frameworks for Python are T ensorFlow\n",
            "(created by Google) and PyT orch  (created by Facebook). The internet is full\n",
            "of tutorials for them that range from wonderful to awful.\n",
            "T ensorFlow  is older and more widely used, but PyT orch is (in my opinion)\n",
            "much easier to use and (in particular) much more beginner -friendly . I prefer\n",
            "(and recommend) PyT orch, but—as they say—no one ever got fired for\n",
            "choosing T ensorFlow .\n",
            "F i n d  D a t aIf  you’re doing data science as part of your job, you’ll most likely get the\n",
            "data as part of your job (although not necessarily). What if you’re doing\n",
            "data science for fun? Data is everywhere, but here are some starting points:\n",
            "Data.gov  is the  government’ s open data portal. If you want data on\n",
            "anything that has to do with the government (which seems to be\n",
            "most things these days), it’ s a good place to start.\n",
            "Reddit has a couple of forums, r/datasets  and r/data , that are places\n",
            "to both ask for and discover data.\n",
            "Amazon.com maintains a collection of public datasets  that they’d\n",
            "like you to analyze using their products (but that you can analyze\n",
            "with whatever products you want).\n",
            "Robb Seaton has a quirky list of curated datasets on his blog .\n",
            "Kaggle  is  a site that holds data science competitions. I never\n",
            "managed to get into it (I don’ t have much of a competitive nature\n",
            "when it comes to data science), but you might. They host a lot of\n",
            "datasets.\n",
            "Google  has a newish Dataset Search  that lets you (you guessed it)\n",
            "search for datasets.\n",
            "D o  D a t a  S c i e n c e\n",
            "Looking through data catalogs is fine, but the best projects (and products)\n",
            "are ones that tickle some sort of itch. Here are a few that I’ve done.\n",
            "Hacker News\n",
            "Hacker News  is a  news aggregation and discussion site for technology-\n",
            "related news. It collects lots and lots of articles, many of which aren’ t\n",
            "interesting to me.Accordingly , several years ago, I set out to build a Hacker News story\n",
            "classifier  to predict whether I would or would not be interested in any given\n",
            "story . This did not go over so well with the users of Hacker News, who\n",
            "resented the idea that someone might not be interested in every story on the\n",
            "site.\n",
            "This involved hand-labeling a lot of stories (in order to have a training set),\n",
            "choosing story features (for example, words in the title, and domains of the\n",
            "links), and training a Naive Bayes classifier not unlike our spam filter .\n",
            "For reasons now lost to history , I built it in Ruby . Learn from my mistakes.\n",
            "Fire T rucks\n",
            "For  many years I lived on a major street in downtown Seattle, halfway\n",
            "between a fire station and most of the city’ s fires (or so it seemed).\n",
            "Accordingly , I developed a recreational interest in the Seattle Fire\n",
            "Department.\n",
            "Luckily (from a data perspective), they maintain a Real-T ime 91 1 site  that\n",
            "lists every fire alarm along with the fire trucks involved.\n",
            "And so, to indulge my interest, I scraped many years’ worth of fire alarm\n",
            "data and performed a social network analysis  of the fire trucks. Among\n",
            "other things, this required me to invent a fire-truck-specific notion of\n",
            "centrality , which I called T ruckRank.\n",
            "T -Shirts\n",
            "I have  a young daughter , and an incessant source of frustration to me\n",
            "throughout her childhood has been that most “girls’ shirts” are quite boring,\n",
            "while many “boys’ shirts” are a lot of fun.\n",
            "In particular , it felt clear to me that there was a distinct dif ference between\n",
            "the shirts marketed to toddler boys and toddler girls. And so I asked myself\n",
            "if I could train a model to recognize these dif ferences.\n",
            "Spoiler: I could .This involved downloading the images of hundreds of shirts, shrinking\n",
            "them all to the same size, turning them into vectors of pixel colors, and\n",
            "using logistic regression to build a classifier .\n",
            "One approach looked simply at which colors were present in each shirt; a\n",
            "second found the first 10 principal components of the shirt image vectors\n",
            "and classified each shirt using its projections into the 10-dimensional space\n",
            "spanned by the “eigenshirts” ( Figure 27-1 ).\n",
            "Figur e 27-1. Eigenshirts corr esponding to the first principal component\n",
            "T weets on a Globe\n",
            "For  many years I’d wanted to build a “spinning globe” visualization.\n",
            "During the 2016 election, I built a small web app  that listened for geotagged\n",
            "tweets matching some search (I used “T rump,” as it appeared in lots of\n",
            "tweets at that time), displayed them, and spun a globe to their location as\n",
            "they appeared.\n",
            "This was entirely a JavaScript data project, so maybe learn some JavaScript.\n",
            "And Y ou?\n",
            "What interests you? What questions keep you up at night? Look for a\n",
            "dataset (or scrape some websites) and do some data science.\n",
            "Let me know what you find! Email me at joelgrus@gmail.com  or find me\n",
            "on T witter at @joelgrus .Index\n",
            "A\n",
            "A/B tests , Example: Running an A/B T est\n",
            "accuracy , Correctness\n",
            "activation functions , Other Activation Functions\n",
            "AllenNLP , For Further Exploration\n",
            "Altair library , For Further Exploration\n",
            "Anaconda Python distribution , Getting Python\n",
            "ar gs , ar gs and kwar gs\n",
            "ar gument unpacking , zip and Ar gument Unpacking\n",
            "arithmetic operations , V ectors\n",
            "arrays , Lists\n",
            "artificial neural networks , Neural Networks\n",
            "assert statements , Automated T esting and assert\n",
            "automated testing , Automated T esting and assert\n",
            "average (mean) , Central T endencies\n",
            "B\n",
            "backpropagation , Backpropagation\n",
            "bagging , Random Forests\n",
            "bar charts , Bar Charts - Line Chartsbatch gradient descent , Minibatch and Stochastic Gradient Descent\n",
            "Bayesian inference , Bayesian Inference\n",
            "Bayes’ s theorem , Bayes’ s Theorem\n",
            "Beautiful Soup library , HTML and the Parsing Thereof\n",
            "bell-shaped curve , The Normal Distribution\n",
            "BernoulliNB model , For Further Exploration\n",
            "Beta distributions , Bayesian Inference\n",
            "betweenness centrality , Betweenness Centrality - Betweenness Centrality\n",
            "bias input , Feed-Forward Neural Networks\n",
            "bias-variance tradeof f , The Bias-V ariance T radeof f\n",
            "biased data , Biased Data\n",
            "bigram models , n-Gram Language Models\n",
            "binary judgments , Correctness\n",
            "Binomial distributions , Bayesian Inference\n",
            "binomial random variables , The Central Limit Theorem\n",
            "Bokeh library , For Further Exploration , V isualization\n",
            "Booleans , T ruthiness\n",
            "bootstrap aggregating , Random Forests\n",
            "bootstrapping , Digression: The Bootstrap\n",
            "bottom-up hierarchical clustering , Bottom-Up Hierarchical Clustering -\n",
            "Bottom-Up Hierarchical Clustering\n",
            "breadth-first search , Betweenness Centralitybusiness models , Modeling\n",
            "Buzzword clouds , W ord Clouds\n",
            "C\n",
            "causation , Correlation and Causation\n",
            "central limit theorem , The Central Limit Theorem\n",
            "central tendencies , Central T endencies\n",
            "centrality\n",
            "betweenness , Betweenness Centrality - Betweenness Centrality\n",
            "closeness , Betweenness Centrality\n",
            "degree , Finding Key Connectors\n",
            "eigenvector , Eigenvector Centrality - Centrality\n",
            "other types of , For Further Exploration\n",
            "character -level RNNs , Example: Using a Character -Level RNN\n",
            "charts\n",
            "bar charts , Bar Charts - Line Charts\n",
            "line charts , matplotlib , Line Charts\n",
            "scatterplots , Scatterplots - For Further Exploration\n",
            "classes , Object-Oriented Programming\n",
            "classification trees , What Is a Decision T ree?\n",
            "cleaning data , Cleaning and Munging\n",
            "closeness centrality , Betweenness Centrality\n",
            "clusteringbottom-up hierarchical clustering , Bottom-Up Hierarchical Clustering -\n",
            "Bottom-Up Hierarchical Clustering\n",
            "choosing k , Choosing k\n",
            "clustering colors example , Example: Clustering Colors\n",
            "concept of , The Idea\n",
            "meetups example , Example: Meetups\n",
            "model for , The Model\n",
            "tools for , For Further Exploration\n",
            "unsupervised learning using , Clustering\n",
            "code examples, obtaining and using , Using Code Examples , Data Science\n",
            "coef ficient of determination , The Model , Goodness of Fit\n",
            "comma-separated files , Delimited Files\n",
            "conda package manager , V irtual Environments\n",
            "conditional probability , Conditional Probability\n",
            "confidence intervals , Confidence Intervals\n",
            "confounding variables , Simpson’ s Paradox\n",
            "confusion matrix , Correctness\n",
            "continuity corrections , p-V alues\n",
            "continuous bag-of-words (CBOW) , W ord V ectors\n",
            "continuous distributions , Continuous Distributions\n",
            "control flow , Control Flow\n",
            "convolutional layers , Example: MNISTcorrectness , Correctness\n",
            "correlation , Correlation - Correlation and Causation\n",
            "correlation matrix , Many Dimensions\n",
            "cosine similarity , W ord V ectors\n",
            "Counter instances , Counters\n",
            "Coursera , For Further Exploration , For Further Exploration\n",
            "covariance , Correlation\n",
            "CREA TE T ABLE statement , CREA TE T ABLE and INSER T\n",
            "cross-entropy loss function , Softmaxes and Cross-Entropy\n",
            "csv module (Python) , Delimited Files\n",
            "cumulative distribution function (CDF) , Continuous Distributions\n",
            "curse of dimensionality , The Curse of Dimensionality\n",
            "D\n",
            "D3-style visualizations , For Further Exploration\n",
            "D3.js library , For Further Exploration , V isualization\n",
            "data\n",
            "collecting\n",
            "piping data with stdin and stdout , stdin and stdout\n",
            "reading files , Reading Files\n",
            "sources for , Find Data\n",
            "tools for , For Further Exploration\n",
            "using T witter APIs , Example: Using the T witter APIs - Using T wythonweb scraping , Scraping the W eb\n",
            "describing single sets of\n",
            "dispersion , Dispersion\n",
            "histograms , Describing a Single Set of Data\n",
            "lar gest and smallest values , Describing a Single Set of Data\n",
            "mean (average) , Central T endencies\n",
            "median , Central T endencies\n",
            "mode , Central T endencies\n",
            "number of data points , Describing a Single Set of Data\n",
            "quantile , Central T endencies\n",
            "specific positions of values , Describing a Single Set of Data\n",
            "standard deviation , Dispersion\n",
            "variance , Dispersion\n",
            "working with\n",
            "cleaning and munging , Cleaning and Munging\n",
            "dataclasses , Dataclasses\n",
            "dimensionality reduction , Dimensionality Reduction\n",
            "exploring your data , Exploring Y our Data - Many Dimensions\n",
            "generating progress bars , An Aside: tqdm\n",
            "manipulating data , Manipulating Data\n",
            "rescaling , Rescaling\n",
            "resources for learning about , For Further Explorationtools for , For Further Exploration\n",
            "using namedtuple class , Using NamedT uples\n",
            "data ethics\n",
            "biased data , Biased Data\n",
            "censorship , Recommendations\n",
            "definition of term , No, Really , What Is Data Ethics?\n",
            "examples of data misuse , What Is Data Ethics?\n",
            "government restrictions , Collaboration\n",
            "issues resulting from bad products , Building Bad Data Products\n",
            "model selection , Interpretability\n",
            "of fensive predictions , Building Bad Data Products\n",
            "privacy , Data Protection\n",
            "resources for learning about , For Further Exploration\n",
            "tradeof fs between accuracy and fairness , T rading Of f Accuracy and\n",
            "Fairness\n",
            "wide-reaching ef fects of data science , Should I Care About Data Ethics?\n",
            "data mining , What Is Machine Learning?\n",
            "data science\n",
            "applications of\n",
            "extracting topics from data , T opics of Interest\n",
            "Hacker News , Hacker News\n",
            "network analysis , Finding Key Connectors - Finding Key Connectors ,\n",
            "Fire T ruckspredictive models , Salaries and Experience - Paid Accounts\n",
            "real-life examples of , What Is Data Science?\n",
            "recommender systems , Data Scientists Y ou May Know - Data Scientists\n",
            "Y ou May Know\n",
            "spinning globe visualization , T weets on a Globe\n",
            "T -shirt analysis , T -Shirts\n",
            "ascendance of data , The Ascendance of Data\n",
            "benefits of Python for , From Scratch\n",
            "definition of term , What Is Data Science?\n",
            "learning “from scratch” , From Scratch , Not from Scratch\n",
            "data visualization\n",
            "bar charts , Bar Charts - Line Charts\n",
            "line charts , Line Charts\n",
            "matplotlib library , matplotlib\n",
            "resources for learning about , V isualization\n",
            "scatterplots , Scatterplots - For Further Exploration\n",
            "tools for , For Further Exploration , V isualization\n",
            "uses for , V isualizing Data\n",
            "Data.gov , Find Data\n",
            "databases and SQL\n",
            "CREA TE T ABLE and INSER T , CREA TE T ABLE and INSER T\n",
            "DELETE , DELETEGROUP BY , GROUP BY\n",
            "indexes , Indexes\n",
            "JOIN , JOIN\n",
            "NoSQL databases , NoSQL\n",
            "ORDER BY , ORDER BY\n",
            "query optimization , Query Optimization\n",
            "resources for learning about , For Further Exploration\n",
            "SELECT , SELECT\n",
            "subqueries , Subqueries\n",
            "tools , For Further Exploration\n",
            "UPDA TE , UPDA TE\n",
            "dataclasses , Dataclasses\n",
            "Dataset Search , Find Data\n",
            "de-meaning data , Dimensionality Reduction\n",
            "decision boundary , Support V ector Machines\n",
            "decision nodes , Creating a Decision T ree\n",
            "decision trees\n",
            "benefits and drawbacks of , What Is a Decision T ree?\n",
            "creating , Creating a Decision T ree\n",
            "decision paths in , What Is a Decision T ree?\n",
            "entropy and , Entropy\n",
            "entropy of partitions , The Entropy of a Partitiongradient boosted decision trees , For Further Exploration\n",
            "implementing , Putting It All T ogether\n",
            "random forests technique , Random Forests\n",
            "resources for learning about , For Further Exploration\n",
            "tools for , For Further Exploration\n",
            "types of , What Is a Decision T ree?\n",
            "deep learning\n",
            "definition of term , Deep Learning\n",
            "dropout , Dropout\n",
            "Fizz Buzz example , Example: FizzBuzz Revisited\n",
            "Layers abstraction , The Layer Abstraction\n",
            "linear layer , The Linear Layer\n",
            "loss and optimization , Loss and Optimization\n",
            "MNIST example , Example: MNIST - Example: MNIST\n",
            "neural networks as sequences of layers , Neural Networks as a Sequence\n",
            "of Layers\n",
            "other activation functions , Other Activation Functions\n",
            "resources for learning about , For Further Exploration\n",
            "saving and loading models , Saving and Loading Models\n",
            "softmaxes and cross-entropy , Softmaxes and Cross-Entropy\n",
            "tensors , The T ensor\n",
            "tools for , For Further Exploration , Deep LearningXOR example , Example: XOR Revisited\n",
            "defaultdict , defaultdict\n",
            "degree centrality , Finding Key Connectors\n",
            "DELETE statement , DELETE\n",
            "delimited files , Delimited Files\n",
            "dependence , Dependence and Independence\n",
            "dictionaries , Dictionaries\n",
            "dimensionality reduction , Dimensionality Reduction\n",
            "directed edges , Network Analysis\n",
            "directed graphs , Directed Graphs and PageRank\n",
            "discrete distributions , Continuous Distributions\n",
            "dispersion , Dispersion\n",
            "distributional similarity , Biased Data\n",
            "domain expertise , Feature Extraction and Selection\n",
            "dot product , V ectors\n",
            "Dropout layer , Dropout\n",
            "dunder methods , Object-Oriented Programming\n",
            "dynamically typed languages , T ype Annotations\n",
            "E\n",
            "edges , Network Analysis\n",
            "eigenvector centrality\n",
            "centrality , Centralitymatrix multiplication , Matrix Multiplication\n",
            "elements\n",
            "creating sets of , Sets\n",
            "finding in collections , Sets\n",
            "embedding layer , W ord V ectors\n",
            "ensemble learning , Random Forests\n",
            "entropy , Entropy\n",
            "enumerate function , Iterables and Generators\n",
            "equivalence classes , Using Our Model\n",
            "ethics , No, Really , What Is Data Ethics?\n",
            "( see also  data ethics)\n",
            "exceptions , Exceptions\n",
            "F\n",
            "f-strings , Strings\n",
            "F1 scores , Correctness\n",
            "false negatives/false positives , Example: Flipping a Coin , Correctness\n",
            "feature extraction and selection , Feature Extraction and Selection\n",
            "features , Feed-Forward Neural Networks\n",
            "feed-forward neural networks , Feed-Forward Neural Networks\n",
            "files\n",
            "basics of text files , The Basics of T ext Files\n",
            "delimited files , Delimited Filesserialization of text files , JSON and XML\n",
            "first-class functions , Functions\n",
            "Fizz Buzz example , Example: Fizz Buzz , Example: FizzBuzz Revisited\n",
            "floating-point numbers , A More Sophisticated Spam Filter\n",
            "functional programming , Functional Programming\n",
            "functions , Functions\n",
            "G\n",
            "generators , Iterables and Generators\n",
            "gensim , For Further Exploration\n",
            "Gephi , For Further Exploration\n",
            "get method , Dictionaries\n",
            "Gibbs sampling , An Aside: Gibbs Sampling\n",
            "gradient boosted decision trees , For Further Exploration\n",
            "gradient descent\n",
            "choosing step size , Choosing the Right Step Size\n",
            "concept of , The Idea Behind Gradient Descent\n",
            "estimating the gradient , Estimating the Gradient\n",
            "minibatch and stochastic gradient descent , Minibatch and Stochastic\n",
            "Gradient Descent\n",
            "Optimizer abstraction for , Loss and Optimization\n",
            "resources for learning , For Further Exploration\n",
            "simple linear regression using , Using Gradient Descentusing the gradient , Using the Gradient\n",
            "using to fit models , Using Gradient Descent to Fit Models\n",
            "grammars , Grammars\n",
            "GROUP BY statement , GROUP BY\n",
            "GRU (gated recurrent unit) , Recurrent Neural Networks\n",
            "H\n",
            "Hacker News , Hacker News\n",
            "harmonic mean , Correctness\n",
            "hierarchical clustering , Bottom-Up Hierarchical Clustering - Bottom-Up\n",
            "Hierarchical Clustering\n",
            "HTML parsing , HTML and the Parsing Thereof\n",
            "hyperplanes , Support V ector Machines\n",
            "hypothesis and inference\n",
            "A/B tests , Example: Running an A/B T est\n",
            "Bayesian inference , Bayesian Inference\n",
            "coin flip example , Example: Flipping a Coin\n",
            "confidence intervals , Confidence Intervals\n",
            "p-hacking , p-Hacking\n",
            "p-values , p-V alues\n",
            "resources for learning , For Further Exploration\n",
            "statistical hypothesis testing , Statistical Hypothesis T esting\n",
            "IID3 algorithm , Creating a Decision T ree\n",
            "identity matrix , Matrices\n",
            "if statements , Control Flow\n",
            "if-then-else statements , Control Flow\n",
            "indentation, tabs versus spaces , Whitespace Formatting\n",
            "independence , Dependence and Independence\n",
            "inference  ( see  hypothesis and inference)\n",
            "INSER T statement , CREA TE T ABLE and INSER T\n",
            "interactive visualizations , V isualization\n",
            "IPython shell , V irtual Environments , For Further Exploration , IPython\n",
            "Iris dataset example , Example: The Iris Dataset\n",
            "item-based collaborative filtering , Item-Based Collaborative Filtering\n",
            "iterables , Iterables and Generators\n",
            "J\n",
            "JavaScript Object Notation (JSON) , JSON and XML\n",
            "JOIN statement , JOIN\n",
            "Jupyter notebook , IPython\n",
            "K\n",
            "k-means clustering , The Model\n",
            "k-nearest neighbors\n",
            "curse of dimensionality , The Curse of DimensionalityIris dataset example , Example: The Iris Dataset\n",
            "model for , The Model\n",
            "tools for , For Further Exploration\n",
            "uses for , k-Nearest Neighbors\n",
            "Kaggle , Find Data\n",
            "kernel trick , Support V ector Machines\n",
            "key connectors, finding , Finding Key Connectors - Finding Key Connectors ,\n",
            "Betweenness Centrality\n",
            "key/value pairs , Dictionaries\n",
            "kwar gs , ar gs and kwar gs\n",
            "L\n",
            "language models , n-Gram Language Models\n",
            "Latent Dirichlet Analysis (LDA) , T opic Modeling\n",
            "Layers abstraction\n",
            "basics of , The Layer Abstraction\n",
            "convolutional layers , Example: MNIST\n",
            "Dropout layer , Dropout\n",
            "linear layer , The Linear Layer\n",
            "layers of neurons , Feed-Forward Neural Networks\n",
            "least squares solution , The Model , Further Assumptions of the Least\n",
            "Squares Model\n",
            "LIBSVM , For Further Investigationline charts , matplotlib , Line Charts\n",
            "linear algebra\n",
            "matrices , Matrices - Matrices\n",
            "resources for learning , For Further Exploration\n",
            "tools for , For Further Exploration\n",
            "vectors , V ectors - V ectors\n",
            "linear independence , Further Assumptions of the Least Squares Model\n",
            "linear layer , The Linear Layer\n",
            "linear_model module , For Further Exploration\n",
            "lists\n",
            "appending items to , Lists\n",
            "checking list membership , Lists\n",
            "concatenating , Lists\n",
            "getting nth element of , Lists\n",
            "slicing , Lists\n",
            "sorting , Sorting\n",
            "transforming , List Comprehensions\n",
            "unpacking , Lists\n",
            "using as vectors , V ectors\n",
            "versus arrays , Lists\n",
            "logistic regression\n",
            "goodness of fit , Goodness of Fitlogistic function , The Logistic Function\n",
            "model application , Applying the Model\n",
            "problem example , The Problem\n",
            "support vector machines , Support V ector Machines\n",
            "tools for , For Further Investigation\n",
            "loss functions , Using Gradient Descent to Fit Models , Loss and\n",
            "Optimization , Softmaxes and Cross-Entropy\n",
            "LSTM (long short-term memory) , Recurrent Neural Networks\n",
            "M\n",
            "machine learning\n",
            "bias-variance tradeof f , The Bias-V ariance T radeof f\n",
            "correctness , Correctness\n",
            "definition of term , What Is Machine Learning?\n",
            "feature extraction and selection , Feature Extraction and Selection\n",
            "modeling , Modeling\n",
            "overfitting and underfitting , Overfitting and Underfitting\n",
            "resources for learning about , For Further Exploration\n",
            "magnitude, computing , V ectors\n",
            "manipulating data , Manipulating Data\n",
            "MapReduce\n",
            "analyzing status updates example , Example: Analyzing Status Updates\n",
            "basic algorithm , MapReducebenefits of , Why MapReduce?\n",
            "generalizing map_reduce function , MapReduce More Generally\n",
            "matrix multiplication example , Example: Matrix Multiplication\n",
            "uses for , MapReduce\n",
            "word count example , Example: W ord Count\n",
            "mathematics\n",
            "linear algebra , Linear Algebra - For Further Exploration\n",
            "probability , Probability - For Further Exploration\n",
            "statistics , Statistics - For Further Exploration\n",
            "matplotlib library , matplotlib , For Further Exploration\n",
            "matrices , Matrices - Matrices\n",
            "matrix decomposition functions , For Further Exploration\n",
            "matrix factorization , Matrix Factorization - Matrix Factorization\n",
            "matrix multiplication , Matrix Multiplication , Example: Matrix\n",
            "Multiplication\n",
            "maximum likelihood estimation , Maximum Likelihood Estimation\n",
            "mean (average) , Central T endencies\n",
            "mean squared error , Using Gradient Descent to Fit Models\n",
            "median , Central T endencies\n",
            "meetups example (clustering) , Example: Meetups\n",
            "member functions , Object-Oriented Programming\n",
            "methodsdunder methods , Object-Oriented Programming\n",
            "private methods , Object-Oriented Programming\n",
            "minibatch gradient descent , Minibatch and Stochastic Gradient Descent\n",
            "MNIST dataset example , Example: MNIST - Example: MNIST\n",
            "mode , Central T endencies\n",
            "modeling , Modeling , T opic Modeling - T opic Modeling\n",
            "models of language , n-Gram Language Models\n",
            "modules , Modules\n",
            "momentum , Loss and Optimization\n",
            "MongoDB , For Further Exploration\n",
            "most_common method , Counters\n",
            "Movie-Lens 100k dataset , Matrix Factorization\n",
            "multi-dimensional datasets , Many Dimensions\n",
            "multiline strings , Strings\n",
            "multiple regression\n",
            "assumptions of least square model , Further Assumptions of the Least\n",
            "Squares Model\n",
            "bootstrapping new datasets , Digression: The Bootstrap\n",
            "goodness of fit , Goodness of Fit\n",
            "model fitting , Fitting the Model\n",
            "model for , The Model\n",
            "model interpretation , Interpreting the Modelregularization , Regularization\n",
            "resources for learning about , For Further Exploration\n",
            "standard errors of regression coef ficients , Standard Errors of Regression\n",
            "Coef ficients\n",
            "tools for , For Further Exploration\n",
            "munging data , Cleaning and Munging\n",
            "MySQL , For Further Exploration\n",
            "N\n",
            "Naive Bayes\n",
            "model testing , T esting Our Model\n",
            "model use , Using Our Model\n",
            "resources for learning about , For Further Exploration\n",
            "spam filter examples , Naive Bayes - A More Sophisticated Spam Filter\n",
            "spam filter implementation , Implementation\n",
            "tools for , For Further Exploration\n",
            "namedtuple class , Using NamedT uples\n",
            "natural language processing (NLP)\n",
            "character -level RNN example , Example: Using a Character -Level RNN\n",
            "definition of term , Natural Language Processing\n",
            "Gibbs sampling , An Aside: Gibbs Sampling\n",
            "grammars , Grammars\n",
            "n-gram language models , n-Gram Language Modelsrecurrent neural networks (RNNs) , Recurrent Neural Networks\n",
            "resources for learning about , For Further Exploration\n",
            "tools for , For Further Exploration\n",
            "topic modeling , T opic Modeling - T opic Modeling\n",
            "word clouds , W ord Clouds\n",
            "word vectors , W ord V ectors - W ord V ectors\n",
            "nearest neighbors classification , k-Nearest Neighbors\n",
            "Netflix Prize , For Further Exploration\n",
            "network analysis\n",
            "betweenness centrality , Betweenness Centrality - Betweenness Centrality\n",
            "directed graphs and PageRank , Directed Graphs and PageRank\n",
            "eigenvector centrality , Eigenvector Centrality - Centrality\n",
            "finding key connectors example , Finding Key Connectors - Finding Key\n",
            "Connectors\n",
            "nodes and edges in , Network Analysis\n",
            "resources for learning about , For Further Exploration\n",
            "tools for , For Further Exploration\n",
            "T ruck-Rank example , Fire T rucks\n",
            "NetworkX , For Further Exploration\n",
            "neural networks\n",
            "as sequences of layers , Neural Networks as a Sequence of Layers\n",
            "backpropagation , Backpropagationcomponents of , Neural Networks\n",
            "feed-forward neural networks , Feed-Forward Neural Networks\n",
            "Fizz Buzz example , Example: Fizz Buzz\n",
            "perceptrons , Perceptrons\n",
            "NL TK , For Further Exploration\n",
            "nodes , Network Analysis\n",
            "None value , T ruthiness\n",
            "nonrepresentative data , Biased Data\n",
            "normal distribution , The Normal Distribution\n",
            "NoSQL databases , NoSQL\n",
            "null hypothesis , Statistical Hypothesis T esting\n",
            "null values , T ruthiness\n",
            "NumPy library , V ectors , For Further Exploration , NumPy\n",
            "O\n",
            "object-oriented programming , Object-Oriented Programming\n",
            "one-dimensional datasets , Exploring One-Dimensional Data\n",
            "one-hot-encoding , W ord V ectors\n",
            "Optimizer abstraction , Loss and Optimization\n",
            "ORDER BY statement , ORDER BY\n",
            "overfitting and underfitting , Overfitting and Underfitting\n",
            "Pp-hacking , p-Hacking\n",
            "p-values , p-V alues\n",
            "PageRank , Directed Graphs and PageRank\n",
            "pandas , For Further Exploration , Delimited Files , For Further Exploration ,\n",
            "For Further Exploration , pandas\n",
            "parameterized models , What Is Machine Learning?\n",
            "partial derivatives , Estimating the Gradient\n",
            "perceptrons , Perceptrons\n",
            "pip package manager , V irtual Environments\n",
            "popularity-based recommender systems , Recommending What’ s Popular\n",
            "Porter Stemmer , Using Our Model\n",
            "posterior distributions , Bayesian Inference\n",
            "PostgreSQL , For Further Exploration\n",
            "precision , Correctness\n",
            "predictive models\n",
            "decision trees , Decision T rees - For Further Exploration\n",
            "definition of modeling , Modeling\n",
            "guarding against potentially of fensive predictions , Building Bad Data\n",
            "Products\n",
            "k-nearest neighbors , k-Nearest Neighbors - For Further Exploration\n",
            "logistic regression , Logistic Regression - Support V ector Machines\n",
            "machine learning and , What Is Machine Learning?\n",
            "multiple regression , Multiple Regression - For Further Explorationneural networks , Neural Networks - For Further Exploration\n",
            "paid accounts example , Paid Accounts\n",
            "salaries and experience example , Salaries and Experience - Salaries and\n",
            "Experience\n",
            "simple linear regression , Simple Linear Regression - For Further\n",
            "Exploration\n",
            "tradeof fs between accuracy and fairness , T rading Of f Accuracy and\n",
            "Fairness\n",
            "types of models , What Is Machine Learning?\n",
            "principal component analysis (PCA) , Dimensionality Reduction\n",
            "prior distributions , Bayesian Inference\n",
            "private methods , Object-Oriented Programming\n",
            "probability\n",
            "Bayes’ s theorem , Bayes’ s Theorem\n",
            "central limit theorem , The Central Limit Theorem\n",
            "conditional probability , Conditional Probability\n",
            "continuous distributions , Continuous Distributions\n",
            "definition of term , Probability\n",
            "dependence and independence , Dependence and Independence\n",
            "normal distribution , The Normal Distribution\n",
            "random variables , Random V ariables\n",
            "resources for learning , For Further Exploration\n",
            "tools for , For Further Explorationprobability density function (PDF) , Continuous Distributions\n",
            "progress bars, generating , An Aside: tqdm\n",
            "pseudocounts , A More Sophisticated Spam Filter\n",
            "Python\n",
            "ar gs , ar gs and kwar gs\n",
            "ar gument unpacking , zip and Ar gument Unpacking\n",
            "automated testing and assert statements , Automated T esting and assert\n",
            "benefits of for data science , From Scratch\n",
            "control flow , Control Flow\n",
            "Counter instances , Counters\n",
            "csv module , Delimited Files\n",
            "default dict , defaultdict\n",
            "dictionaries , Dictionaries\n",
            "downloading and installing , Getting Python\n",
            "exceptions , Exceptions\n",
            "functional programming , Functional Programming\n",
            "functions , Functions\n",
            "iterables and generators , Iterables and Generators\n",
            "json module , JSON and XML\n",
            "kwar gs , ar gs and kwar gs\n",
            "list comprehensions , List Comprehensions\n",
            "lists , Listsmodules , Modules\n",
            "object-oriented programming , Object-Oriented Programming\n",
            "randomness , Randomness\n",
            "regular expressions , Regular Expressions\n",
            "sets , Sets\n",
            "sorting , Sorting\n",
            "statsmodels module , For Further Exploration\n",
            "strings , Strings\n",
            "truthiness , T ruthiness\n",
            "tuples , T uples\n",
            "tutorials and documentation , For Further Exploration\n",
            "type annotations , T ype Annotations - How to W rite T ype Annotations\n",
            "versions , Getting Python\n",
            "virtual environments , V irtual Environments\n",
            "whitespace formatting , Whitespace Formatting\n",
            "Zen of Python , The Zen of Python\n",
            "zip function , zip and Ar gument Unpacking\n",
            "PyT orch , For Further Exploration , Deep Learning\n",
            "Q\n",
            "quantile , Central T endencies\n",
            "RR , R\n",
            "R-squared , The Model , Goodness of Fit\n",
            "random forests technique , Random Forests\n",
            "random variables , Random V ariables\n",
            "randomness , Randomness\n",
            "raw strings , Strings\n",
            "recall , Correctness\n",
            "recommender systems\n",
            "“Data Scientists Y ou May Know” suggester , Data Scientists Y ou May\n",
            "Know - Data Scientists Y ou May Know\n",
            "dataset of users_interests , Recommender Systems\n",
            "item-based collaborative filtering , Item-Based Collaborative Filtering\n",
            "manual curation , Manual Curation\n",
            "matrix factorization , Matrix Factorization - Matrix Factorization\n",
            "popularity-based , Recommending What’ s Popular\n",
            "tools for , For Further Exploration\n",
            "user -based collaborative filtering , User -Based Collaborative Filtering\n",
            "recurrent neural networks (RNNs) , Recurrent Neural Networks\n",
            "regression coef ficients , Standard Errors of Regression Coef ficients\n",
            "regression trees , What Is a Decision T ree?\n",
            "regular expressions , Regular Expressions\n",
            "regularization , Regularizationreinforcement models , What Is Machine Learning?\n",
            "relational databases , Databases and SQL\n",
            "requests library , HTML and the Parsing Thereof\n",
            "rescaling data , Rescaling\n",
            "robots.txt files , Example: Keeping T abs on Congress\n",
            "S\n",
            "scalar multiplication , V ectors\n",
            "scale , Rescaling\n",
            "scatterplot matrix , Many Dimensions\n",
            "scatterplots , Scatterplots - For Further Exploration\n",
            "scikit-learn , For Further Exploration , For Further Exploration , For Further\n",
            "Exploration , For Further Exploration , For Further Investigation , For Further\n",
            "Exploration , For Further Exploration , scikit-learn\n",
            "SciPy , For Further Exploration , For Further Exploration\n",
            "scipy .stats , For Further Exploration\n",
            "Scrapy , For Further Exploration\n",
            "seaborn , For Further Exploration , V isualization\n",
            "SELECT statement , SELECT\n",
            "semisupervised models , What Is Machine Learning?\n",
            "serialization , JSON and XML\n",
            "sets , Sets\n",
            "sigmoid function , Feed-Forward Neural Networks , Other Activation\n",
            "Functionssignificance , Example: Flipping a Coin\n",
            "simple linear regression\n",
            "maximum likelihood estimation , Maximum Likelihood Estimation\n",
            "model for , The Model\n",
            "using gradient descent , Using Gradient Descent\n",
            "Simpson's paradox , Simpson’ s Paradox\n",
            "skip-gram model , W ord V ectors\n",
            "slicing lists , Lists\n",
            "softmax function , Softmaxes and Cross-Entropy\n",
            "sorting , Sorting\n",
            "spaCy , For Further Exploration\n",
            "spam filter example , Feature Extraction and Selection , Naive Bayes - Using\n",
            "Our Model\n",
            "SpamAssassin public corpus , Using Our Model\n",
            "SQLite , For Further Exploration\n",
            "standard deviation , Dispersion\n",
            "standard errors , Standard Errors of Regression Coef ficients\n",
            "standard normal distribution , The Normal Distribution\n",
            "statically typed languages , T ype Annotations\n",
            "statistical models of language , n-Gram Language Models\n",
            "statistics\n",
            "correlation , Correlation - Correlation\n",
            "causation and , Correlation and Causationcorrelational caveats , Some Other Correlational Caveats\n",
            "Simpson's paradox , Simpson’ s Paradox\n",
            "describing single sets of data , Describing a Single Set of Data - Dispersion\n",
            "resources for learning , For Further Exploration\n",
            "tools for , For Further Exploration\n",
            "StatsModels , For Further Exploration\n",
            "statsmodels , For Further Exploration\n",
            "status updates, analyzing , Example: Analyzing Status Updates\n",
            "stemmer functions , Using Our Model\n",
            "stochastic gradient descent , Minibatch and Stochastic Gradient Descent\n",
            "stride , Lists\n",
            "strings , Strings , JSON and XML\n",
            "Structured Query Language (SQL) , Databases and SQL  ( see  also databases\n",
            "and SQL)\n",
            "Student’ s t-distribution , Standard Errors of Regression Coef ficients\n",
            "Sum layer , Recurrent Neural Networks\n",
            "sum of squares, computing , V ectors\n",
            "supervised models , What Is Machine Learning?\n",
            "support vector machines , Support V ector Machines\n",
            "Surprise , For Further Exploration\n",
            "sys.stdin , stdin and stdout\n",
            "sys.stdout , stdin and stdoutT\n",
            "tab-separated files , Delimited Files\n",
            "tanh function , Other Activation Functions\n",
            "T ensorFlow , Deep Learning\n",
            "tensors , The T ensor\n",
            "ternerary operators , Control Flow\n",
            "test sets , Overfitting and Underfitting\n",
            "text files , The Basics of T ext Files , JSON and XML\n",
            "topic modeling , T opic Modeling - T opic Modeling\n",
            "tqdm library , An Aside: tqdm\n",
            "training sets , Overfitting and Underfitting\n",
            "trigrams , n-Gram Language Models\n",
            "true positives/true negatives , Correctness\n",
            "truthiness , T ruthiness\n",
            "tuples , T uples , Using NamedT uples\n",
            "T witter APIs , Example: Using the T witter APIs - Using T wython\n",
            "two-dimensional datasets , T wo Dimensions\n",
            "T wython library , Example: Using the T witter APIs - Using T wython\n",
            "type 1/type 2 errors , Example: Flipping a Coin , Correctness\n",
            "type annotations , T ype Annotations - How to W rite T ype Annotations\n",
            "Uunauthenticated APIs , Using an Unauthenticated API\n",
            "underfitting and overfitting , Overfitting and Underfitting\n",
            "underflow , A More Sophisticated Spam Filter\n",
            "undirected edges , Network Analysis\n",
            "uniform distributions , Continuous Distributions\n",
            "unit tests , T esting Our Model\n",
            "unpacking lists , Lists\n",
            "unsupervised learning , Clustering\n",
            "unsupervised models , What Is Machine Learning?\n",
            "UPDA TE statement , UPDA TE\n",
            "user -based collaborative filtering , User -Based Collaborative Filtering\n",
            "V\n",
            "validation sets , Overfitting and Underfitting\n",
            "variables\n",
            "binomial random variables , The Central Limit Theorem\n",
            "confounding variables , Simpson’ s Paradox\n",
            "random variables , Random V ariables\n",
            "variance , Dispersion , The Bias-V ariance T radeof f\n",
            "vectors , V ectors - V ectors\n",
            "virtual environments , V irtual Environments\n",
            "Wweak learners , Random Forests\n",
            "web scraping\n",
            "HTML parsing , HTML and the Parsing Thereof\n",
            "press release data example , Example: Keeping T abs on Congress\n",
            "using APIs , Using APIs\n",
            "weight tensors, randomly generating , The Linear Layer\n",
            "while loops , Control Flow\n",
            "whitespace formatting , Whitespace Formatting\n",
            "word clouds , W ord Clouds\n",
            "word counting , Counters , Example: W ord Count\n",
            "word vectors , W ord V ectors - W ord V ectors\n",
            "X\n",
            "Xavier initialization , The Linear Layer\n",
            "XGBoost , For Further Exploration\n",
            "XOR example , Example: XOR Revisited\n",
            "Z\n",
            "Zen of Python , The Zen of Python\n",
            "zip function , zip and Ar gument UnpackingAbout the Author\n",
            "Joel Grus  is a research engineer at the Allen Institute for Artificial\n",
            "Intelligence. Previously he worked as a software engineer at Google and a\n",
            "data scientist at several startups. He lives in Seattle, where he regularly\n",
            "attends data science happy hours. He blogs infrequently at joelgrus.com  and\n",
            "tweets all day long at @joelgrus .Colophon\n",
            "The animal on the cover of Data Science fr om Scratch , Second Edition, is a\n",
            "rock ptarmigan ( Lagopus muta ). This hardy , chicken-sized member of the\n",
            "grouse family inhabits the tundra environments of the northern hemisphere,\n",
            "living in the arctic and subarctic regions of Eurasia and North America. A\n",
            "ground feeder , it forages across these grasslands on its well-feathered feet,\n",
            "eating birch and willow buds, as well as seeds, flowers, leaves, and berries.\n",
            "Rock ptarmigan chicks also eat insects.\n",
            "Rock ptarmigan are best known for the striking annual changes in their\n",
            "cryptic camouflage, having evolved to molt and regrow white and\n",
            "brownish-colored feathers a few times over the course of a year to best\n",
            "match the changing seasonal colors of their environment. In winter they\n",
            "have white feathers; in spring and fall, as snow cover mixes with open\n",
            "grassland, their feathers mix white and brown; and in summer , their\n",
            "patterned brown feathers match the varied coloring of the tundra. W ith this\n",
            "camouflage female ptarmigan can near -invisibly incubate their eggs, which\n",
            "are laid in nests on the ground.\n",
            "Mature male rock ptarmigan also have a fringed, red comb structure over\n",
            "their eyes. During breeding season this is used for courtship display as well\n",
            "as signaling between contending males (studies have shown a correlation\n",
            "between comb size and male testosterone levels).\n",
            "Ptarmigan populations are currently declining, though in their range they\n",
            "remain common (albeit dif ficult to spot). Ptarmigan have many predators,\n",
            "including arctic foxes, gyrfalcons, gulls, and jaegers. Also, in time, climate\n",
            "change may make their seasonal color changes a liability .\n",
            "Many of the animals on O’Reilly covers are endangered; all of them are\n",
            "important to the world.\n",
            "The cover image is from Cassell’ s Book of Bir ds  (1875), by Thomas R ymer\n",
            "Jones. The cover fonts are Gilroy and Guardian Sans. The text font is\n",
            "Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the\n",
            "code font is Dalton Maag’ s Ubuntu Mono.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(full_document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-JNlW6lLnPf",
        "outputId": "c87bd586-057d-4f51-c979-daecd474d1a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RAG.index(\n",
        "    collection=[full_document],\n",
        "    index_name=\"Data Science Book\",\n",
        "    max_document_length=512,\n",
        "    split_documents=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "vc7w2Ut7LAcR",
        "outputId": "5e5b7dd4-9ff1-4d9a-8fb9-d77940d15681"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
            "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
            "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
            "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
            "--------------------\n",
            "\n",
            "\n",
            "[Oct 12, 11:56:52] #> Creating directory .ragatouille/colbert/indexes/Utility Bills \n",
            "\n",
            "\n",
            "[Oct 12, 11:56:54] [0] \t\t #> Encoding 410 passages..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 12, 11:56:59] [0] \t\t avg_doclen_est = 337.4073181152344 \t len(local_sample) = 410\n",
            "[Oct 12, 11:56:59] [0] \t\t Creating 4,096 partitions.\n",
            "[Oct 12, 11:56:59] [0] \t\t *Estimated* 138,337 embeddings.\n",
            "[Oct 12, 11:56:59] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/Utility Bills/plan.json ..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/colbert/indexing/collection_indexer.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  sub_sample = torch.load(sub_sample_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "used 20 iterations (3.387s) to cluster 131421 items into 4096 clusters\n",
            "[Oct 12, 11:57:03] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 12, 11:58:36] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colbert/indexing/codecs/residual.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  centroids = torch.load(centroids_path, map_location='cpu')\n",
            "/usr/local/lib/python3.10/dist-packages/colbert/indexing/codecs/residual.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  avg_residual = torch.load(avgresidual_path, map_location='cpu')\n",
            "/usr/local/lib/python3.10/dist-packages/colbert/indexing/codecs/residual.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bucket_cutoffs, bucket_weights = torch.load(buckets_path, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.033, 0.032, 0.03, 0.029, 0.028, 0.032, 0.029, 0.028, 0.03, 0.03, 0.03, 0.031, 0.03, 0.032, 0.03, 0.032, 0.028, 0.031, 0.03, 0.029, 0.03, 0.03, 0.028, 0.031, 0.029, 0.029, 0.031, 0.031, 0.031, 0.032, 0.03, 0.033, 0.032, 0.028, 0.029, 0.028, 0.034, 0.03, 0.03, 0.037, 0.033, 0.031, 0.03, 0.03, 0.029, 0.028, 0.03, 0.033, 0.032, 0.031, 0.03, 0.029, 0.032, 0.03, 0.031, 0.031, 0.033, 0.032, 0.037, 0.027, 0.031, 0.032, 0.031, 0.03, 0.032, 0.032, 0.032, 0.03, 0.03, 0.03, 0.031, 0.028, 0.03, 0.031, 0.031, 0.032, 0.031, 0.031, 0.033, 0.035, 0.032, 0.031, 0.031, 0.03, 0.029, 0.03, 0.029, 0.03, 0.03, 0.035, 0.03, 0.032, 0.029, 0.033, 0.029, 0.029, 0.033, 0.029, 0.031, 0.028, 0.03, 0.032, 0.03, 0.031, 0.029, 0.028, 0.028, 0.029, 0.031, 0.029, 0.032, 0.033, 0.031, 0.031, 0.033, 0.029, 0.032, 0.03, 0.029, 0.032, 0.029, 0.03, 0.03, 0.032, 0.029, 0.033, 0.03, 0.029]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 12, 12:00:13] [0] \t\t #> Encoding 410 passages..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "1it [00:03,  3.82s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colbert/indexing/codecs/residual_embeddings.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(codes_path, map_location='cpu')\n",
            "100%|██████████| 1/1 [00:00<00:00, 914.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 12, 12:00:17] #> Optimizing IVF to store map from centroids to list of pids..\n",
            "[Oct 12, 12:00:17] #> Building the emb2pid mapping..\n",
            "[Oct 12, 12:00:17] len(emb2pid) = 138337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4096/4096 [00:00<00:00, 37404.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 12, 12:00:17] #> Saved optimized IVF to .ragatouille/colbert/indexes/Utility Bills/ivf.pid.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done indexing!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.ragatouille/colbert/indexes/Utility Bills'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do Retrieval"
      ],
      "metadata": {
        "id": "IXHrYoQqMrFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = RAG.search(query=\"What metric should I use for Loss in Multi Class Classification problem?\", k=3)\n"
      ],
      "metadata": {
        "id": "Zwz5trRaLNOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f6da0b-1450-4b46-8203-2867b81bc0dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading searcher for index Utility Bills for the first time... This may take a few seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 12, 12:01:37] #> Loading codec...\n",
            "[Oct 12, 12:01:37] #> Loading IVF...\n",
            "[Oct 12, 12:01:37] #> Loading doclens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.10/dist-packages/colbert/search/index_loader.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ivf, ivf_lengths = torch.load(os.path.join(self.index_path, \"ivf.pid.pt\"), map_location='cpu')\n",
            "100%|██████████| 1/1 [00:00<00:00, 3170.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 12, 12:01:37] #> Loading codes and residuals...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colbert/indexing/codecs/residual_embeddings.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(residuals_path, map_location='cpu')\n",
            "100%|██████████| 1/1 [00:00<00:00, 40.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searcher loaded!\n",
            "\n",
            "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
            "#> Input: . What metric should I use for Loss in Multi Class Classification problem?, \t\t True, \t\t None\n",
            "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054, 12046,  2323,  1045,  2224,  2005,  3279,  1999,\n",
            "         4800,  2465,  5579,  3291,  1029,   102,   103,   103,   103,   103,\n",
            "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103], device='cuda:0')\n",
            "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJBXVqVBM1bR",
        "outputId": "9c3d551e-811c-46a5-98c2-188a62385886"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': 'So let’ s write that first.\\nW e’ll pass it our model, the data, a loss function, and (if we’re training) an\\noptimizer .It will make a pass through our data, track performance, and (if we passed\\nin an optimizer) update our parameters:\\nimport tqdm \\n \\ndef loop(model: Layer, \\n         images: List[Tensor], \\n         labels: List[Tensor], \\n         loss: Loss, \\n         optimizer: Optimizer = None) -> None: \\n    correct = 0         # Track number of correct predictions. \\n    total_loss = 0.0    # Track total loss. \\n \\n    with tqdm.trange(len(images)) as t: \\n        for i in t: \\n            predicted = model.forward(images[i])             # Predict. \\n            if argmax(predicted) == argmax(labels[i]):       # Check for \\n                correct += 1                                 # correctness. \\n            total_loss += loss.loss(predicted, labels[i])    # Compute loss. \\n \\n            # If we\\'re training, backpropagate gradient and update weights. \\n            if optimizer is not None: \\n                gradient = loss.gradient(predicted, labels[i]) \\n                model.backward(gradient) \\n                optimizer.step(model) \\n \\n            # And update our metrics in the progress bar. \\n            avg_loss = total_loss / (i + 1) \\n            acc = correct / (i + 1) \\n            t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")\\nAs a baseline, we can use our deep learning library to train a (multiclass)\\nlogistic regression model, which is just a single linear layer followed by a\\nsoftmax. This model (in essence) just looks for 10 linear functions such that\\nif the input represents, say , a 5, then the 5th linear function produces the\\nlar gest output.',\n",
              "  'score': 16.375,\n",
              "  'rank': 1,\n",
              "  'document_id': '77351f43-8e44-413e-ad03-1ccbb5e5ef3a',\n",
              "  'passage_id': 269},\n",
              " {'content': 'The only\\ntrick is that we’ll need to use tensor_combine :\\nclass SSE(Loss): \\n    \"\"\"Loss function that computes the sum of the squared errors.\"\"\" \\n    def loss(self, predicted: Tensor, actual: Tensor) -> float: \\n        # Compute the tensor of squared differences \\n        squared_errors = tensor_combine( \\n            lambda predicted, actual: (predicted - actual) ** 2, \\n            predicted, \\n            actual) \\n \\n        # And just add them up \\n        return tensor_sum(squared_errors) \\n \\n    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor: \\n        return tensor_combine( \\n            lambda predicted, actual: 2 * (predicted - actual), \\n            predicted, \\n            actual)\\n(W e’ll look at a dif ferent loss function in a bit.)\\nThe  last piece to figure out is gradient descent. Throughout the book we’ve\\ndone all of our gradient descent manually by having a training loop that\\ninvolves something like:\\ntheta = gradient_step(theta, grad, -learning_rate)\\nHere that won’ t quite work for us, for a couple reasons. The first is that our\\nneural nets will have many parameters, and we’ll need to update all ofthem. The second is that we’d like to be able to use more clever variants of\\ngradient descent, and we don’ t want to have to rewrite them each time.\\nAccordingly , we’ll introduce a (you guessed it) Optimizer  abstraction, of\\nwhich gradient descent will be a specific instance:\\nclass Optimizer: \\n    \"\"\" \\n    An optimizer updates the weights of a layer (in place) using information \\n    known by either the layer or the optimizer (or by both). \\n    \"\"\"',\n",
              "  'score': 16.046875,\n",
              "  'rank': 2,\n",
              "  'document_id': '77351f43-8e44-413e-ad03-1ccbb5e5ef3a',\n",
              "  'passage_id': 255},\n",
              " {'content': 'def __init__(self, layers: List[Layer]) -> None: \\n        self.layers = layers \\n \\n    def forward(self, input): \\n        \"\"\"Just forward the input through the layers in order.\"\"\" \\n        for layer in self.layers: \\n            input = layer.forward(input) \\n        return input \\n \\n    def backward(self, gradient): \\n        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\" \\n        for layer in reversed(self.layers): \\n            gradient = layer.backward(gradient) \\n        return gradient \\n \\n    def params(self) -> Iterable[Tensor]: \\n        \"\"\"Just return the params from each layer.\"\"\" \\n        return (param for layer in self.layers for param in layer.params()) \\n \\n    def grads(self) -> Iterable[Tensor]: \\n        \"\"\"Just return the grads from each layer.\"\"\" \\n        return (grad for layer in self.layers for grad in layer.grads())\\nSo we could represent the neural network we used for XOR as:\\nxor_net = Sequential([ \\n    Linear(input_dim=2, output_dim=2), \\n    Sigmoid(), \\n    Linear(input_dim=2, output_dim=1), \\n    Sigmoid() \\n])\\nBut we still need a little more machinery to train it.\\nL o s s  a n d  O p t i m i z a t i o n\\nPreviously  we wrote out individual loss functions and gradient functions for\\nour models. Here we’ll want to experiment with dif ferent loss functions, so\\n(as usual) we’ll introduce a new Loss  abstraction that encapsulates both the\\nloss computation and the gradient computation:class Loss: \\n    def loss(self, predicted: Tensor, actual: Tensor) -> float: \\n        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\" \\n        raise NotImplementedError \\n \\n    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor: \\n        \"\"\"How does the loss change as the predictions change?\"\"\" \\n        raise NotImplementedError\\nW e’ve already worked many times with the loss that’ s the sum of the\\nsquared errors, so we should have an easy time implementing that. The only\\ntrick is that we’ll need to use tensor_combine :\\nclass SSE(Loss): \\n    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"',\n",
              "  'score': 15.3359375,\n",
              "  'rank': 3,\n",
              "  'document_id': '77351f43-8e44-413e-ad03-1ccbb5e5ef3a',\n",
              "  'passage_id': 254}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use as LangChain Retriever"
      ],
      "metadata": {
        "id": "_3ZU9i2qNBD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = RAG.as_langchain_retriever(k=3)"
      ],
      "metadata": {
        "id": "RdzBoo9VM4ol"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"What metric should I use for Loss in Multi Class Classification problem\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyPr7C_kNJM6",
        "outputId": "9590e182-3197-41da-84fa-c480aa020f61"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='So let’ s write that first.\\nW e’ll pass it our model, the data, a loss function, and (if we’re training) an\\noptimizer .It will make a pass through our data, track performance, and (if we passed\\nin an optimizer) update our parameters:\\nimport tqdm \\n \\ndef loop(model: Layer, \\n         images: List[Tensor], \\n         labels: List[Tensor], \\n         loss: Loss, \\n         optimizer: Optimizer = None) -> None: \\n    correct = 0         # Track number of correct predictions. \\n    total_loss = 0.0    # Track total loss. \\n \\n    with tqdm.trange(len(images)) as t: \\n        for i in t: \\n            predicted = model.forward(images[i])             # Predict. \\n            if argmax(predicted) == argmax(labels[i]):       # Check for \\n                correct += 1                                 # correctness. \\n            total_loss += loss.loss(predicted, labels[i])    # Compute loss. \\n \\n            # If we\\'re training, backpropagate gradient and update weights. \\n            if optimizer is not None: \\n                gradient = loss.gradient(predicted, labels[i]) \\n                model.backward(gradient) \\n                optimizer.step(model) \\n \\n            # And update our metrics in the progress bar. \\n            avg_loss = total_loss / (i + 1) \\n            acc = correct / (i + 1) \\n            t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")\\nAs a baseline, we can use our deep learning library to train a (multiclass)\\nlogistic regression model, which is just a single linear layer followed by a\\nsoftmax. This model (in essence) just looks for 10 linear functions such that\\nif the input represents, say , a 5, then the 5th linear function produces the\\nlar gest output.'),\n",
              " Document(metadata={}, page_content='The only\\ntrick is that we’ll need to use tensor_combine :\\nclass SSE(Loss): \\n    \"\"\"Loss function that computes the sum of the squared errors.\"\"\" \\n    def loss(self, predicted: Tensor, actual: Tensor) -> float: \\n        # Compute the tensor of squared differences \\n        squared_errors = tensor_combine( \\n            lambda predicted, actual: (predicted - actual) ** 2, \\n            predicted, \\n            actual) \\n \\n        # And just add them up \\n        return tensor_sum(squared_errors) \\n \\n    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor: \\n        return tensor_combine( \\n            lambda predicted, actual: 2 * (predicted - actual), \\n            predicted, \\n            actual)\\n(W e’ll look at a dif ferent loss function in a bit.)\\nThe  last piece to figure out is gradient descent. Throughout the book we’ve\\ndone all of our gradient descent manually by having a training loop that\\ninvolves something like:\\ntheta = gradient_step(theta, grad, -learning_rate)\\nHere that won’ t quite work for us, for a couple reasons. The first is that our\\nneural nets will have many parameters, and we’ll need to update all ofthem. The second is that we’d like to be able to use more clever variants of\\ngradient descent, and we don’ t want to have to rewrite them each time.\\nAccordingly , we’ll introduce a (you guessed it) Optimizer  abstraction, of\\nwhich gradient descent will be a specific instance:\\nclass Optimizer: \\n    \"\"\" \\n    An optimizer updates the weights of a layer (in place) using information \\n    known by either the layer or the optimizer (or by both). \\n    \"\"\"'),\n",
              " Document(metadata={}, page_content='def __init__(self, layers: List[Layer]) -> None: \\n        self.layers = layers \\n \\n    def forward(self, input): \\n        \"\"\"Just forward the input through the layers in order.\"\"\" \\n        for layer in self.layers: \\n            input = layer.forward(input) \\n        return input \\n \\n    def backward(self, gradient): \\n        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\" \\n        for layer in reversed(self.layers): \\n            gradient = layer.backward(gradient) \\n        return gradient \\n \\n    def params(self) -> Iterable[Tensor]: \\n        \"\"\"Just return the params from each layer.\"\"\" \\n        return (param for layer in self.layers for param in layer.params()) \\n \\n    def grads(self) -> Iterable[Tensor]: \\n        \"\"\"Just return the grads from each layer.\"\"\" \\n        return (grad for layer in self.layers for grad in layer.grads())\\nSo we could represent the neural network we used for XOR as:\\nxor_net = Sequential([ \\n    Linear(input_dim=2, output_dim=2), \\n    Sigmoid(), \\n    Linear(input_dim=2, output_dim=1), \\n    Sigmoid() \\n])\\nBut we still need a little more machinery to train it.\\nL o s s  a n d  O p t i m i z a t i o n\\nPreviously  we wrote out individual loss functions and gradient functions for\\nour models. Here we’ll want to experiment with dif ferent loss functions, so\\n(as usual) we’ll introduce a new Loss  abstraction that encapsulates both the\\nloss computation and the gradient computation:class Loss: \\n    def loss(self, predicted: Tensor, actual: Tensor) -> float: \\n        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\" \\n        raise NotImplementedError \\n \\n    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor: \\n        \"\"\"How does the loss change as the predictions change?\"\"\" \\n        raise NotImplementedError\\nW e’ve already worked many times with the loss that’ s the sum of the\\nsquared errors, so we should have an easy time implementing that. The only\\ntrick is that we’ll need to use tensor_combine :\\nclass SSE(Loss): \\n    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4NAR6WT2Nkwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Chain"
      ],
      "metadata": {
        "id": "xAaHfKgZNUYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "TH75CEjmNliW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\"\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ],
      "metadata": {
        "id": "EEJqDGJFNOzE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain.invoke({\"input\": \"What metric should I use for Loss in Multi Class Classification problem\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4HWoBtTNcuj",
        "outputId": "98eb90e8-0623-4e33-c8e3-efb7fa1efd7b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What metric should I use for Loss in Multi Class Classification problem',\n",
              " 'context': [Document(metadata={}, page_content='So let’ s write that first.\\nW e’ll pass it our model, the data, a loss function, and (if we’re training) an\\noptimizer .It will make a pass through our data, track performance, and (if we passed\\nin an optimizer) update our parameters:\\nimport tqdm \\n \\ndef loop(model: Layer, \\n         images: List[Tensor], \\n         labels: List[Tensor], \\n         loss: Loss, \\n         optimizer: Optimizer = None) -> None: \\n    correct = 0         # Track number of correct predictions. \\n    total_loss = 0.0    # Track total loss. \\n \\n    with tqdm.trange(len(images)) as t: \\n        for i in t: \\n            predicted = model.forward(images[i])             # Predict. \\n            if argmax(predicted) == argmax(labels[i]):       # Check for \\n                correct += 1                                 # correctness. \\n            total_loss += loss.loss(predicted, labels[i])    # Compute loss. \\n \\n            # If we\\'re training, backpropagate gradient and update weights. \\n            if optimizer is not None: \\n                gradient = loss.gradient(predicted, labels[i]) \\n                model.backward(gradient) \\n                optimizer.step(model) \\n \\n            # And update our metrics in the progress bar. \\n            avg_loss = total_loss / (i + 1) \\n            acc = correct / (i + 1) \\n            t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")\\nAs a baseline, we can use our deep learning library to train a (multiclass)\\nlogistic regression model, which is just a single linear layer followed by a\\nsoftmax. This model (in essence) just looks for 10 linear functions such that\\nif the input represents, say , a 5, then the 5th linear function produces the\\nlar gest output.'),\n",
              "  Document(metadata={}, page_content='The only\\ntrick is that we’ll need to use tensor_combine :\\nclass SSE(Loss): \\n    \"\"\"Loss function that computes the sum of the squared errors.\"\"\" \\n    def loss(self, predicted: Tensor, actual: Tensor) -> float: \\n        # Compute the tensor of squared differences \\n        squared_errors = tensor_combine( \\n            lambda predicted, actual: (predicted - actual) ** 2, \\n            predicted, \\n            actual) \\n \\n        # And just add them up \\n        return tensor_sum(squared_errors) \\n \\n    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor: \\n        return tensor_combine( \\n            lambda predicted, actual: 2 * (predicted - actual), \\n            predicted, \\n            actual)\\n(W e’ll look at a dif ferent loss function in a bit.)\\nThe  last piece to figure out is gradient descent. Throughout the book we’ve\\ndone all of our gradient descent manually by having a training loop that\\ninvolves something like:\\ntheta = gradient_step(theta, grad, -learning_rate)\\nHere that won’ t quite work for us, for a couple reasons. The first is that our\\nneural nets will have many parameters, and we’ll need to update all ofthem. The second is that we’d like to be able to use more clever variants of\\ngradient descent, and we don’ t want to have to rewrite them each time.\\nAccordingly , we’ll introduce a (you guessed it) Optimizer  abstraction, of\\nwhich gradient descent will be a specific instance:\\nclass Optimizer: \\n    \"\"\" \\n    An optimizer updates the weights of a layer (in place) using information \\n    known by either the layer or the optimizer (or by both). \\n    \"\"\"'),\n",
              "  Document(metadata={}, page_content='def __init__(self, layers: List[Layer]) -> None: \\n        self.layers = layers \\n \\n    def forward(self, input): \\n        \"\"\"Just forward the input through the layers in order.\"\"\" \\n        for layer in self.layers: \\n            input = layer.forward(input) \\n        return input \\n \\n    def backward(self, gradient): \\n        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\" \\n        for layer in reversed(self.layers): \\n            gradient = layer.backward(gradient) \\n        return gradient \\n \\n    def params(self) -> Iterable[Tensor]: \\n        \"\"\"Just return the params from each layer.\"\"\" \\n        return (param for layer in self.layers for param in layer.params()) \\n \\n    def grads(self) -> Iterable[Tensor]: \\n        \"\"\"Just return the grads from each layer.\"\"\" \\n        return (grad for layer in self.layers for grad in layer.grads())\\nSo we could represent the neural network we used for XOR as:\\nxor_net = Sequential([ \\n    Linear(input_dim=2, output_dim=2), \\n    Sigmoid(), \\n    Linear(input_dim=2, output_dim=1), \\n    Sigmoid() \\n])\\nBut we still need a little more machinery to train it.\\nL o s s  a n d  O p t i m i z a t i o n\\nPreviously  we wrote out individual loss functions and gradient functions for\\nour models. Here we’ll want to experiment with dif ferent loss functions, so\\n(as usual) we’ll introduce a new Loss  abstraction that encapsulates both the\\nloss computation and the gradient computation:class Loss: \\n    def loss(self, predicted: Tensor, actual: Tensor) -> float: \\n        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\" \\n        raise NotImplementedError \\n \\n    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor: \\n        \"\"\"How does the loss change as the predictions change?\"\"\" \\n        raise NotImplementedError\\nW e’ve already worked many times with the loss that’ s the sum of the\\nsquared errors, so we should have an easy time implementing that. The only\\ntrick is that we’ll need to use tensor_combine :\\nclass SSE(Loss): \\n    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"')],\n",
              " 'answer': 'For a multi-class classification problem, you can consider using metrics such as categorical cross-entropy or softmax cross-entropy as the Loss function. These metrics are commonly used in neural network models for multi-class classification tasks.'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"What metric should I use for Loss in Multi Class Classification problem\"})"
      ],
      "metadata": {
        "id": "pqKARPTeNzXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04fc690-0d57-4145-a152-a68f4fc90349"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "FrPrJ5pON45d",
        "outputId": "46201570-b43f-41ff-93c8-1c92e76226eb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'For a multi-class classification problem, you can use metrics such as cross-entropy loss or categorical cross-entropy loss as the Loss function. These metrics are commonly used in neural network models for multi-class classification tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}